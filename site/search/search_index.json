{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FPCMCI - Filtered PCMCI","text":"<p>Extension of the state-of-the-art causal discovery method PCMCI augmented with a feature-selection method based on Transfer Entropy. The algorithm, starting from a prefixed set of variables, identifies the correct subset of features and possible links between them which describe the observed process. Then, from the selected features and links, a causal model is built.</p>"},{"location":"#useful-links","title":"Useful links","text":"<ul> <li>Documentation</li> <li>Tutorials</li> </ul>"},{"location":"#why-fpcmci","title":"Why FPCMCI?","text":"<p>Current state-of-the-art causal discovery approaches suffer in terms of speed and accuracy of the causal analysis when the process to be analysed is composed by a large number of features. FPCMCI is able to select the most meaningful features from a set of variables and build a causal model from such selection. To this end, the causal analysis results faster and more accurate.</p> <p>In the following it is presented an example showing a comparison between causal models obtained by PCMCI and FPCMCI causal discovery algorithms on the same data. The latter have been created by defining a 6-variables system defined as follows:</p> <pre><code>min_lag = 1\nmax_lag = 1\nnp.random.seed(1)\nnsample = 1500\nnfeature = 6\n\nd = np.random.random(size = (nsample, feature))\nfor t in range(max_lag, nsample):\n  d[t, 0] += 2 * d[t-1, 1] + 3 * d[t-1, 3]\n  d[t, 2] += 1.1 * d[t-1, 1]**2\n  d[t, 3] += d[t-1, 3] * d[t-1, 2]\n  d[t, 4] += d[t-1, 4] + d[t-1, 5] * d[t-1, 0]\n</code></pre> Causal Model by PCMCI Causal Model by FPCMCI Execution time ~ 6min 50sec Execution time ~ 2min 45sec <p>The causal analysis performed by the FPCMCI results not only faster but also more accurate. Indeed, the causal model derived by the FPCMCI agrees with the structure of the system of equations, instead the one derived by the PCMCI presents spurious links: * $X_2$ \u2192 $X_4$ * $X_2$ \u2192 $X_5$</p> <p>Note that, since all the 6 variables were involved in the evolution of the system, the FPCMCI did not remove any of them. In the following example instead, we added a new variable in the system which is defined just by the noise component (as $X_1$ and $X_5$) and does not appear in any other equation, defined as follows: $X_6(t) = \\eta_6(t)$. In the following the comparison between PCMCI and FPCMCI with this new system configuration:</p> Causal Model by PCMCI Causal Model by FPCMCI Execution time ~ 8min 40sec Execution time ~ 3min 00sec <p>In this case the FPCMCI removes the $X_6$ variable from the causal graph leading to generate exactly the same causal model as in the previous example, with comparable executional time. Instead, the PCMCI suffers the presence of $X_6$ in terms of time and accuracy of the causal structure. Indeed, a spurious link $X_6$ \u2192 $X_5$ appears in the causal graph derived by the PCMCI.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you found this useful for your work, please cite this papers:</p> <pre><code>@inproceedings{castri2023fpcmci,\n    title={Enhancing Causal Discovery from Robot Sensor Data in Dynamic Scenarios},\n    author={Castri, Luca and Mghames, Sariah and Hanheide, Marc and Bellotto, Nicola},\n    booktitle={Conference on Causal Learning and Reasoning (CLeaR)},\n    year={2023},\n}\n</code></pre>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>tigramite&gt;=5.1.0.3</li> <li>pandas&gt;=1.5.2</li> <li>netgraph&gt;=4.10.2</li> <li>networkx&gt;=2.8.6</li> <li>ruptures&gt;=1.1.7</li> <li>scikit_learn&gt;=1.1.3</li> <li>torch&gt;=1.11.0</li> <li>gpytorch&gt;=1.4</li> <li>dcor&gt;=0.5.3</li> <li>h5py&gt;=3.7.0    </li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Before installing the FPCMCI package, you need to install Java and the IDTxl package used for the feature-selection process, following the guide described here. Once complete, you can install the current release of <code>FPCMCI</code> with:</p> <pre><code>pip install fpcmci\n</code></pre> <p>For a complete installation Java - IDTxl - FPCMCI, follow the following procedure.</p>"},{"location":"#1-java-installation","title":"1 - Java installation","text":"<p>Verify that you have not already installed Java:</p> <pre><code>java -version\n</code></pre> <p>if the latter returns <code>Command 'java' not found, ...</code>, you can install Java by the following commands, otherwise you can jump to IDTxl installation.</p> <pre><code># Java\nsudo apt-get update\nsudo apt install default-jdk\n</code></pre> <p>Then, you need to add JAVA_HOME to the environment</p> <pre><code>sudo nano /etc/environment\nJAVA_HOME=\"/lib/jvm/java-11-openjdk-amd64/bin/java\" # Paste the JAVA_HOME assignment at the bottom of the file\nsource /etc/environment\n</code></pre>"},{"location":"#2-idtxl-installation","title":"2 - IDTxl installation","text":"<pre><code># IDTxl\ngit clone https://github.com/pwollstadt/IDTxl.git\nconda create --name fpcmci python=3.8 pip matplotlib h5py scipy networkx\nconda activate fpcmci\nconda install -c conda-forge jpype1    # required by CPU JIDT estimators\nconda install -c conda-forge pyopencl  # required by GPU OpenCL estimators\nconda install -c anaconda ecos         # required by Tartu PID estimator\nconda install numba                    # required by NumbaCuda estimators\nconda install cudatoolkit              # required by NumbaCuda estimators\nconda install mpmath\n\ncd IDTxl\npip install -e .\n</code></pre>"},{"location":"#3-fpcmci-installation","title":"3 - FPCMCI installation","text":"<pre><code>pip install fpcmci\n</code></pre>"},{"location":"#recent-changes","title":"Recent changes","text":"Version Changes 4.2.1 fixed dependency error in setup.py 4.2.0 causal model with only selected features fixadapted to tigramite 5.2get_causal_matrix FPCMCI method addedf_alpha and pcmci_alpha instead of alpharequirements changedtutorials adapted to new version 4.1.2 tutorials adapted to 4.1.1 and get_SCM method added in FPCMCI 4.1.1 PCMCI dependencies fix: FPCMCI causal model field added, FPCMCI.run() and .run_pcmci() outputs the selected variables and the corresponding causal model 4.1.0 FSelector and FValidator turned into FPCMCI and PCMCIshow_edge_label removed and dag optimizednew package included in the setup.pyadded tutorialsnew example in README.md 4.0.1 online documentation and paths fixes 4.0.0 package published"},{"location":"causal_graph/","title":"Causal Graph","text":""},{"location":"causal_graph/#fpcmci.causal_graph.__scale","title":"<code>__scale(score, min_width, max_width, min_score=0, max_score=1)</code>","text":"<p>Scales the score of the cause-effect relationship strength to a linewitdth</p> <p>Parameters:</p> Name Type Description Default <code>score</code> <code>float</code> <p>score to scale</p> required <code>min_width</code> <code>float</code> <p>minimum linewidth</p> required <code>max_width</code> <code>float</code> <p>maximum linewidth</p> required <code>min_score</code> <code>int</code> <p>minimum score range. Defaults to 0.</p> <code>0</code> <code>max_score</code> <code>int</code> <p>maximum score range. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>float</code> <p>scaled score</p> Source code in <code>fpcmci/causal_graph.py</code> <pre><code>def __scale(score, min_width, max_width, min_score = 0, max_score = 1):\n\"\"\"\n    Scales the score of the cause-effect relationship strength to a linewitdth\n\n    Args:\n        score (float): score to scale\n        min_width (float): minimum linewidth\n        max_width (float): maximum linewidth\n        min_score (int, optional): minimum score range. Defaults to 0.\n        max_score (int, optional): maximum score range. Defaults to 1.\n\n    Returns:\n        (float): scaled score\n    \"\"\"\n    return ((score - min_score) / (max_score - min_score)) * (max_width - min_width) + min_width\n</code></pre>"},{"location":"causal_graph/#fpcmci.causal_graph.dag","title":"<code>dag(res, node_layout='dot', min_width=1, max_width=5, min_score=0, max_score=1, node_size=8, node_color='orange', edge_color='grey', font_size=12, label_type=LabelType.Lag, save_name=None)</code>","text":"<p>build a dag</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>dict</code> <p>dependencies result</p> required <code>node_layout</code> <code>str</code> <p>Node layout. Defaults to 'dot'.</p> <code>'dot'</code> <code>min_width</code> <code>int</code> <p>minimum linewidth. Defaults to 1.</p> <code>1</code> <code>max_width</code> <code>int</code> <p>maximum linewidth. Defaults to 5.</p> <code>5</code> <code>min_score</code> <code>int</code> <p>minimum score range. Defaults to 0.</p> <code>0</code> <code>max_score</code> <code>int</code> <p>maximum score range. Defaults to 1.</p> <code>1</code> <code>node_size</code> <code>int</code> <p>node size. Defaults to 8.</p> <code>8</code> <code>node_color</code> <code>str</code> <p>node color. Defaults to 'orange'.</p> <code>'orange'</code> <code>edge_color</code> <code>str</code> <p>edge color. Defaults to 'grey'.</p> <code>'grey'</code> <code>font_size</code> <code>int</code> <p>font size. Defaults to 12.</p> <code>12</code> <code>label_type</code> <code>LabelType</code> <p>enum to set whether to show the lag time (LabelType.Lag) or the strength (LabelType.Score) of the dependencies on each link/node or not showing the labels (LabelType.NoLabels). Default LabelType.Lag.</p> <code>LabelType.Lag</code> <code>save_name</code> <code>str</code> <p>Filename path. If None, plot is shown and not saved. Defaults to None.</p> <code>None</code> Source code in <code>fpcmci/causal_graph.py</code> <pre><code>def dag(res,\n        node_layout = 'dot',\n        min_width = 1,\n        max_width = 5,\n        min_score = 0,\n        max_score = 1,\n        node_size = 8,\n        node_color = 'orange',\n        edge_color = 'grey',\n        font_size = 12,\n        label_type = LabelType.Lag,\n        save_name = None):\n\"\"\"\n    build a dag\n\n    Args:\n        res (dict): dependencies result\n        node_layout (str, optional): Node layout. Defaults to 'dot'.\n        min_width (int, optional): minimum linewidth. Defaults to 1.\n        max_width (int, optional): maximum linewidth. Defaults to 5.\n        min_score (int, optional): minimum score range. Defaults to 0.\n        max_score (int, optional): maximum score range. Defaults to 1.\n        node_size (int, optional): node size. Defaults to 8.\n        node_color (str, optional): node color. Defaults to 'orange'.\n        edge_color (str, optional): edge color. Defaults to 'grey'.\n        font_size (int, optional): font size. Defaults to 12.\n        label_type (LabelType, optional): enum to set whether to show the lag time (LabelType.Lag) or the strength (LabelType.Score) of the dependencies on each link/node or not showing the labels (LabelType.NoLabels). Default LabelType.Lag.\n        save_name (str, optional): Filename path. If None, plot is shown and not saved. Defaults to None.\n    \"\"\"\n\n    G = nx.DiGraph()\n\n    # NODES DEFINITION\n    G.add_nodes_from(res.keys())\n\n    # BORDER LINE\n    border = dict()\n    for t in res.keys():\n        border[t] = 0\n        for s in res[t]:\n            if t == s[SOURCE]:\n                border[t] = max(__scale(s[SCORE], min_width, max_width, min_score, max_score), border[t])\n\n    # BORDER LABEL\n    node_label = None\n    if label_type == LabelType.Lag or label_type == LabelType.Score:\n        node_label = {t: [] for t in res.keys()}\n        for t in res.keys():\n            for s in res[t]:\n                if t == s[SOURCE]:\n                    if label_type == LabelType.Lag:\n                        node_label[t].append(s[LAG])\n                    elif label_type == LabelType.Score:\n                        node_label[t].append(round(s[SCORE], 3))\n            node_label[t] = \",\".join(str(s) for s in node_label[t])\n\n\n    # EDGE DEFINITION\n    edges = [(s[SOURCE], t) for t in res.keys() for s in res[t] if t != s[SOURCE]]\n    G.add_edges_from(edges)\n\n    # EDGE LINE\n    edge_width = {(s[SOURCE], t): 0 for t in res.keys() for s in res[t] if t != s[SOURCE]}\n    for t in res.keys():\n        for s in res[t]:\n            if t != s[SOURCE]:\n                edge_width[(s[SOURCE], t)] = max(__scale(s[SCORE], min_width, max_width, min_score, max_score), edge_width[(s[SOURCE], t)])\n\n    # EDGE LABEL\n    edge_label = None\n    if label_type == LabelType.Lag or label_type == LabelType.Score:\n        edge_label = {(s[SOURCE], t): [] for t in res.keys() for s in res[t] if t != s[SOURCE]}\n        for t in res.keys():\n            for s in res[t]:\n                if t != s[SOURCE]:\n                    if label_type == LabelType.Lag:\n                        edge_label[(s[SOURCE], t)].append(s[LAG])\n                    elif label_type == LabelType.Score:\n                        edge_label[(s[SOURCE], t)].append(round(s[SCORE], 3))\n        for k in edge_label.keys():\n            edge_label[k] = \",\".join(str(s) for s in edge_label[k])\n\n    fig, ax = plt.subplots(figsize=(8,6))\n\n    if edges:\n        a = Graph(G, \n                node_layout = node_layout,\n                node_size = node_size,\n                node_color = node_color,\n                node_labels = node_label,\n                node_edge_width = border,\n                node_label_fontdict = dict(size=font_size),\n                node_edge_color = edge_color,\n                node_label_offset = 0.1,\n                node_alpha = 1,\n\n                arrows = True,\n                edge_layout = 'curved',\n                edge_label = label_type != LabelType.NoLabels,\n                edge_labels = edge_label,\n                edge_label_fontdict = dict(size=font_size),\n                edge_color = edge_color, \n                edge_width = edge_width,\n                edge_alpha = 1,\n                edge_zorder = 1,\n                edge_label_position = 0.35,\n                edge_layout_kwargs = dict(bundle_parallel_edges = False, k = 0.05))\n\n        nx.draw_networkx_labels(G, \n                                pos = a.node_positions,\n                                labels = {n: n for n in G},\n                                font_size = font_size)\n\n    if save_name is not None:\n        plt.savefig(save_name, dpi = 300)\n    else:\n        plt.show()\n</code></pre>"},{"location":"causal_graph/#fpcmci.causal_graph.ts_dag","title":"<code>ts_dag(res, tau, min_width=1, max_width=5, min_score=0, max_score=1, node_size=8, node_color='orange', edge_color='grey', font_size=12, save_name=None)</code>","text":"<p>build a timeseries dag</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>dict</code> <p>dependencies result</p> required <code>tau</code> <code>int</code> <p>max time lag</p> required <code>min_width</code> <code>int</code> <p>minimum linewidth. Defaults to 1.</p> <code>1</code> <code>max_width</code> <code>int</code> <p>maximum linewidth. Defaults to 5.</p> <code>5</code> <code>min_score</code> <code>int</code> <p>minimum score range. Defaults to 0.</p> <code>0</code> <code>max_score</code> <code>int</code> <p>maximum score range. Defaults to 1.</p> <code>1</code> <code>node_size</code> <code>int</code> <p>node size. Defaults to 8.</p> <code>8</code> <code>node_color</code> <code>str</code> <p>node color. Defaults to 'orange'.</p> <code>'orange'</code> <code>edge_color</code> <code>str</code> <p>edge color. Defaults to 'grey'.</p> <code>'grey'</code> <code>font_size</code> <code>int</code> <p>font size. Defaults to 12.</p> <code>12</code> <code>save_name</code> <code>str</code> <p>Filename path. If None, plot is shown and not saved. Defaults to None.</p> <code>None</code> Source code in <code>fpcmci/causal_graph.py</code> <pre><code>def ts_dag(res,\n           tau,\n           min_width = 1,\n           max_width = 5,\n           min_score = 0,\n           max_score = 1,\n           node_size = 8,\n           node_color = 'orange',\n           edge_color = 'grey',\n           font_size = 12,\n           save_name = None):\n\"\"\"\n    build a timeseries dag\n\n    Args:\n        res (dict): dependencies result\n        tau (int): max time lag\n        min_width (int, optional): minimum linewidth. Defaults to 1.\n        max_width (int, optional): maximum linewidth. Defaults to 5.\n        min_score (int, optional): minimum score range. Defaults to 0.\n        max_score (int, optional): maximum score range. Defaults to 1.\n        node_size (int, optional): node size. Defaults to 8.\n        node_color (str, optional): node color. Defaults to 'orange'.\n        edge_color (str, optional): edge color. Defaults to 'grey'.\n        font_size (int, optional): font size. Defaults to 12.\n        save_name (str, optional): Filename path. If None, plot is shown and not saved. Defaults to None.\n    \"\"\"\n\n    # add nodes\n    G = nx.grid_2d_graph(tau + 1, len(res.keys()))\n    pos = dict()\n    for n in G.nodes():\n        if n[0] == 0:\n            pos[n] = (n[0], n[1]/2)\n        else:\n            pos[n] = (n[0] + .5, n[1]/2)\n    scale = max(pos.values())\n    G.remove_edges_from(G.edges())\n\n    # edges definition\n    edges = list()\n    edge_width = dict()\n    for t in res.keys():\n        for s in res[t]:\n            s_index = len(res.keys())-1 - list(res.keys()).index(s[SOURCE])\n            t_index = len(res.keys())-1 - list(res.keys()).index(t)\n            s_node = (tau - s[LAG], s_index)\n            t_node = (tau, t_index)\n            edges.append((s_node, t_node))\n            edge_width[(s_node, t_node)] = __scale(s[SCORE], min_width, max_width, min_score, max_score)\n    G.add_edges_from(edges)\n\n    # label definition\n    labeldict = {}\n    for n in G.nodes():\n        if n[0] == 0:\n            labeldict[n] = list(res.keys())[len(res.keys()) - 1 - n[1]]\n\n    fig, ax = plt.subplots(figsize=(8,6))\n\n    # time line text drawing\n    pos_tau = set([pos[p][0] for p in pos])\n    max_y = max([pos[p][1] for p in pos])\n    for p in pos_tau:\n        if abs(int(p) - tau) == 0:\n            ax.text(p, max_y + .3, r\"$t$\", horizontalalignment='center', fontsize=font_size)\n        else:\n            ax.text(p, max_y + .3, r\"$t-\" + str(abs(int(p) - tau)) + \"$\", horizontalalignment='center', fontsize=font_size)\n\n    Graph(G,\n          node_layout = {p : np.array(pos[p]) for p in pos},\n          node_size = node_size,\n          node_color = node_color,\n          node_labels = labeldict,\n          node_label_offset = 0,\n          node_edge_width = 0,\n          node_label_fontdict = dict(size=font_size),\n          node_alpha = 1,\n\n          arrows = True,\n          edge_layout = 'curved',\n          edge_label = False,\n          edge_color = edge_color, \n          edge_width = edge_width,\n          edge_alpha = 1,\n          edge_zorder = 1,\n          scale = (scale[0] + 2, scale[1] + 2))\n\n    if save_name is not None:\n        plt.savefig(save_name, dpi = 300)\n    else:\n        plt.show()\n</code></pre>"},{"location":"feature_selection_method/","title":"Feature Selection Methods","text":""},{"location":"feature_selection_method/#fpcmci.selection_methods.SelectionMethod.SelectionMethod","title":"<code>SelectionMethod</code>","text":"<p>         Bases: <code>ABC</code></p> <p>SelectionMethod abstract class</p> Source code in <code>fpcmci/selection_methods/SelectionMethod.py</code> <pre><code>class SelectionMethod(ABC):\n\"\"\"\n    SelectionMethod abstract class\n    \"\"\"\n    def __init__(self, ctest):\n        self.ctest = ctest\n        self.data = None\n        self.alpha = None\n        self.min_lag = None\n        self.max_lag = None\n        self.result = dict()\n\n\n    @property\n    def name(self):\n\"\"\"\n        Returns Selection Method name\n\n        Returns:\n            (str): Selection Method name\n        \"\"\"\n        return self.ctest.value\n\n\n    def initialise(self, data: Data, alpha, min_lag, max_lag):\n\"\"\"\n        Initialises the selection method\n\n        Args:\n            data (Data): Data\n            alpha (float): significance threshold\n            min_lag (int): min lag time\n            max_lag (int): max lag time\n        \"\"\"\n        self.data = data\n        self.alpha = alpha\n        self.min_lag = min_lag\n        self.max_lag = max_lag\n        self.result = {f:list() for f in self.data.features}\n\n\n    @abstractmethod\n    def compute_dependencies(self) -&gt; dict:\n\"\"\"\n        abstract method\n        \"\"\"\n        pass\n\n\n    def _prepare_ts(self, target, lag, apply_lag = True, consider_autodep = True):\n\"\"\"\n        prepare the dataframe to the analysis\n\n        Args:\n            target (str): name target var\n            lag (int): lag time to apply\n            apply_lag (bool, optional): True if you want to apply the lag, False otherwise. Defaults to True.\n\n        Returns:\n            tuple(DataFrame, DataFrame): source and target dataframe\n        \"\"\"\n        if not consider_autodep:\n            if apply_lag:\n                Y = self.data.d[target][lag:]\n                X = self.data.d.loc[:, self.data.d.columns != target][:-lag]\n            else:\n                Y = self.data.d[target]\n                X = self.data.d.loc[:, self.data.d.columns != target]\n        else:\n            if apply_lag:\n                Y = self.data.d[target][lag:]\n                X = self.data.d[:-lag]\n            else:\n                Y = self.data.d[target]\n                X = self.data.d\n        return X, Y\n\n\n    def _get_sources(self, t):\n\"\"\"\n        Return target sources\n\n        Args:\n            t (str): target variable name\n\n        Returns:\n            list(str): list of target sources\n        \"\"\"\n        return [s[SOURCE] for s in self.result[t]]\n\n\n    def _add_dependecies(self, t, s, score, pval, lag):\n\"\"\"\n        Adds found dependency from source (s) to target (t) specifying the \n        score, pval and the lag\n\n        Args:\n            t (str): target feature name\n            s (str): source feature name\n            score (float): selection method score\n            pval (float): pval associated to the dependency\n            lag (int): lag time of the dependency\n        \"\"\"\n        self.result[t].append({SOURCE:s, \n                               SCORE:score,\n                               PVAL:pval, \n                               LAG:lag})\n        str_s = \"(\" + s + \" -\" + str(lag) + \")\"\n        str_arrow = \" --&gt; \"\n        str_t = \"(\" + t + \")\"\n        str_score = \"|score: \" + \"{:.3f}\".format(score)\n        str_pval = \"|pval: \" + \"{:.3f}\".format(pval)\n        print('{:&lt;20s}{:&lt;10s}{:&lt;10s}{:&lt;20s}{:&lt;20s}'.format(str_s, str_arrow, str_t, str_score, str_pval))\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.SelectionMethod.SelectionMethod.name","title":"<code>name</code>  <code>property</code>","text":"<p>Returns Selection Method name</p> <p>Returns:</p> Type Description <code>str</code> <p>Selection Method name</p>"},{"location":"feature_selection_method/#fpcmci.selection_methods.SelectionMethod.SelectionMethod.compute_dependencies","title":"<code>compute_dependencies()</code>  <code>abstractmethod</code>","text":"<p>abstract method</p> Source code in <code>fpcmci/selection_methods/SelectionMethod.py</code> <pre><code>@abstractmethod\ndef compute_dependencies(self) -&gt; dict:\n\"\"\"\n    abstract method\n    \"\"\"\n    pass\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.SelectionMethod.SelectionMethod.initialise","title":"<code>initialise(data, alpha, min_lag, max_lag)</code>","text":"<p>Initialises the selection method</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>Data</p> required <code>alpha</code> <code>float</code> <p>significance threshold</p> required <code>min_lag</code> <code>int</code> <p>min lag time</p> required <code>max_lag</code> <code>int</code> <p>max lag time</p> required Source code in <code>fpcmci/selection_methods/SelectionMethod.py</code> <pre><code>def initialise(self, data: Data, alpha, min_lag, max_lag):\n\"\"\"\n    Initialises the selection method\n\n    Args:\n        data (Data): Data\n        alpha (float): significance threshold\n        min_lag (int): min lag time\n        max_lag (int): max lag time\n    \"\"\"\n    self.data = data\n    self.alpha = alpha\n    self.min_lag = min_lag\n    self.max_lag = max_lag\n    self.result = {f:list() for f in self.data.features}\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.Corr.Corr","title":"<code>Corr</code>","text":"<p>         Bases: <code>SelectionMethod</code></p> <p>Feature selection method based on Correlation analysis</p> Source code in <code>fpcmci/selection_methods/Corr.py</code> <pre><code>class Corr(SelectionMethod):\n\"\"\"\n    Feature selection method based on Correlation analysis\n    \"\"\"\n    def __init__(self):\n\"\"\"\n        Corr contructor class\n        \"\"\"\n        super().__init__(CTest.Corr)\n\n\n    def compute_dependencies(self):\n\"\"\"\n        compute list of dependencies for each target by correlation analysis\n\n        Returns:\n            (dict): dictonary(TARGET: list SOURCES)\n        \"\"\"\n        CP.info(\"\\n##\")\n        CP.info(\"## \" + self.name + \" analysis\")\n        CP.info(\"##\")\n\n        for lag in range(self.min_lag, self.max_lag + 1):\n            for target in self.data.features:\n                CP.info(\"\\n## Target variable: \" + target)\n\n                X, Y = self._prepare_ts(target, lag)\n                scores, pval = f_regression(X, Y)\n\n                # Filter on pvalue\n                f = pval &lt; self.alpha\n\n                # Result of the selection\n                sel_sources, sel_sources_score, sel_sources_pval = X.columns[f].tolist(), scores[f].tolist(), pval[f].tolist()\n\n                for s, score, pval in zip(sel_sources, sel_sources_score, sel_sources_pval):\n                    self._add_dependecies(target, s, score, pval, lag)\n\n        return self.result\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.Corr.Corr.__init__","title":"<code>__init__()</code>","text":"<p>Corr contructor class</p> Source code in <code>fpcmci/selection_methods/Corr.py</code> <pre><code>def __init__(self):\n\"\"\"\n    Corr contructor class\n    \"\"\"\n    super().__init__(CTest.Corr)\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.Corr.Corr.compute_dependencies","title":"<code>compute_dependencies()</code>","text":"<p>compute list of dependencies for each target by correlation analysis</p> <p>Returns:</p> Type Description <code>dict</code> <p>dictonary(TARGET: list SOURCES)</p> Source code in <code>fpcmci/selection_methods/Corr.py</code> <pre><code>def compute_dependencies(self):\n\"\"\"\n    compute list of dependencies for each target by correlation analysis\n\n    Returns:\n        (dict): dictonary(TARGET: list SOURCES)\n    \"\"\"\n    CP.info(\"\\n##\")\n    CP.info(\"## \" + self.name + \" analysis\")\n    CP.info(\"##\")\n\n    for lag in range(self.min_lag, self.max_lag + 1):\n        for target in self.data.features:\n            CP.info(\"\\n## Target variable: \" + target)\n\n            X, Y = self._prepare_ts(target, lag)\n            scores, pval = f_regression(X, Y)\n\n            # Filter on pvalue\n            f = pval &lt; self.alpha\n\n            # Result of the selection\n            sel_sources, sel_sources_score, sel_sources_pval = X.columns[f].tolist(), scores[f].tolist(), pval[f].tolist()\n\n            for s, score, pval in zip(sel_sources, sel_sources_score, sel_sources_pval):\n                self._add_dependecies(target, s, score, pval, lag)\n\n    return self.result\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.ParCorr.ParCorr","title":"<code>ParCorr</code>","text":"<p>         Bases: <code>SelectionMethod</code></p> <p>Feature selection method based on Partial Correlation analysis</p> Source code in <code>fpcmci/selection_methods/ParCorr.py</code> <pre><code>class ParCorr(SelectionMethod):\n\"\"\"\n    Feature selection method based on Partial Correlation analysis\n    \"\"\"\n    def __init__(self):\n\"\"\"\n        ParCorr class contructor\n        \"\"\"\n        super().__init__(CTest.Corr)\n\n\n    def get_residual(self, covar, target):\n\"\"\"\n        Calculate residual of the target variable obtaining conditioning on the covar variables\n\n        Args:\n            covar (np.array): conditioning variables\n            target (np.array): target variable\n\n        Returns:\n            (np.array): residual\n        \"\"\"\n        beta = np.linalg.lstsq(covar, target, rcond=None)[0]\n        return target - np.dot(covar, beta)\n\n\n    def partial_corr(self, X, Y, Z):\n\"\"\"\n        Calculate Partial correlation between X and Y conditioning on Z\n\n        Args:\n            X (np.array): source candidate variable\n            Y (np.array): target variable\n            Z (np.array): conditioning variable\n\n        Returns:\n            (float, float): partial correlation, p-value\n        \"\"\"\n\n        pcorr, pval = stats.pearsonr(self.get_residual(Z, X), self.get_residual(Z, Y))\n\n        return pcorr, pval\n\n    def compute_dependencies(self):\n\"\"\"\n        compute list of dependencies for each target by partial correlation analysis\n\n        Returns:\n            (dict): dictonary(TARGET: list SOURCES)\n        \"\"\"\n        CP.info(\"\\n##\")\n        CP.info(\"## \" + self.name + \" analysis\")\n        CP.info(\"##\")\n\n        for lag in range(self.min_lag, self.max_lag + 1):\n            for target in self.data.features:\n                CP.info(\"\\n## Target variable: \" + target)\n                candidates = self.data.features\n\n                Y = np.array(self.data.d[target][lag:])\n\n                while candidates:\n                    tmp_res = None\n                    covars = self._get_sources(target)\n                    Z = np.array(self.data.d[covars][:-lag])\n\n                    for candidate in candidates:\n                        X = np.array(self.data.d[candidate][:-lag])\n                        score, pval = self.partial_corr(X, Y, Z)\n                        if pval &lt; self.alpha and (tmp_res is None or abs(tmp_res[1]) &lt; abs(score)):\n                            tmp_res = (candidate, score, pval)\n\n                    if tmp_res is not None: \n                        self._add_dependecies(target, tmp_res[0], tmp_res[1], tmp_res[2], lag)\n                        candidates.remove(tmp_res[0])\n                    else:\n                        break\n        return self.result\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.ParCorr.ParCorr.__init__","title":"<code>__init__()</code>","text":"<p>ParCorr class contructor</p> Source code in <code>fpcmci/selection_methods/ParCorr.py</code> <pre><code>def __init__(self):\n\"\"\"\n    ParCorr class contructor\n    \"\"\"\n    super().__init__(CTest.Corr)\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.ParCorr.ParCorr.compute_dependencies","title":"<code>compute_dependencies()</code>","text":"<p>compute list of dependencies for each target by partial correlation analysis</p> <p>Returns:</p> Type Description <code>dict</code> <p>dictonary(TARGET: list SOURCES)</p> Source code in <code>fpcmci/selection_methods/ParCorr.py</code> <pre><code>def compute_dependencies(self):\n\"\"\"\n    compute list of dependencies for each target by partial correlation analysis\n\n    Returns:\n        (dict): dictonary(TARGET: list SOURCES)\n    \"\"\"\n    CP.info(\"\\n##\")\n    CP.info(\"## \" + self.name + \" analysis\")\n    CP.info(\"##\")\n\n    for lag in range(self.min_lag, self.max_lag + 1):\n        for target in self.data.features:\n            CP.info(\"\\n## Target variable: \" + target)\n            candidates = self.data.features\n\n            Y = np.array(self.data.d[target][lag:])\n\n            while candidates:\n                tmp_res = None\n                covars = self._get_sources(target)\n                Z = np.array(self.data.d[covars][:-lag])\n\n                for candidate in candidates:\n                    X = np.array(self.data.d[candidate][:-lag])\n                    score, pval = self.partial_corr(X, Y, Z)\n                    if pval &lt; self.alpha and (tmp_res is None or abs(tmp_res[1]) &lt; abs(score)):\n                        tmp_res = (candidate, score, pval)\n\n                if tmp_res is not None: \n                    self._add_dependecies(target, tmp_res[0], tmp_res[1], tmp_res[2], lag)\n                    candidates.remove(tmp_res[0])\n                else:\n                    break\n    return self.result\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.ParCorr.ParCorr.get_residual","title":"<code>get_residual(covar, target)</code>","text":"<p>Calculate residual of the target variable obtaining conditioning on the covar variables</p> <p>Parameters:</p> Name Type Description Default <code>covar</code> <code>np.array</code> <p>conditioning variables</p> required <code>target</code> <code>np.array</code> <p>target variable</p> required <p>Returns:</p> Type Description <code>np.array</code> <p>residual</p> Source code in <code>fpcmci/selection_methods/ParCorr.py</code> <pre><code>def get_residual(self, covar, target):\n\"\"\"\n    Calculate residual of the target variable obtaining conditioning on the covar variables\n\n    Args:\n        covar (np.array): conditioning variables\n        target (np.array): target variable\n\n    Returns:\n        (np.array): residual\n    \"\"\"\n    beta = np.linalg.lstsq(covar, target, rcond=None)[0]\n    return target - np.dot(covar, beta)\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.ParCorr.ParCorr.partial_corr","title":"<code>partial_corr(X, Y, Z)</code>","text":"<p>Calculate Partial correlation between X and Y conditioning on Z</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.array</code> <p>source candidate variable</p> required <code>Y</code> <code>np.array</code> <p>target variable</p> required <code>Z</code> <code>np.array</code> <p>conditioning variable</p> required <p>Returns:</p> Type Description <code>float, float</code> <p>partial correlation, p-value</p> Source code in <code>fpcmci/selection_methods/ParCorr.py</code> <pre><code>def partial_corr(self, X, Y, Z):\n\"\"\"\n    Calculate Partial correlation between X and Y conditioning on Z\n\n    Args:\n        X (np.array): source candidate variable\n        Y (np.array): target variable\n        Z (np.array): conditioning variable\n\n    Returns:\n        (float, float): partial correlation, p-value\n    \"\"\"\n\n    pcorr, pval = stats.pearsonr(self.get_residual(Z, X), self.get_residual(Z, Y))\n\n    return pcorr, pval\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.MI.MI","title":"<code>MI</code>","text":"<p>         Bases: <code>SelectionMethod</code></p> <p>Feature selection method based on Mutual Information analysis</p> Source code in <code>fpcmci/selection_methods/MI.py</code> <pre><code>class MI(SelectionMethod):\n\"\"\"\n    Feature selection method based on Mutual Information analysis\n    \"\"\"\n    def __init__(self, estimator: MIestimator):\n\"\"\"\n        MI class contructor\n\n        Args:\n            estimator (MIestimator): Gaussian/Kraskov\n        \"\"\"\n        super().__init__(CTest.MI)\n        self.estimator = estimator\n\n    def compute_dependencies(self):\n\"\"\"\n        compute list of dependencies for each target by mutual information analysis\n\n        Returns:\n            (dict): dictonary(TARGET: list SOURCES)\n        \"\"\"\n        with _suppress_stdout():\n            data = Data(self.d.values, dim_order='sp') # sp = samples(row) x processes(col)\n\n            network_analysis = MultivariateMI()\n            settings = {'cmi_estimator': self.estimator.value,\n                        'max_lag_sources': self.max_lag,\n                        'min_lag_sources': self.min_lag,\n                        'alpha_max_stats': self.alpha,\n                        'alpha_min_stats': self.alpha,\n                        'alpha_omnibus': self.alpha,\n                        'alpha_max_seq': self.alpha,\n                        'verbose': False}\n            results = network_analysis.analyse_network(settings=settings, data=data)\n\n        for t in results._single_target.keys():\n            sel_sources = [s[0] for s in results._single_target[t]['selected_vars_sources']]\n            if sel_sources:\n                sel_sources_lag = [s[1] for s in results._single_target[t]['selected_vars_sources']]\n                sel_sources_score = results._single_target[t]['selected_sources_mi']\n                sel_sources_pval = results._single_target[t]['selected_sources_pval']\n                for s, score, pval, lag in zip(sel_sources, sel_sources_score, sel_sources_pval, sel_sources_lag):\n                    self._add_dependecies(self.features[t], self.features[s], score, pval, lag)\n\n        return self.result\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.MI.MI.__init__","title":"<code>__init__(estimator)</code>","text":"<p>MI class contructor</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>MIestimator</code> <p>Gaussian/Kraskov</p> required Source code in <code>fpcmci/selection_methods/MI.py</code> <pre><code>def __init__(self, estimator: MIestimator):\n\"\"\"\n    MI class contructor\n\n    Args:\n        estimator (MIestimator): Gaussian/Kraskov\n    \"\"\"\n    super().__init__(CTest.MI)\n    self.estimator = estimator\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.MI.MI.compute_dependencies","title":"<code>compute_dependencies()</code>","text":"<p>compute list of dependencies for each target by mutual information analysis</p> <p>Returns:</p> Type Description <code>dict</code> <p>dictonary(TARGET: list SOURCES)</p> Source code in <code>fpcmci/selection_methods/MI.py</code> <pre><code>def compute_dependencies(self):\n\"\"\"\n    compute list of dependencies for each target by mutual information analysis\n\n    Returns:\n        (dict): dictonary(TARGET: list SOURCES)\n    \"\"\"\n    with _suppress_stdout():\n        data = Data(self.d.values, dim_order='sp') # sp = samples(row) x processes(col)\n\n        network_analysis = MultivariateMI()\n        settings = {'cmi_estimator': self.estimator.value,\n                    'max_lag_sources': self.max_lag,\n                    'min_lag_sources': self.min_lag,\n                    'alpha_max_stats': self.alpha,\n                    'alpha_min_stats': self.alpha,\n                    'alpha_omnibus': self.alpha,\n                    'alpha_max_seq': self.alpha,\n                    'verbose': False}\n        results = network_analysis.analyse_network(settings=settings, data=data)\n\n    for t in results._single_target.keys():\n        sel_sources = [s[0] for s in results._single_target[t]['selected_vars_sources']]\n        if sel_sources:\n            sel_sources_lag = [s[1] for s in results._single_target[t]['selected_vars_sources']]\n            sel_sources_score = results._single_target[t]['selected_sources_mi']\n            sel_sources_pval = results._single_target[t]['selected_sources_pval']\n            for s, score, pval, lag in zip(sel_sources, sel_sources_score, sel_sources_pval, sel_sources_lag):\n                self._add_dependecies(self.features[t], self.features[s], score, pval, lag)\n\n    return self.result\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.TE.TE","title":"<code>TE</code>","text":"<p>         Bases: <code>SelectionMethod</code></p> <p>Feature selection method based on Trasfer Entropy analysis</p> Source code in <code>fpcmci/selection_methods/TE.py</code> <pre><code>class TE(SelectionMethod):\n\"\"\"\n    Feature selection method based on Trasfer Entropy analysis\n    \"\"\"\n    def __init__(self, estimator: TEestimator):\n\"\"\"\n        TE class contructor\n\n        Args:\n            estimator (TEestimator): Gaussian/Kraskov\n        \"\"\"\n        super().__init__(CTest.TE)\n        self.estimator = estimator\n\n\n    def compute_dependencies(self):\n\"\"\"\n        compute list of dependencies for each target by transfer entropy analysis\n\n        Returns:\n            (dict): dictonary(TARGET: list SOURCES)\n        \"\"\"\n        multi_network_analysis = MultivariateTE()\n        bi_network_analysis = BivariateMI()\n        settings = {'cmi_estimator': self.estimator.value,\n                    'max_lag_sources': self.max_lag,\n                    'min_lag_sources': self.min_lag,\n                    'max_lag_target': self.max_lag,\n                    'min_lag_target': self.min_lag,\n                    'alpha_max_stats': self.alpha,\n                    'alpha_min_stats': self.alpha,\n                    'alpha_omnibus': self.alpha,\n                    'alpha_max_seq': self.alpha,\n                    'verbose': False}\n\n        CP.info(\"\\n##\")\n        CP.info(\"## \" + self.name + \" analysis\")\n        CP.info(\"##\")\n        for target in self.data.features:\n            CP.info(\"\\n## Target variable: \" + target)\n            with _suppress_stdout():\n                t = self.data.features.index(target)\n\n                # Check auto-dependency\n                tmp_d = np.c_[self.data.d.values[:, t], self.data.d.values[:, t]]\n                data = Data(tmp_d, dim_order='sp') # sp = samples(row) x processes(col)\n                res_auto = bi_network_analysis.analyse_single_target(settings = settings, data = data, target = 0, sources = 1)\n\n                # Check cross-dependencies\n                data = Data(self.data.d.values, dim_order='sp') # sp = samples(row) x processes(col)\n                res_cross = multi_network_analysis.analyse_single_target(settings = settings, data = data, target = t)\n\n            # Auto-dependency handling\n            auto_lag = [s[1] for s in res_auto._single_target[0]['selected_vars_sources']]\n            auto_score = res_auto._single_target[0]['selected_sources_mi']\n            auto_pval = res_auto._single_target[0]['selected_sources_pval']\n            if auto_score is not None:\n                for score, pval, lag in zip(auto_score, auto_pval, auto_lag):\n                    self._add_dependecies(self.data.features[t], self.data.features[t], score, pval, lag)\n\n            # Cross-dependencies handling    \n            sel_sources = [s[0] for s in res_cross._single_target[t]['selected_vars_sources']]\n            if sel_sources:\n                sel_sources_lag = [s[1] for s in res_cross._single_target[t]['selected_vars_sources']]\n                sel_sources_score = res_cross._single_target[t]['selected_sources_te']\n                sel_sources_pval = res_cross._single_target[t]['selected_sources_pval']\n                for s, score, pval, lag in zip(sel_sources, sel_sources_score, sel_sources_pval, sel_sources_lag):\n                    self._add_dependecies(self.data.features[t], self.data.features[s], score, pval, lag)\n\n            if auto_score is None and not sel_sources:\n                CP.info(\"no sources selected\")\n\n        return self.result\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.TE.TE.__init__","title":"<code>__init__(estimator)</code>","text":"<p>TE class contructor</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>TEestimator</code> <p>Gaussian/Kraskov</p> required Source code in <code>fpcmci/selection_methods/TE.py</code> <pre><code>def __init__(self, estimator: TEestimator):\n\"\"\"\n    TE class contructor\n\n    Args:\n        estimator (TEestimator): Gaussian/Kraskov\n    \"\"\"\n    super().__init__(CTest.TE)\n    self.estimator = estimator\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.TE.TE.compute_dependencies","title":"<code>compute_dependencies()</code>","text":"<p>compute list of dependencies for each target by transfer entropy analysis</p> <p>Returns:</p> Type Description <code>dict</code> <p>dictonary(TARGET: list SOURCES)</p> Source code in <code>fpcmci/selection_methods/TE.py</code> <pre><code>def compute_dependencies(self):\n\"\"\"\n    compute list of dependencies for each target by transfer entropy analysis\n\n    Returns:\n        (dict): dictonary(TARGET: list SOURCES)\n    \"\"\"\n    multi_network_analysis = MultivariateTE()\n    bi_network_analysis = BivariateMI()\n    settings = {'cmi_estimator': self.estimator.value,\n                'max_lag_sources': self.max_lag,\n                'min_lag_sources': self.min_lag,\n                'max_lag_target': self.max_lag,\n                'min_lag_target': self.min_lag,\n                'alpha_max_stats': self.alpha,\n                'alpha_min_stats': self.alpha,\n                'alpha_omnibus': self.alpha,\n                'alpha_max_seq': self.alpha,\n                'verbose': False}\n\n    CP.info(\"\\n##\")\n    CP.info(\"## \" + self.name + \" analysis\")\n    CP.info(\"##\")\n    for target in self.data.features:\n        CP.info(\"\\n## Target variable: \" + target)\n        with _suppress_stdout():\n            t = self.data.features.index(target)\n\n            # Check auto-dependency\n            tmp_d = np.c_[self.data.d.values[:, t], self.data.d.values[:, t]]\n            data = Data(tmp_d, dim_order='sp') # sp = samples(row) x processes(col)\n            res_auto = bi_network_analysis.analyse_single_target(settings = settings, data = data, target = 0, sources = 1)\n\n            # Check cross-dependencies\n            data = Data(self.data.d.values, dim_order='sp') # sp = samples(row) x processes(col)\n            res_cross = multi_network_analysis.analyse_single_target(settings = settings, data = data, target = t)\n\n        # Auto-dependency handling\n        auto_lag = [s[1] for s in res_auto._single_target[0]['selected_vars_sources']]\n        auto_score = res_auto._single_target[0]['selected_sources_mi']\n        auto_pval = res_auto._single_target[0]['selected_sources_pval']\n        if auto_score is not None:\n            for score, pval, lag in zip(auto_score, auto_pval, auto_lag):\n                self._add_dependecies(self.data.features[t], self.data.features[t], score, pval, lag)\n\n        # Cross-dependencies handling    \n        sel_sources = [s[0] for s in res_cross._single_target[t]['selected_vars_sources']]\n        if sel_sources:\n            sel_sources_lag = [s[1] for s in res_cross._single_target[t]['selected_vars_sources']]\n            sel_sources_score = res_cross._single_target[t]['selected_sources_te']\n            sel_sources_pval = res_cross._single_target[t]['selected_sources_pval']\n            for s, score, pval, lag in zip(sel_sources, sel_sources_score, sel_sources_pval, sel_sources_lag):\n                self._add_dependecies(self.data.features[t], self.data.features[s], score, pval, lag)\n\n        if auto_score is None and not sel_sources:\n            CP.info(\"no sources selected\")\n\n    return self.result\n</code></pre>"},{"location":"fpcmci/","title":"FPCMCI","text":""},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI","title":"<code>FPCMCI</code>","text":"<p>FPCMCI class.</p> <p>FPCMCI is a causal feature selector framework for large-scale time series datasets. Sarting from a Data object and it selects the main features responsible for the evolution of the analysed system. Based on the selected features, the framework outputs a causal model.</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>class FPCMCI():\n\"\"\"\n    FPCMCI class.\n\n    FPCMCI is a causal feature selector framework for large-scale time series\n    datasets. Sarting from a Data object and it selects the main features\n    responsible for the evolution of the analysed system. Based on the selected features,\n    the framework outputs a causal model.\n    \"\"\"\n\n    def __init__(self, \n                 data: Data, \n                 min_lag, max_lag, \n                 sel_method: SelectionMethod, val_condtest: CondIndTest, \n                 verbosity: CPLevel, \n                 f_alpha = 0.05, \n                 pcmci_alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False):\n\"\"\"\n        FPCMCI class contructor\n\n        Args:\n            data (Data): data to analyse\n            min_lag (int): minimum time lag\n            max_lag (int): maximum time lag\n            sel_method (SelectionMethod): selection method\n            val_condtest (CondIndTest): validation method\n            verbosity (CPLevel): verbosity level\n            f_alpha (float, optional): filter significance level. Defaults to 0.05.\n            pcmci_alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        \"\"\"\n\n        self.data = data\n        self.f_alpha = f_alpha\n        self.pcmci_alpha = pcmci_alpha\n        self.min_lag = min_lag\n        self.max_lag = max_lag\n        self.sel_method = sel_method\n        self.filter_dependencies = None\n        self.o_filter_dependencies = None\n        self.causal_model = None\n        self.result = None\n        self.neglect_only_autodep = neglect_only_autodep\n\n        self.dependency_path = None\n        if resfolder is not None:\n            utils.create_results_folder()\n            logpath, self.dependency_path = utils.get_selectorpath(resfolder)\n            sys.stdout = Logger(logpath)\n\n        self.validator = PCMCI(data, self.pcmci_alpha, min_lag, max_lag, val_condtest, resfolder, verbosity)       \n        CP.set_verbosity(verbosity)\n\n\n    def run_filter(self):\n\"\"\"\n        Run filter method\n        \"\"\"\n        CP.info(\"\\n\")\n        CP.info(DASH)\n        CP.info(\"Selecting relevant features among: \" + str(self.data.features))\n        CP.info(\"Selection method: \" + self.sel_method.name)\n        CP.info(\"Significance level: \" + str(self.f_alpha))\n        CP.info(\"Max lag time: \" + str(self.max_lag))\n        CP.info(\"Min lag time: \" + str(self.min_lag))\n        CP.info(\"Data length: \" + str(self.data.T))\n\n        self.sel_method.initialise(self.data, self.f_alpha, self.min_lag, self.max_lag)\n        self.filter_dependencies = self.sel_method.compute_dependencies()\n        self.o_filter_dependecies = copy.deepcopy(self.filter_dependencies)\n\n\n    def run_pcmci(self):\n\"\"\"\n        Run PCMCI\n\n        Returns:\n            list(str): list of selected variable names\n            dict(str:list(tuple)): causal model\n        \"\"\"\n        CP.info(\"Significance level: \" + str(self.pcmci_alpha))\n        CP.info(\"Max lag time: \" + str(self.max_lag))\n        CP.info(\"Min lag time: \" + str(self.min_lag))\n        CP.info(\"Data length: \" + str(self.data.T))\n\n        # causal model\n        self.validator.data = self.data\n        self.validator.run()\n        self.causal_model = self.validator.dependencies\n\n        self.result = self.data.features\n\n        self.save_validator_res()\n\n        return self.result, self.causal_model\n\n\n    def run(self):\n\"\"\"\n        Run Selector and Validator\n\n        Returns:\n            list(str): list of selected variable names\n            dict(str:list(tuple)): causal model\n        \"\"\"\n\n        self.run_filter()        \n\n        # list of selected features based on dependencies\n        tmp_sel_features = self.get_selected_features()\n        if not tmp_sel_features:\n            return self.result\n\n        # shrink dataframe d and dependencies by the selector result\n        self.shrink(tmp_sel_features)\n\n        # selected links to check by the validator\n        link_assumptions = self.__get_link_assumptions()\n\n        # causal model on selected links\n        self.validator.data = self.data\n        pcmci_result = self.validator.run(link_assumptions)\n\n        # application of the validator result to the filter_dependencies field\n        self.__apply_validator_result(pcmci_result)\n\n        self.result = self.get_selected_features()\n        # shrink dataframe d and dependencies by the validator result\n        self.shrink(self.result)\n\n        # final causal model\n        self.causal_model = self.validator.dependencies\n        self.save_validator_res()\n\n        CP.info(\"\\nFeature selected: \" + str(self.result))\n        return self.result, self.causal_model\n\n\n    def get_causal_matrix(self):\n\"\"\"\n        Returns a dictionary with keys the lags and values the causal matrix containing the causal weights between targets (rows) and sources (columns)\n\n        Returns:\n            dict/np.ndarray: causal matrix per \n        \"\"\"\n        cm_per_lag = {lag : np.zeros((len(self.result), len(self.result))) for lag in range(self.min_lag, self.max_lag + 1)}\n        vars = ['$' + var +'$' for var in self.result]\n        for lag in cm_per_lag:\n            for var in vars:\n                for source in self.causal_model[var]:\n                    if source[LAG] == lag: cm_per_lag[lag][vars.index(var)][vars.index(source[SOURCE])] = source[SCORE]\n        if len(cm_per_lag) == 1: return list(cm_per_lag.values())[0]\n        return cm_per_lag\n\n\n    def get_SCM(self):\n\"\"\"\n        Return Structural Causal Model\n\n        Raises:\n            ValueError: \"Causal Model not estimated yet\" if self.causal_model is None\n\n        Returns:\n            dict(str:list(tuple)): SCM of the causal model in the format \"target\": [(source, lag) ...] (e.g. \"$X0$\" : [(\"$X0$\", -1), (\"$X1$\", -2)]) \n        \"\"\"\n        if self.causal_model is None:\n            raise ValueError(\"Causal Model not estimated yet.\")\n        scm = {v: list() for v in self.data.pretty_features}\n        for t in self.causal_model.keys():\n            for s in self.causal_model[t]:\n                scm[t].append((s[SOURCE], -s[LAG])) \n        return scm\n\n\n    def shrink(self, sel_features):\n\"\"\"\n        Wrapper in order to shrink data.d and dependencies\n\n        Args:\n            sel_features (list(str)): list of selected features\n        \"\"\"\n        self.data.shrink(sel_features)\n        self.__shrink_dependencies()\n\n\n    def save_validator_res(self):\n\"\"\"\n        Saves dag plot if resfolder has been set otherwise it shows the figure\n        \"\"\"\n        if self.result:\n            self.validator.save_result()\n        else:\n            CP.warning(\"Result impossible to save: no feature selected\")\n\n\n    def dag(self,\n            node_layout = 'dot',\n            min_width = 1,\n            max_width = 5,\n            min_score = 0,\n            max_score = 1,\n            node_size = 8,\n            node_color = 'orange',\n            edge_color = 'grey',\n            font_size = 12,\n            label_type = LabelType.Lag):\n\"\"\"\n        Saves dag plot if resfolder has been set otherwise it shows the figure\n\n        Args:\n            node_layout (str, optional): Node layout. Defaults to 'dot'.\n            min_width (int, optional): minimum linewidth. Defaults to 1.\n            max_width (int, optional): maximum linewidth. Defaults to 5.\n            min_score (int, optional): minimum score range. Defaults to 0.\n            max_score (int, optional): maximum score range. Defaults to 1.\n            node_size (int, optional): node size. Defaults to 8.\n            node_color (str, optional): node color. Defaults to 'orange'.\n            edge_color (str, optional): edge color. Defaults to 'grey'.\n            font_size (int, optional): font size. Defaults to 12.\n            label_type (LabelType, optional): enum to set whether to show the lag time (LabelType.Lag) or the strength (LabelType.Score) of the dependencies on each link/node or not showing the labels (LabelType.NoLabels). Default LabelType.Lag.\n        \"\"\"\n\n        if self.result:\n            self.validator.build_dag(node_layout,\n                                     min_width, \n                                     max_width,\n                                     min_score,\n                                     max_score,\n                                     node_size,\n                                     node_color,\n                                     edge_color,\n                                     font_size,\n                                     label_type)\n        else:\n            CP.warning(\"Dag impossible to create: no feature selected\")\n\n\n    def timeseries_dag(self,\n                       min_width = 1,\n                       max_width = 5,\n                       min_score = 0,\n                       max_score = 1,\n                       node_size = 8,\n                       font_size = 12,\n                       node_color = 'orange',\n                       edge_color = 'grey'):\n\"\"\"\n        Saves timeseries dag plot if resfolder has been set otherwise it shows the figure\n\n        Args:\n            min_width (int, optional): minimum linewidth. Defaults to 1.\n            max_width (int, optional): maximum linewidth. Defaults to 5.\n            min_score (int, optional): minimum score range. Defaults to 0.\n            max_score (int, optional): maximum score range. Defaults to 1.\n            node_size (int, optional): node size. Defaults to 8.\n            node_color (str, optional): node color. Defaults to 'orange'.\n            edge_color (str, optional): edge color. Defaults to 'grey'.\n            font_size (int, optional): font size. Defaults to 12.\n        \"\"\"\n\n        if self.result:\n            self.validator.build_ts_dag(min_width,\n                                        max_width,\n                                        min_score,\n                                        max_score,\n                                        node_size,\n                                        node_color,\n                                        edge_color,\n                                        font_size)\n        else:\n            CP.warning(\"Timeseries dag impossible to create: no feature selected\")\n\n\n    def get_selected_features(self):\n\"\"\"\n        Defines the list of selected variables for d\n\n        Returns:\n            list(str): list of selected variable names\n        \"\"\"\n        f_list = list()\n        for t in self.filter_dependencies:\n            sources_t = self.__get_dependencies_for_target(t)\n            if self.neglect_only_autodep and self.__is_only_autodep(sources_t, t):\n                sources_t.remove(t)\n            if sources_t: sources_t.append(t)\n            f_list = list(set(f_list + sources_t))\n        res = [f for f in self.data.features if f in f_list]\n\n        return res\n\n\n    def show_dependencies(self):\n\"\"\"\n        Saves dependencies graph if resfolder is set otherwise it shows the figure\n        \"\"\"\n        # FIXME: LAG not considered\n        dependencies_matrix = self.__get_dependencies_matrix()\n\n        fig, ax = plt.subplots()\n        im = ax.imshow(dependencies_matrix, cmap=plt.cm.Greens, interpolation='nearest', vmin=0, vmax=1, origin='lower')\n        fig.colorbar(im, orientation='vertical', label=\"score\")\n\n        plt.xlabel(\"Sources\")\n        plt.ylabel(\"Targets\")\n        plt.xticks(ticks = range(0, self.data.orig_N), labels = self.data.orig_pretty_features, fontsize = 8)\n        plt.yticks(ticks = range(0, self.data.orig_N), labels = self.data.orig_pretty_features, fontsize = 8)\n        plt.title(\"Dependencies\")\n\n        if self.dependency_path is not None:\n            plt.savefig(self.dependency_path, dpi = 300)\n        else:\n            plt.show()\n\n\n    def print_dependencies(self):\n\"\"\"\n        Print dependencies found by the selector\n        \"\"\"\n        for t in self.o_filter_dependecies:\n            print()\n            print()\n            print(DASH)\n            print(\"Target\", t)\n            print(DASH)\n            print('{:&lt;10s}{:&gt;15s}{:&gt;15s}{:&gt;15s}'.format('SOURCE', 'SCORE', 'PVAL', 'LAG'))\n            print(DASH)\n            for s in self.o_filter_dependecies[t]:\n                print('{:&lt;10s}{:&gt;15.3f}{:&gt;15.3f}{:&gt;15d}'.format(s[SOURCE], s[SCORE], s[PVAL], s[LAG]))      \n\n\n    def load_result(self, res_path):\n        with open(res_path, 'rb') as f:\n            self.validator.result = pickle.load(f)\n\n\n    def __shrink_dependencies(self):\n\"\"\"\n        Shrinks dependencies based on the selected features\n        \"\"\"\n        difference_set = self.filter_dependencies.keys() - self.data.features\n        for d in difference_set: \n            del self.filter_dependencies[d]\n            if self.validator.dependencies is not None: del self.validator.dependencies['$' + d + '$']\n\n\n    def __get_dependencies_for_target(self, t):\n\"\"\"\n        Returns list of sources for a specified target\n\n        Args:\n            t (str): target variable name\n\n        Returns:\n            list(str): list of sources for target t\n        \"\"\"\n        return [s[SOURCE] for s in self.filter_dependencies[t]]\n\n\n    def __is_only_autodep(self, sources, t):\n\"\"\"\n        Returns list of sources for a specified target\n\n        Args:\n            sources (list(str)): list of sources for the selected target\n            t (str): target variable name\n\n        Returns:\n            (bool): True if sources list contains only the target. False otherwise\n        \"\"\"\n        if len(sources) == 1 and sources[0] == t: return True\n        return False\n\n\n    def __get_dependencies_matrix(self):\n\"\"\"\n        Returns a matrix composed by scores for each target\n\n        Returns:\n            (np.array): score matrix\n        \"\"\"\n        dep_mat = list()\n        for t in self.o_filter_dependecies:\n            dep_vet = [0] * self.data.orig_N\n            for s in self.o_filter_dependecies[t]:\n                dep_vet[self.data.orig_features.index(s[SOURCE])] = s[SCORE]\n            dep_mat.append(dep_vet)\n\n        dep_mat = np.array(dep_mat)\n        inf_mask = np.isinf(dep_mat)\n        neginf_mask = np.isneginf(dep_mat)\n        max_dep_mat = np.max(dep_mat[(dep_mat != -np.inf) &amp; (dep_mat != np.inf)])\n        min_dep_mat = np.min(dep_mat[(dep_mat != -np.inf) &amp; (dep_mat != np.inf)])\n\n        dep_mat[inf_mask] = max_dep_mat\n        dep_mat[neginf_mask] = min_dep_mat\n        dep_mat = (dep_mat - min_dep_mat) / (max_dep_mat - min_dep_mat)\n        return dep_mat\n\n\n    def __get_link_assumptions(self):\n\"\"\"\n        Return selected links found by the selector\n        in this form: {0: {(0,-1) : \"-?&gt;\", (2,-1) : \"-?&gt;\"}}\n\n        Returns:\n            (dict): selected links\n        \"\"\"\n        sel_links = {self.data.features.index(f):dict() for f in self.data.features}\n        for t in self.filter_dependencies:\n\n            # add links\n            for s in self.filter_dependencies[t]:\n                sel_links[self.data.features.index(t)][(self.data.features.index(s[SOURCE]), -s[LAG])] = '-?&gt;'\n\n        return sel_links\n\n\n    def __apply_validator_result(self, causal_model):\n\"\"\"\n        Exclude dependencies based on validator result\n        It does not overwrite the filter_dependencies' inference/p-values matrix with the ones found by the validator\n        \"\"\"\n        list_diffs = list()\n        tmp_dependencies = copy.deepcopy(self.filter_dependencies)\n        for t in tmp_dependencies:\n            for s in tmp_dependencies[t]:\n                if (self.data.features.index(s[SOURCE]), -s[LAG]) not in causal_model[self.data.features.index(t)]:\n                    list_diffs.append((s[SOURCE], str(s[LAG]), t))\n                    self.filter_dependencies[t].remove(s)\n        if list_diffs:\n            CP.debug(DASH)\n            CP.debug(\"Difference(s)\")\n            CP.debug(DASH)\n            for diff in list_diffs:\n                CP.debug(\"Removing (\" + diff[0] + \" -\" + diff[1] +\") --&gt; (\" + diff[2] + \")\")\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.__apply_validator_result","title":"<code>__apply_validator_result(causal_model)</code>","text":"<p>Exclude dependencies based on validator result It does not overwrite the filter_dependencies' inference/p-values matrix with the ones found by the validator</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def __apply_validator_result(self, causal_model):\n\"\"\"\n    Exclude dependencies based on validator result\n    It does not overwrite the filter_dependencies' inference/p-values matrix with the ones found by the validator\n    \"\"\"\n    list_diffs = list()\n    tmp_dependencies = copy.deepcopy(self.filter_dependencies)\n    for t in tmp_dependencies:\n        for s in tmp_dependencies[t]:\n            if (self.data.features.index(s[SOURCE]), -s[LAG]) not in causal_model[self.data.features.index(t)]:\n                list_diffs.append((s[SOURCE], str(s[LAG]), t))\n                self.filter_dependencies[t].remove(s)\n    if list_diffs:\n        CP.debug(DASH)\n        CP.debug(\"Difference(s)\")\n        CP.debug(DASH)\n        for diff in list_diffs:\n            CP.debug(\"Removing (\" + diff[0] + \" -\" + diff[1] +\") --&gt; (\" + diff[2] + \")\")\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.__get_dependencies_for_target","title":"<code>__get_dependencies_for_target(t)</code>","text":"<p>Returns list of sources for a specified target</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>str</code> <p>target variable name</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>str</code> <p>list of sources for target t</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def __get_dependencies_for_target(self, t):\n\"\"\"\n    Returns list of sources for a specified target\n\n    Args:\n        t (str): target variable name\n\n    Returns:\n        list(str): list of sources for target t\n    \"\"\"\n    return [s[SOURCE] for s in self.filter_dependencies[t]]\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.__get_dependencies_matrix","title":"<code>__get_dependencies_matrix()</code>","text":"<p>Returns a matrix composed by scores for each target</p> <p>Returns:</p> Type Description <code>np.array</code> <p>score matrix</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def __get_dependencies_matrix(self):\n\"\"\"\n    Returns a matrix composed by scores for each target\n\n    Returns:\n        (np.array): score matrix\n    \"\"\"\n    dep_mat = list()\n    for t in self.o_filter_dependecies:\n        dep_vet = [0] * self.data.orig_N\n        for s in self.o_filter_dependecies[t]:\n            dep_vet[self.data.orig_features.index(s[SOURCE])] = s[SCORE]\n        dep_mat.append(dep_vet)\n\n    dep_mat = np.array(dep_mat)\n    inf_mask = np.isinf(dep_mat)\n    neginf_mask = np.isneginf(dep_mat)\n    max_dep_mat = np.max(dep_mat[(dep_mat != -np.inf) &amp; (dep_mat != np.inf)])\n    min_dep_mat = np.min(dep_mat[(dep_mat != -np.inf) &amp; (dep_mat != np.inf)])\n\n    dep_mat[inf_mask] = max_dep_mat\n    dep_mat[neginf_mask] = min_dep_mat\n    dep_mat = (dep_mat - min_dep_mat) / (max_dep_mat - min_dep_mat)\n    return dep_mat\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.__get_link_assumptions","title":"<code>__get_link_assumptions()</code>","text":"<p>Return selected links found by the selector in this form: {0: {(0,-1) : \"-?&gt;\", (2,-1) : \"-?&gt;\"}}</p> <p>Returns:</p> Type Description <code>dict</code> <p>selected links</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def __get_link_assumptions(self):\n\"\"\"\n    Return selected links found by the selector\n    in this form: {0: {(0,-1) : \"-?&gt;\", (2,-1) : \"-?&gt;\"}}\n\n    Returns:\n        (dict): selected links\n    \"\"\"\n    sel_links = {self.data.features.index(f):dict() for f in self.data.features}\n    for t in self.filter_dependencies:\n\n        # add links\n        for s in self.filter_dependencies[t]:\n            sel_links[self.data.features.index(t)][(self.data.features.index(s[SOURCE]), -s[LAG])] = '-?&gt;'\n\n    return sel_links\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.__init__","title":"<code>__init__(data, min_lag, max_lag, sel_method, val_condtest, verbosity, f_alpha=0.05, pcmci_alpha=0.05, resfolder=None, neglect_only_autodep=False)</code>","text":"<p>FPCMCI class contructor</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag</p> required <code>sel_method</code> <code>SelectionMethod</code> <p>selection method</p> required <code>val_condtest</code> <code>CondIndTest</code> <p>validation method</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level</p> required <code>f_alpha</code> <code>float</code> <p>filter significance level. Defaults to 0.05.</p> <code>0.05</code> <code>pcmci_alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def __init__(self, \n             data: Data, \n             min_lag, max_lag, \n             sel_method: SelectionMethod, val_condtest: CondIndTest, \n             verbosity: CPLevel, \n             f_alpha = 0.05, \n             pcmci_alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False):\n\"\"\"\n    FPCMCI class contructor\n\n    Args:\n        data (Data): data to analyse\n        min_lag (int): minimum time lag\n        max_lag (int): maximum time lag\n        sel_method (SelectionMethod): selection method\n        val_condtest (CondIndTest): validation method\n        verbosity (CPLevel): verbosity level\n        f_alpha (float, optional): filter significance level. Defaults to 0.05.\n        pcmci_alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n    \"\"\"\n\n    self.data = data\n    self.f_alpha = f_alpha\n    self.pcmci_alpha = pcmci_alpha\n    self.min_lag = min_lag\n    self.max_lag = max_lag\n    self.sel_method = sel_method\n    self.filter_dependencies = None\n    self.o_filter_dependencies = None\n    self.causal_model = None\n    self.result = None\n    self.neglect_only_autodep = neglect_only_autodep\n\n    self.dependency_path = None\n    if resfolder is not None:\n        utils.create_results_folder()\n        logpath, self.dependency_path = utils.get_selectorpath(resfolder)\n        sys.stdout = Logger(logpath)\n\n    self.validator = PCMCI(data, self.pcmci_alpha, min_lag, max_lag, val_condtest, resfolder, verbosity)       \n    CP.set_verbosity(verbosity)\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.__is_only_autodep","title":"<code>__is_only_autodep(sources, t)</code>","text":"<p>Returns list of sources for a specified target</p> <p>Parameters:</p> Name Type Description Default <code>sources</code> <code>list(str</code> <p>list of sources for the selected target</p> required <code>t</code> <code>str</code> <p>target variable name</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if sources list contains only the target. False otherwise</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def __is_only_autodep(self, sources, t):\n\"\"\"\n    Returns list of sources for a specified target\n\n    Args:\n        sources (list(str)): list of sources for the selected target\n        t (str): target variable name\n\n    Returns:\n        (bool): True if sources list contains only the target. False otherwise\n    \"\"\"\n    if len(sources) == 1 and sources[0] == t: return True\n    return False\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.__shrink_dependencies","title":"<code>__shrink_dependencies()</code>","text":"<p>Shrinks dependencies based on the selected features</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def __shrink_dependencies(self):\n\"\"\"\n    Shrinks dependencies based on the selected features\n    \"\"\"\n    difference_set = self.filter_dependencies.keys() - self.data.features\n    for d in difference_set: \n        del self.filter_dependencies[d]\n        if self.validator.dependencies is not None: del self.validator.dependencies['$' + d + '$']\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.dag","title":"<code>dag(node_layout='dot', min_width=1, max_width=5, min_score=0, max_score=1, node_size=8, node_color='orange', edge_color='grey', font_size=12, label_type=LabelType.Lag)</code>","text":"<p>Saves dag plot if resfolder has been set otherwise it shows the figure</p> <p>Parameters:</p> Name Type Description Default <code>node_layout</code> <code>str</code> <p>Node layout. Defaults to 'dot'.</p> <code>'dot'</code> <code>min_width</code> <code>int</code> <p>minimum linewidth. Defaults to 1.</p> <code>1</code> <code>max_width</code> <code>int</code> <p>maximum linewidth. Defaults to 5.</p> <code>5</code> <code>min_score</code> <code>int</code> <p>minimum score range. Defaults to 0.</p> <code>0</code> <code>max_score</code> <code>int</code> <p>maximum score range. Defaults to 1.</p> <code>1</code> <code>node_size</code> <code>int</code> <p>node size. Defaults to 8.</p> <code>8</code> <code>node_color</code> <code>str</code> <p>node color. Defaults to 'orange'.</p> <code>'orange'</code> <code>edge_color</code> <code>str</code> <p>edge color. Defaults to 'grey'.</p> <code>'grey'</code> <code>font_size</code> <code>int</code> <p>font size. Defaults to 12.</p> <code>12</code> <code>label_type</code> <code>LabelType</code> <p>enum to set whether to show the lag time (LabelType.Lag) or the strength (LabelType.Score) of the dependencies on each link/node or not showing the labels (LabelType.NoLabels). Default LabelType.Lag.</p> <code>LabelType.Lag</code> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def dag(self,\n        node_layout = 'dot',\n        min_width = 1,\n        max_width = 5,\n        min_score = 0,\n        max_score = 1,\n        node_size = 8,\n        node_color = 'orange',\n        edge_color = 'grey',\n        font_size = 12,\n        label_type = LabelType.Lag):\n\"\"\"\n    Saves dag plot if resfolder has been set otherwise it shows the figure\n\n    Args:\n        node_layout (str, optional): Node layout. Defaults to 'dot'.\n        min_width (int, optional): minimum linewidth. Defaults to 1.\n        max_width (int, optional): maximum linewidth. Defaults to 5.\n        min_score (int, optional): minimum score range. Defaults to 0.\n        max_score (int, optional): maximum score range. Defaults to 1.\n        node_size (int, optional): node size. Defaults to 8.\n        node_color (str, optional): node color. Defaults to 'orange'.\n        edge_color (str, optional): edge color. Defaults to 'grey'.\n        font_size (int, optional): font size. Defaults to 12.\n        label_type (LabelType, optional): enum to set whether to show the lag time (LabelType.Lag) or the strength (LabelType.Score) of the dependencies on each link/node or not showing the labels (LabelType.NoLabels). Default LabelType.Lag.\n    \"\"\"\n\n    if self.result:\n        self.validator.build_dag(node_layout,\n                                 min_width, \n                                 max_width,\n                                 min_score,\n                                 max_score,\n                                 node_size,\n                                 node_color,\n                                 edge_color,\n                                 font_size,\n                                 label_type)\n    else:\n        CP.warning(\"Dag impossible to create: no feature selected\")\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.get_SCM","title":"<code>get_SCM()</code>","text":"<p>Return Structural Causal Model</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>\"Causal Model not estimated yet\" if self.causal_model is None</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>str:list(tuple)</code> <p>SCM of the causal model in the format \"target\": [(source, lag) ...] (e.g. \"$X0$\" : [(\"$X0$\", -1), (\"$X1$\", -2)])</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def get_SCM(self):\n\"\"\"\n    Return Structural Causal Model\n\n    Raises:\n        ValueError: \"Causal Model not estimated yet\" if self.causal_model is None\n\n    Returns:\n        dict(str:list(tuple)): SCM of the causal model in the format \"target\": [(source, lag) ...] (e.g. \"$X0$\" : [(\"$X0$\", -1), (\"$X1$\", -2)]) \n    \"\"\"\n    if self.causal_model is None:\n        raise ValueError(\"Causal Model not estimated yet.\")\n    scm = {v: list() for v in self.data.pretty_features}\n    for t in self.causal_model.keys():\n        for s in self.causal_model[t]:\n            scm[t].append((s[SOURCE], -s[LAG])) \n    return scm\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.get_causal_matrix","title":"<code>get_causal_matrix()</code>","text":"<p>Returns a dictionary with keys the lags and values the causal matrix containing the causal weights between targets (rows) and sources (columns)</p> <p>Returns:</p> Type Description <p>dict/np.ndarray: causal matrix per</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def get_causal_matrix(self):\n\"\"\"\n    Returns a dictionary with keys the lags and values the causal matrix containing the causal weights between targets (rows) and sources (columns)\n\n    Returns:\n        dict/np.ndarray: causal matrix per \n    \"\"\"\n    cm_per_lag = {lag : np.zeros((len(self.result), len(self.result))) for lag in range(self.min_lag, self.max_lag + 1)}\n    vars = ['$' + var +'$' for var in self.result]\n    for lag in cm_per_lag:\n        for var in vars:\n            for source in self.causal_model[var]:\n                if source[LAG] == lag: cm_per_lag[lag][vars.index(var)][vars.index(source[SOURCE])] = source[SCORE]\n    if len(cm_per_lag) == 1: return list(cm_per_lag.values())[0]\n    return cm_per_lag\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.get_selected_features","title":"<code>get_selected_features()</code>","text":"<p>Defines the list of selected variables for d</p> <p>Returns:</p> Name Type Description <code>list</code> <code>str</code> <p>list of selected variable names</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def get_selected_features(self):\n\"\"\"\n    Defines the list of selected variables for d\n\n    Returns:\n        list(str): list of selected variable names\n    \"\"\"\n    f_list = list()\n    for t in self.filter_dependencies:\n        sources_t = self.__get_dependencies_for_target(t)\n        if self.neglect_only_autodep and self.__is_only_autodep(sources_t, t):\n            sources_t.remove(t)\n        if sources_t: sources_t.append(t)\n        f_list = list(set(f_list + sources_t))\n    res = [f for f in self.data.features if f in f_list]\n\n    return res\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.print_dependencies","title":"<code>print_dependencies()</code>","text":"<p>Print dependencies found by the selector</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def print_dependencies(self):\n\"\"\"\n    Print dependencies found by the selector\n    \"\"\"\n    for t in self.o_filter_dependecies:\n        print()\n        print()\n        print(DASH)\n        print(\"Target\", t)\n        print(DASH)\n        print('{:&lt;10s}{:&gt;15s}{:&gt;15s}{:&gt;15s}'.format('SOURCE', 'SCORE', 'PVAL', 'LAG'))\n        print(DASH)\n        for s in self.o_filter_dependecies[t]:\n            print('{:&lt;10s}{:&gt;15.3f}{:&gt;15.3f}{:&gt;15d}'.format(s[SOURCE], s[SCORE], s[PVAL], s[LAG]))      \n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.run","title":"<code>run()</code>","text":"<p>Run Selector and Validator</p> <p>Returns:</p> Name Type Description <code>list</code> <code>str</code> <p>list of selected variable names</p> <code>dict</code> <code>str:list(tuple)</code> <p>causal model</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def run(self):\n\"\"\"\n    Run Selector and Validator\n\n    Returns:\n        list(str): list of selected variable names\n        dict(str:list(tuple)): causal model\n    \"\"\"\n\n    self.run_filter()        \n\n    # list of selected features based on dependencies\n    tmp_sel_features = self.get_selected_features()\n    if not tmp_sel_features:\n        return self.result\n\n    # shrink dataframe d and dependencies by the selector result\n    self.shrink(tmp_sel_features)\n\n    # selected links to check by the validator\n    link_assumptions = self.__get_link_assumptions()\n\n    # causal model on selected links\n    self.validator.data = self.data\n    pcmci_result = self.validator.run(link_assumptions)\n\n    # application of the validator result to the filter_dependencies field\n    self.__apply_validator_result(pcmci_result)\n\n    self.result = self.get_selected_features()\n    # shrink dataframe d and dependencies by the validator result\n    self.shrink(self.result)\n\n    # final causal model\n    self.causal_model = self.validator.dependencies\n    self.save_validator_res()\n\n    CP.info(\"\\nFeature selected: \" + str(self.result))\n    return self.result, self.causal_model\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.run_filter","title":"<code>run_filter()</code>","text":"<p>Run filter method</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def run_filter(self):\n\"\"\"\n    Run filter method\n    \"\"\"\n    CP.info(\"\\n\")\n    CP.info(DASH)\n    CP.info(\"Selecting relevant features among: \" + str(self.data.features))\n    CP.info(\"Selection method: \" + self.sel_method.name)\n    CP.info(\"Significance level: \" + str(self.f_alpha))\n    CP.info(\"Max lag time: \" + str(self.max_lag))\n    CP.info(\"Min lag time: \" + str(self.min_lag))\n    CP.info(\"Data length: \" + str(self.data.T))\n\n    self.sel_method.initialise(self.data, self.f_alpha, self.min_lag, self.max_lag)\n    self.filter_dependencies = self.sel_method.compute_dependencies()\n    self.o_filter_dependecies = copy.deepcopy(self.filter_dependencies)\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.run_pcmci","title":"<code>run_pcmci()</code>","text":"<p>Run PCMCI</p> <p>Returns:</p> Name Type Description <code>list</code> <code>str</code> <p>list of selected variable names</p> <code>dict</code> <code>str:list(tuple)</code> <p>causal model</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def run_pcmci(self):\n\"\"\"\n    Run PCMCI\n\n    Returns:\n        list(str): list of selected variable names\n        dict(str:list(tuple)): causal model\n    \"\"\"\n    CP.info(\"Significance level: \" + str(self.pcmci_alpha))\n    CP.info(\"Max lag time: \" + str(self.max_lag))\n    CP.info(\"Min lag time: \" + str(self.min_lag))\n    CP.info(\"Data length: \" + str(self.data.T))\n\n    # causal model\n    self.validator.data = self.data\n    self.validator.run()\n    self.causal_model = self.validator.dependencies\n\n    self.result = self.data.features\n\n    self.save_validator_res()\n\n    return self.result, self.causal_model\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.save_validator_res","title":"<code>save_validator_res()</code>","text":"<p>Saves dag plot if resfolder has been set otherwise it shows the figure</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def save_validator_res(self):\n\"\"\"\n    Saves dag plot if resfolder has been set otherwise it shows the figure\n    \"\"\"\n    if self.result:\n        self.validator.save_result()\n    else:\n        CP.warning(\"Result impossible to save: no feature selected\")\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.show_dependencies","title":"<code>show_dependencies()</code>","text":"<p>Saves dependencies graph if resfolder is set otherwise it shows the figure</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def show_dependencies(self):\n\"\"\"\n    Saves dependencies graph if resfolder is set otherwise it shows the figure\n    \"\"\"\n    # FIXME: LAG not considered\n    dependencies_matrix = self.__get_dependencies_matrix()\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(dependencies_matrix, cmap=plt.cm.Greens, interpolation='nearest', vmin=0, vmax=1, origin='lower')\n    fig.colorbar(im, orientation='vertical', label=\"score\")\n\n    plt.xlabel(\"Sources\")\n    plt.ylabel(\"Targets\")\n    plt.xticks(ticks = range(0, self.data.orig_N), labels = self.data.orig_pretty_features, fontsize = 8)\n    plt.yticks(ticks = range(0, self.data.orig_N), labels = self.data.orig_pretty_features, fontsize = 8)\n    plt.title(\"Dependencies\")\n\n    if self.dependency_path is not None:\n        plt.savefig(self.dependency_path, dpi = 300)\n    else:\n        plt.show()\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.shrink","title":"<code>shrink(sel_features)</code>","text":"<p>Wrapper in order to shrink data.d and dependencies</p> <p>Parameters:</p> Name Type Description Default <code>sel_features</code> <code>list(str</code> <p>list of selected features</p> required Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def shrink(self, sel_features):\n\"\"\"\n    Wrapper in order to shrink data.d and dependencies\n\n    Args:\n        sel_features (list(str)): list of selected features\n    \"\"\"\n    self.data.shrink(sel_features)\n    self.__shrink_dependencies()\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.timeseries_dag","title":"<code>timeseries_dag(min_width=1, max_width=5, min_score=0, max_score=1, node_size=8, font_size=12, node_color='orange', edge_color='grey')</code>","text":"<p>Saves timeseries dag plot if resfolder has been set otherwise it shows the figure</p> <p>Parameters:</p> Name Type Description Default <code>min_width</code> <code>int</code> <p>minimum linewidth. Defaults to 1.</p> <code>1</code> <code>max_width</code> <code>int</code> <p>maximum linewidth. Defaults to 5.</p> <code>5</code> <code>min_score</code> <code>int</code> <p>minimum score range. Defaults to 0.</p> <code>0</code> <code>max_score</code> <code>int</code> <p>maximum score range. Defaults to 1.</p> <code>1</code> <code>node_size</code> <code>int</code> <p>node size. Defaults to 8.</p> <code>8</code> <code>node_color</code> <code>str</code> <p>node color. Defaults to 'orange'.</p> <code>'orange'</code> <code>edge_color</code> <code>str</code> <p>edge color. Defaults to 'grey'.</p> <code>'grey'</code> <code>font_size</code> <code>int</code> <p>font size. Defaults to 12.</p> <code>12</code> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def timeseries_dag(self,\n                   min_width = 1,\n                   max_width = 5,\n                   min_score = 0,\n                   max_score = 1,\n                   node_size = 8,\n                   font_size = 12,\n                   node_color = 'orange',\n                   edge_color = 'grey'):\n\"\"\"\n    Saves timeseries dag plot if resfolder has been set otherwise it shows the figure\n\n    Args:\n        min_width (int, optional): minimum linewidth. Defaults to 1.\n        max_width (int, optional): maximum linewidth. Defaults to 5.\n        min_score (int, optional): minimum score range. Defaults to 0.\n        max_score (int, optional): maximum score range. Defaults to 1.\n        node_size (int, optional): node size. Defaults to 8.\n        node_color (str, optional): node color. Defaults to 'orange'.\n        edge_color (str, optional): edge color. Defaults to 'grey'.\n        font_size (int, optional): font size. Defaults to 12.\n    \"\"\"\n\n    if self.result:\n        self.validator.build_ts_dag(min_width,\n                                    max_width,\n                                    min_score,\n                                    max_score,\n                                    node_size,\n                                    node_color,\n                                    edge_color,\n                                    font_size)\n    else:\n        CP.warning(\"Timeseries dag impossible to create: no feature selected\")\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI","title":"<code>PCMCI</code>","text":"<p>PCMCI class.</p> <p>PCMCI works with FSelector in order to find the causal  model starting from a prefixed set of variables and links.</p> Source code in <code>fpcmci/PCMCI.py</code> <pre><code>class PCMCI():\n\"\"\"\n    PCMCI class.\n\n    PCMCI works with FSelector in order to find the causal \n    model starting from a prefixed set of variables and links.\n    \"\"\"\n    def __init__(self, data: Data, alpha, min_lag, max_lag, val_condtest: CondIndTest, resfolder, verbosity: CPLevel):\n\"\"\"\n        PCMCI class constructor\n\n        Args:\n            data (Data): data to analyse\n            alpha (float): significance level\n            min_lag (int): minimum time lag\n            max_lag (int): maximum time lag\n            val_condtest (CondIndTest): validation method\n            resfolder (str): result folder. If None then the results are not saved.\n            verbosity (CPLevel): verbosity level\n        \"\"\"\n        self.data = data\n        self.alpha = alpha\n        self.min_lag = min_lag\n        self.max_lag = max_lag\n        self.result = None\n        self.dependencies = None\n        self.val_method = None\n        self.val_condtest = val_condtest\n        self.verbosity = verbosity.value\n\n        self.respath = None\n        self.dag_path = None\n        self.ts_dag_path = None\n        if resfolder is not None:\n            self.respath, self.dag_path, self.ts_dag_path = utils.get_validatorpaths(resfolder)  \n\n\n    def run(self, link_assumptions = None):\n\"\"\"\n        Run causal discovery algorithm\n\n        Returns:\n            (dict): estimated causal model\n        \"\"\"\n        CP.info('\\n')\n        CP.info(DASH)\n        CP.info(\"Running Causal Discovery Algorithm\")\n\n        # build tigramite dataset\n        vector = np.vectorize(float)\n        data = vector(self.data.d)\n        dataframe = pp.DataFrame(data = data,\n                                 var_names = self.data.pretty_features)\n\n        # init and run pcmci\n        self.val_method = VAL(dataframe = dataframe,\n                                cond_ind_test = self.val_condtest,\n                                verbosity = self.verbosity)\n\n        self.result = self.val_method.run_pcmci(link_assumptions = link_assumptions,\n                                                tau_max = self.max_lag,\n                                                tau_min = self.min_lag)\n\n        self.result['var_names'] = self.data.pretty_features\n        # apply significance level\n        self.result['graph'] = self.__apply_alpha()\n        self.dependencies = self.__PCMCIres_converter()\n        return self.__return_parents_dict()\n\n\n    def build_ts_dag(self,\n                     min_width,\n                     max_width,\n                     min_score,\n                     max_score,\n                     node_size,\n                     node_color,\n                     edge_color,\n                     font_size):\n\"\"\"\n        Saves timeseries dag plot if resfolder is set otherwise it shows the figure\n\n        Args:\n            min_width (int): minimum linewidt\n            max_width (int): maximum linewidth\n            min_score (int): minimum score range\n            max_score (int): maximum score range\n            node_size (int): node size\n            node_color (str): node color\n            edge_color (str): edge color \n            font_size (int): font size\n        \"\"\"\n\n        # # convert to dictionary\n        # res = self.__PCMCIres_converter()\n\n        # # filter only dependencies\n        # tmp_res = {k: res[k] for k in self.data.pretty_features}\n        # res = tmp_res\n\n        ts_dag(self.dependencies, \n               tau = self.max_lag,\n               min_width = min_width,\n               max_width = max_width,\n               min_score = min_score,\n               max_score = max_score,\n               node_size = node_size,\n               node_color = node_color,\n               edge_color = edge_color,\n               font_size = font_size,\n               save_name = self.ts_dag_path)\n\n\n    def build_dag(self,\n                  node_layout,\n                  min_width,\n                  max_width,\n                  min_score,\n                  max_score,\n                  node_size,\n                  node_color,\n                  edge_color,\n                  font_size,\n                  label_type):\n\"\"\"\n        Saves dag plot if resfolder is set otherwise it shows the figure\n\n        Args:\n            node_layout (str): node_layout\n            min_width (int): minimum linewidth\n            max_width (int): maximum linewidth\n            min_score (int): minimum score range\n            max_score (int): maximum score range\n            node_size (int): node size\n            node_color (str): node color\n            edge_color (str): edge color\n            font_size (int): font size\n            label_type (LabelType, optional): enum to set whether to show the lag time (LabelType.Lag) or the strength (LabelType.Score) of the dependencies on each link/node or not showing the labels (LabelType.NoLabels). Default LabelType.Lag.\n\n        \"\"\"               \n\n        # # convert to dictionary\n        # res = self.__PCMCIres_converter()\n\n        # # filter only dependencies\n        # tmp_res = {k: res[k] for k in self.data.pretty_features}\n        # res = tmp_res\n\n        dag(self.dependencies,\n            node_layout = node_layout,\n            min_width = min_width,\n            max_width = max_width,\n            min_score = min_score,\n            max_score = max_score,\n            node_size = node_size,\n            font_size = font_size,\n            node_color = node_color,\n            edge_color = edge_color,\n            label_type = label_type,\n            save_name = self.dag_path)\n\n\n    def save_result(self):\n\"\"\"\n        Save causal discovery results as pickle file if resfolder is set\n        \"\"\"\n        if self.respath is not None:\n            res = dict()\n            res['dependencies'] = copy.deepcopy(self.dependencies)\n            # res = copy.deepcopy(self.result)\n            res['alpha'] = self.alpha\n            res['var_names'] = self.data.pretty_features\n            res['dag_path'] = self.dag_path\n            res['ts_dag_path'] = self.ts_dag_path\n            with open(self.respath, 'wb') as resfile:\n                pickle.dump(res, resfile)\n\n\n    def __return_parents_dict(self):\n\"\"\"\n        Returns dictionary of parents sorted by val_matrix filtered by alpha\n\n        Returns:\n            (dict): Dictionary of form {0:[(0, -1), (3, -2), ...], 1:[], ...} containing estimated parents.\n        \"\"\"\n        graph = self.result['graph']\n        val_matrix = self.result['val_matrix']\n        p_matrix = self.result['p_matrix']\n\n        # Initialize the return value\n        parents_dict = dict()\n        for j in range(self.data.N):\n            # Get the good links\n            good_links = np.argwhere(graph[:, j, 1:] == \"--&gt;\")\n            # Build a dictionary from these links to their values\n            links = {(i, -tau - 1): np.abs(val_matrix[i, j, abs(tau) + 1]) \n                     for i, tau in good_links if p_matrix[i, j, abs(tau) + 1] &lt;= self.alpha}\n            # Sort by value\n            parents_dict[j] = sorted(links, key=links.get, reverse=True)\n\n        return parents_dict\n\n\n    def __PCMCIres_converter(self):\n\"\"\"\n        Re-elaborates the PCMCI result in a new dictionary\n\n        Returns:\n            (dict): pcmci result re-elaborated\n        \"\"\"\n        res_dict = {f:list() for f in self.result['var_names']}\n        N, lags = self.result['graph'][0].shape\n        for s in range(len(self.result['graph'])):\n            for t in range(N):\n                for lag in range(lags):\n                    if self.result['graph'][s][t,lag] == '--&gt;':\n                        res_dict[self.result['var_names'][t]].append({SOURCE : self.result['var_names'][s],\n                                                                      SCORE : self.result['val_matrix'][s][t,lag],\n                                                                      PVAL : self.result['p_matrix'][s][t,lag],\n                                                                      LAG : lag})\n        return res_dict\n\n\n    def __apply_alpha(self):\n\"\"\"\n        Applies alpha threshold to the pcmci result\n\n        Returns:\n            (ndarray): graph filtered by alpha \n        \"\"\"\n        mask = np.ones(self.result['p_matrix'].shape, dtype='bool')\n\n        # Set all p-values of absent links to 1.\n        self.result['p_matrix'][mask==False] == 1.\n\n        # Threshold p_matrix to get graph\n        graph_bool = self.result['p_matrix'] &lt;= self.alpha\n\n        # Convert to string graph representation\n        graph = self.__convert_to_string_graph(graph_bool)\n\n        return graph\n\n\n    def __convert_to_string_graph(self, graph_bool):\n\"\"\"\n        Converts the 0,1-based graph returned by PCMCI to a string array\n        with links '--&gt;'\n\n        Args:\n            graph_bool (array): 0,1-based graph array output by PCMCI\n\n        Returns:\n            (array): graph as string array with links '--&gt;'.\n        \"\"\"\n\n        graph = np.zeros(graph_bool.shape, dtype='&lt;U3')\n        graph[:] = \"\"\n        # Lagged links\n        graph[:,:,1:][graph_bool[:,:,1:]==1] = \"--&gt;\"\n        # Unoriented contemporaneous links\n        graph[:,:,0][np.logical_and(graph_bool[:,:,0]==1, \n                                    graph_bool[:,:,0].T==1)] = \"o-o\"\n        # Conflicting contemporaneous links\n        graph[:,:,0][np.logical_and(graph_bool[:,:,0]==2, \n                                    graph_bool[:,:,0].T==2)] = \"x-x\"\n        # Directed contemporaneous links\n        for (i,j) in zip(*np.where(\n            np.logical_and(graph_bool[:,:,0]==1, graph_bool[:,:,0].T==0))):\n            graph[i,j,0] = \"--&gt;\"\n            graph[j,i,0] = \"&lt;--\"\n        return graph\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI.__PCMCIres_converter","title":"<code>__PCMCIres_converter()</code>","text":"<p>Re-elaborates the PCMCI result in a new dictionary</p> <p>Returns:</p> Type Description <code>dict</code> <p>pcmci result re-elaborated</p> Source code in <code>fpcmci/PCMCI.py</code> <pre><code>def __PCMCIres_converter(self):\n\"\"\"\n    Re-elaborates the PCMCI result in a new dictionary\n\n    Returns:\n        (dict): pcmci result re-elaborated\n    \"\"\"\n    res_dict = {f:list() for f in self.result['var_names']}\n    N, lags = self.result['graph'][0].shape\n    for s in range(len(self.result['graph'])):\n        for t in range(N):\n            for lag in range(lags):\n                if self.result['graph'][s][t,lag] == '--&gt;':\n                    res_dict[self.result['var_names'][t]].append({SOURCE : self.result['var_names'][s],\n                                                                  SCORE : self.result['val_matrix'][s][t,lag],\n                                                                  PVAL : self.result['p_matrix'][s][t,lag],\n                                                                  LAG : lag})\n    return res_dict\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI.__apply_alpha","title":"<code>__apply_alpha()</code>","text":"<p>Applies alpha threshold to the pcmci result</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>graph filtered by alpha</p> Source code in <code>fpcmci/PCMCI.py</code> <pre><code>def __apply_alpha(self):\n\"\"\"\n    Applies alpha threshold to the pcmci result\n\n    Returns:\n        (ndarray): graph filtered by alpha \n    \"\"\"\n    mask = np.ones(self.result['p_matrix'].shape, dtype='bool')\n\n    # Set all p-values of absent links to 1.\n    self.result['p_matrix'][mask==False] == 1.\n\n    # Threshold p_matrix to get graph\n    graph_bool = self.result['p_matrix'] &lt;= self.alpha\n\n    # Convert to string graph representation\n    graph = self.__convert_to_string_graph(graph_bool)\n\n    return graph\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI.__convert_to_string_graph","title":"<code>__convert_to_string_graph(graph_bool)</code>","text":"<p>Converts the 0,1-based graph returned by PCMCI to a string array with links '--&gt;'</p> <p>Parameters:</p> Name Type Description Default <code>graph_bool</code> <code>array</code> <p>0,1-based graph array output by PCMCI</p> required <p>Returns:</p> Type Description <code>array</code> <p>graph as string array with links '--&gt;'.</p> Source code in <code>fpcmci/PCMCI.py</code> <pre><code>def __convert_to_string_graph(self, graph_bool):\n\"\"\"\n    Converts the 0,1-based graph returned by PCMCI to a string array\n    with links '--&gt;'\n\n    Args:\n        graph_bool (array): 0,1-based graph array output by PCMCI\n\n    Returns:\n        (array): graph as string array with links '--&gt;'.\n    \"\"\"\n\n    graph = np.zeros(graph_bool.shape, dtype='&lt;U3')\n    graph[:] = \"\"\n    # Lagged links\n    graph[:,:,1:][graph_bool[:,:,1:]==1] = \"--&gt;\"\n    # Unoriented contemporaneous links\n    graph[:,:,0][np.logical_and(graph_bool[:,:,0]==1, \n                                graph_bool[:,:,0].T==1)] = \"o-o\"\n    # Conflicting contemporaneous links\n    graph[:,:,0][np.logical_and(graph_bool[:,:,0]==2, \n                                graph_bool[:,:,0].T==2)] = \"x-x\"\n    # Directed contemporaneous links\n    for (i,j) in zip(*np.where(\n        np.logical_and(graph_bool[:,:,0]==1, graph_bool[:,:,0].T==0))):\n        graph[i,j,0] = \"--&gt;\"\n        graph[j,i,0] = \"&lt;--\"\n    return graph\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI.__init__","title":"<code>__init__(data, alpha, min_lag, max_lag, val_condtest, resfolder, verbosity)</code>","text":"<p>PCMCI class constructor</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse</p> required <code>alpha</code> <code>float</code> <p>significance level</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag</p> required <code>val_condtest</code> <code>CondIndTest</code> <p>validation method</p> required <code>resfolder</code> <code>str</code> <p>result folder. If None then the results are not saved.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level</p> required Source code in <code>fpcmci/PCMCI.py</code> <pre><code>def __init__(self, data: Data, alpha, min_lag, max_lag, val_condtest: CondIndTest, resfolder, verbosity: CPLevel):\n\"\"\"\n    PCMCI class constructor\n\n    Args:\n        data (Data): data to analyse\n        alpha (float): significance level\n        min_lag (int): minimum time lag\n        max_lag (int): maximum time lag\n        val_condtest (CondIndTest): validation method\n        resfolder (str): result folder. If None then the results are not saved.\n        verbosity (CPLevel): verbosity level\n    \"\"\"\n    self.data = data\n    self.alpha = alpha\n    self.min_lag = min_lag\n    self.max_lag = max_lag\n    self.result = None\n    self.dependencies = None\n    self.val_method = None\n    self.val_condtest = val_condtest\n    self.verbosity = verbosity.value\n\n    self.respath = None\n    self.dag_path = None\n    self.ts_dag_path = None\n    if resfolder is not None:\n        self.respath, self.dag_path, self.ts_dag_path = utils.get_validatorpaths(resfolder)  \n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI.__return_parents_dict","title":"<code>__return_parents_dict()</code>","text":"<p>Returns dictionary of parents sorted by val_matrix filtered by alpha</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of form {0:[(0, -1), (3, -2), ...], 1:[], ...} containing estimated parents.</p> Source code in <code>fpcmci/PCMCI.py</code> <pre><code>def __return_parents_dict(self):\n\"\"\"\n    Returns dictionary of parents sorted by val_matrix filtered by alpha\n\n    Returns:\n        (dict): Dictionary of form {0:[(0, -1), (3, -2), ...], 1:[], ...} containing estimated parents.\n    \"\"\"\n    graph = self.result['graph']\n    val_matrix = self.result['val_matrix']\n    p_matrix = self.result['p_matrix']\n\n    # Initialize the return value\n    parents_dict = dict()\n    for j in range(self.data.N):\n        # Get the good links\n        good_links = np.argwhere(graph[:, j, 1:] == \"--&gt;\")\n        # Build a dictionary from these links to their values\n        links = {(i, -tau - 1): np.abs(val_matrix[i, j, abs(tau) + 1]) \n                 for i, tau in good_links if p_matrix[i, j, abs(tau) + 1] &lt;= self.alpha}\n        # Sort by value\n        parents_dict[j] = sorted(links, key=links.get, reverse=True)\n\n    return parents_dict\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI.build_dag","title":"<code>build_dag(node_layout, min_width, max_width, min_score, max_score, node_size, node_color, edge_color, font_size, label_type)</code>","text":"<p>Saves dag plot if resfolder is set otherwise it shows the figure</p> <p>Parameters:</p> Name Type Description Default <code>node_layout</code> <code>str</code> <p>node_layout</p> required <code>min_width</code> <code>int</code> <p>minimum linewidth</p> required <code>max_width</code> <code>int</code> <p>maximum linewidth</p> required <code>min_score</code> <code>int</code> <p>minimum score range</p> required <code>max_score</code> <code>int</code> <p>maximum score range</p> required <code>node_size</code> <code>int</code> <p>node size</p> required <code>node_color</code> <code>str</code> <p>node color</p> required <code>edge_color</code> <code>str</code> <p>edge color</p> required <code>font_size</code> <code>int</code> <p>font size</p> required <code>label_type</code> <code>LabelType</code> <p>enum to set whether to show the lag time (LabelType.Lag) or the strength (LabelType.Score) of the dependencies on each link/node or not showing the labels (LabelType.NoLabels). Default LabelType.Lag.</p> required Source code in <code>fpcmci/PCMCI.py</code> <pre><code>def build_dag(self,\n              node_layout,\n              min_width,\n              max_width,\n              min_score,\n              max_score,\n              node_size,\n              node_color,\n              edge_color,\n              font_size,\n              label_type):\n\"\"\"\n    Saves dag plot if resfolder is set otherwise it shows the figure\n\n    Args:\n        node_layout (str): node_layout\n        min_width (int): minimum linewidth\n        max_width (int): maximum linewidth\n        min_score (int): minimum score range\n        max_score (int): maximum score range\n        node_size (int): node size\n        node_color (str): node color\n        edge_color (str): edge color\n        font_size (int): font size\n        label_type (LabelType, optional): enum to set whether to show the lag time (LabelType.Lag) or the strength (LabelType.Score) of the dependencies on each link/node or not showing the labels (LabelType.NoLabels). Default LabelType.Lag.\n\n    \"\"\"               \n\n    # # convert to dictionary\n    # res = self.__PCMCIres_converter()\n\n    # # filter only dependencies\n    # tmp_res = {k: res[k] for k in self.data.pretty_features}\n    # res = tmp_res\n\n    dag(self.dependencies,\n        node_layout = node_layout,\n        min_width = min_width,\n        max_width = max_width,\n        min_score = min_score,\n        max_score = max_score,\n        node_size = node_size,\n        font_size = font_size,\n        node_color = node_color,\n        edge_color = edge_color,\n        label_type = label_type,\n        save_name = self.dag_path)\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI.build_ts_dag","title":"<code>build_ts_dag(min_width, max_width, min_score, max_score, node_size, node_color, edge_color, font_size)</code>","text":"<p>Saves timeseries dag plot if resfolder is set otherwise it shows the figure</p> <p>Parameters:</p> Name Type Description Default <code>min_width</code> <code>int</code> <p>minimum linewidt</p> required <code>max_width</code> <code>int</code> <p>maximum linewidth</p> required <code>min_score</code> <code>int</code> <p>minimum score range</p> required <code>max_score</code> <code>int</code> <p>maximum score range</p> required <code>node_size</code> <code>int</code> <p>node size</p> required <code>node_color</code> <code>str</code> <p>node color</p> required <code>edge_color</code> <code>str</code> <p>edge color </p> required <code>font_size</code> <code>int</code> <p>font size</p> required Source code in <code>fpcmci/PCMCI.py</code> <pre><code>def build_ts_dag(self,\n                 min_width,\n                 max_width,\n                 min_score,\n                 max_score,\n                 node_size,\n                 node_color,\n                 edge_color,\n                 font_size):\n\"\"\"\n    Saves timeseries dag plot if resfolder is set otherwise it shows the figure\n\n    Args:\n        min_width (int): minimum linewidt\n        max_width (int): maximum linewidth\n        min_score (int): minimum score range\n        max_score (int): maximum score range\n        node_size (int): node size\n        node_color (str): node color\n        edge_color (str): edge color \n        font_size (int): font size\n    \"\"\"\n\n    # # convert to dictionary\n    # res = self.__PCMCIres_converter()\n\n    # # filter only dependencies\n    # tmp_res = {k: res[k] for k in self.data.pretty_features}\n    # res = tmp_res\n\n    ts_dag(self.dependencies, \n           tau = self.max_lag,\n           min_width = min_width,\n           max_width = max_width,\n           min_score = min_score,\n           max_score = max_score,\n           node_size = node_size,\n           node_color = node_color,\n           edge_color = edge_color,\n           font_size = font_size,\n           save_name = self.ts_dag_path)\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI.run","title":"<code>run(link_assumptions=None)</code>","text":"<p>Run causal discovery algorithm</p> <p>Returns:</p> Type Description <code>dict</code> <p>estimated causal model</p> Source code in <code>fpcmci/PCMCI.py</code> <pre><code>def run(self, link_assumptions = None):\n\"\"\"\n    Run causal discovery algorithm\n\n    Returns:\n        (dict): estimated causal model\n    \"\"\"\n    CP.info('\\n')\n    CP.info(DASH)\n    CP.info(\"Running Causal Discovery Algorithm\")\n\n    # build tigramite dataset\n    vector = np.vectorize(float)\n    data = vector(self.data.d)\n    dataframe = pp.DataFrame(data = data,\n                             var_names = self.data.pretty_features)\n\n    # init and run pcmci\n    self.val_method = VAL(dataframe = dataframe,\n                            cond_ind_test = self.val_condtest,\n                            verbosity = self.verbosity)\n\n    self.result = self.val_method.run_pcmci(link_assumptions = link_assumptions,\n                                            tau_max = self.max_lag,\n                                            tau_min = self.min_lag)\n\n    self.result['var_names'] = self.data.pretty_features\n    # apply significance level\n    self.result['graph'] = self.__apply_alpha()\n    self.dependencies = self.__PCMCIres_converter()\n    return self.__return_parents_dict()\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI.save_result","title":"<code>save_result()</code>","text":"<p>Save causal discovery results as pickle file if resfolder is set</p> Source code in <code>fpcmci/PCMCI.py</code> <pre><code>def save_result(self):\n\"\"\"\n    Save causal discovery results as pickle file if resfolder is set\n    \"\"\"\n    if self.respath is not None:\n        res = dict()\n        res['dependencies'] = copy.deepcopy(self.dependencies)\n        # res = copy.deepcopy(self.result)\n        res['alpha'] = self.alpha\n        res['var_names'] = self.data.pretty_features\n        res['dag_path'] = self.dag_path\n        res['ts_dag_path'] = self.ts_dag_path\n        with open(self.respath, 'wb') as resfile:\n            pickle.dump(res, resfile)\n</code></pre>"},{"location":"preprocessing/","title":"Preprocessing","text":""},{"location":"preprocessing/#fpcmci.preprocessing.data.Data","title":"<code>Data</code>","text":"<p>Data class manages the preprocess of the data before the causal analysis</p> Source code in <code>fpcmci/preprocessing/data.py</code> <pre><code>class Data():\n\"\"\"\n    Data class manages the preprocess of the data before the causal analysis\n    \"\"\"\n    def __init__(self, data, vars = None, fill_nan = True, stand = False, subsampling : SubsamplingMethod = None, show_subsampling = False):\n\"\"\"\n        Data class constructor\n\n        Args:\n            data (str/DataFrame/np.array): it can be a string specifing the path of a csv file to load/pandas.DataFrame/numpy.array\n            vars (list(str), optional): List containing variable names. If unset then, \n                if data = (str/DataFrame) vars = data columns name elif data = np.array vars = [X_0 .. X_N]\n                Defaults to None.\n            fill_nan (bool, optional): Fill NaNs bit. Defaults to True.\n            stand (bool, optional): Standardization bit. Defaults to False.\n            subsampling (SubsamplingMethod, optional): Subsampling method. If None not active. Defaults to None.\n            show_subsampling (bool, optional): If True shows subsampling result. Defaults to False.\n\n        Raises:\n            TypeError: if data is not str - DataFrame - ndarray\n        \"\"\"\n        # Data handling\n        if type(data) == np.ndarray:\n            self.d = pd.DataFrame(data)\n            if vars is None: self.d.columns = list(['X_' + str(f) for f in range(len(self.d.columns))])\n        elif type(data) == pd.DataFrame:\n            self.d = data\n        elif type(data) == str:\n            self.d = pd.read_csv(data)\n        else:\n            raise TypeError(\"data field not in the correct type\\ndata must be one of the following type:\\n- numpy.ndarray\\n- pandas.DataFrame\\n- .csv path\")\n\n\n        # Columns name handling\n        if vars is not None:\n            self.d.columns = list(vars)\n\n\n        self.orig_features = self.features\n        self.orig_pretty_features = self.pretty_features\n        self.orig_N = self.N\n        self.orig_T = len(self.d)\n\n        # Filling NaNs\n        if fill_nan:\n            if self.d.isnull().values.any():\n                self.d.fillna(inplace=True, method=\"ffill\")\n                self.d.fillna(inplace=True, method=\"bfill\")\n\n        # Subsampling data\n        if subsampling is not None:\n            subsampler = Subsampler(self.d, ss_method = subsampling)\n            self.d = pd.DataFrame(subsampler.subsample(), columns = self.features)\n            if show_subsampling: subsampler.plot_subsampled_data()\n\n        # Standardize data\n        if stand:\n            scaler = StandardScaler()\n            scaler = scaler.fit(self.d)\n            self.d = pd.DataFrame(scaler.transform(self.d), columns = self.features)\n\n\n    @property  \n    def features(self):\n\"\"\"\n        Returns list of features\n\n        Returns:\n            list(str): list of feature names\n        \"\"\"\n        return list(self.d.columns)\n\n    @property\n    def pretty_features(self):\n\"\"\"\n        Returns list of features with LATEX symbols\n\n        Returns:\n            list(str): list of feature names\n        \"\"\"\n        return [r'$' + str(v) + '$' for v in self.d.columns]\n\n    @property\n    def N(self):\n\"\"\"\n        Number of features\n\n        Returns:\n            (int): number of features\n        \"\"\"\n        return len(self.d.columns)\n\n    @property\n    def T(self):\n\"\"\"\n        Dataframe length\n\n        Returns:\n            (int): dataframe length\n        \"\"\"\n        return len(self.d)\n\n\n    def shrink(self, selected_features):\n\"\"\"\n        Shrinks dataframe d and dependencies based on the selected features\n\n        Args:\n            selected_features (list(str)): features selected by the selector\n        \"\"\"\n        self.d = self.d[selected_features]\n\n\n    def plot_timeseries(self):\n\"\"\"\n        Plots timeseries data\n        \"\"\"\n        # Create grid\n        gs = gridspec.GridSpec(self.N, 1)\n\n        # Time vector\n        T = list(range(self.T))\n\n        plt.figure()\n        for i in range(0, self.d.shape[1]):\n            ax = plt.subplot(gs[i, 0])\n            plt.plot(T, self.d.values[:, i], color = 'tab:red')\n            plt.ylabel(str(self.pretty_features[i]))\n\n        plt.show()\n</code></pre>"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.N","title":"<code>N</code>  <code>property</code>","text":"<p>Number of features</p> <p>Returns:</p> Type Description <code>int</code> <p>number of features</p>"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.T","title":"<code>T</code>  <code>property</code>","text":"<p>Dataframe length</p> <p>Returns:</p> Type Description <code>int</code> <p>dataframe length</p>"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.features","title":"<code>features</code>  <code>property</code>","text":"<p>Returns list of features</p> <p>Returns:</p> Name Type Description <code>list</code> <code>str</code> <p>list of feature names</p>"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.pretty_features","title":"<code>pretty_features</code>  <code>property</code>","text":"<p>Returns list of features with LATEX symbols</p> <p>Returns:</p> Name Type Description <code>list</code> <code>str</code> <p>list of feature names</p>"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.__init__","title":"<code>__init__(data, vars=None, fill_nan=True, stand=False, subsampling=None, show_subsampling=False)</code>","text":"<p>Data class constructor</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str/DataFrame/np.array</code> <p>it can be a string specifing the path of a csv file to load/pandas.DataFrame/numpy.array</p> required <code>vars</code> <code>list(str)</code> <p>List containing variable names. If unset then,  if data = (str/DataFrame) vars = data columns name elif data = np.array vars = [X_0 .. X_N] Defaults to None.</p> <code>None</code> <code>fill_nan</code> <code>bool</code> <p>Fill NaNs bit. Defaults to True.</p> <code>True</code> <code>stand</code> <code>bool</code> <p>Standardization bit. Defaults to False.</p> <code>False</code> <code>subsampling</code> <code>SubsamplingMethod</code> <p>Subsampling method. If None not active. Defaults to None.</p> <code>None</code> <code>show_subsampling</code> <code>bool</code> <p>If True shows subsampling result. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>if data is not str - DataFrame - ndarray</p> Source code in <code>fpcmci/preprocessing/data.py</code> <pre><code>def __init__(self, data, vars = None, fill_nan = True, stand = False, subsampling : SubsamplingMethod = None, show_subsampling = False):\n\"\"\"\n    Data class constructor\n\n    Args:\n        data (str/DataFrame/np.array): it can be a string specifing the path of a csv file to load/pandas.DataFrame/numpy.array\n        vars (list(str), optional): List containing variable names. If unset then, \n            if data = (str/DataFrame) vars = data columns name elif data = np.array vars = [X_0 .. X_N]\n            Defaults to None.\n        fill_nan (bool, optional): Fill NaNs bit. Defaults to True.\n        stand (bool, optional): Standardization bit. Defaults to False.\n        subsampling (SubsamplingMethod, optional): Subsampling method. If None not active. Defaults to None.\n        show_subsampling (bool, optional): If True shows subsampling result. Defaults to False.\n\n    Raises:\n        TypeError: if data is not str - DataFrame - ndarray\n    \"\"\"\n    # Data handling\n    if type(data) == np.ndarray:\n        self.d = pd.DataFrame(data)\n        if vars is None: self.d.columns = list(['X_' + str(f) for f in range(len(self.d.columns))])\n    elif type(data) == pd.DataFrame:\n        self.d = data\n    elif type(data) == str:\n        self.d = pd.read_csv(data)\n    else:\n        raise TypeError(\"data field not in the correct type\\ndata must be one of the following type:\\n- numpy.ndarray\\n- pandas.DataFrame\\n- .csv path\")\n\n\n    # Columns name handling\n    if vars is not None:\n        self.d.columns = list(vars)\n\n\n    self.orig_features = self.features\n    self.orig_pretty_features = self.pretty_features\n    self.orig_N = self.N\n    self.orig_T = len(self.d)\n\n    # Filling NaNs\n    if fill_nan:\n        if self.d.isnull().values.any():\n            self.d.fillna(inplace=True, method=\"ffill\")\n            self.d.fillna(inplace=True, method=\"bfill\")\n\n    # Subsampling data\n    if subsampling is not None:\n        subsampler = Subsampler(self.d, ss_method = subsampling)\n        self.d = pd.DataFrame(subsampler.subsample(), columns = self.features)\n        if show_subsampling: subsampler.plot_subsampled_data()\n\n    # Standardize data\n    if stand:\n        scaler = StandardScaler()\n        scaler = scaler.fit(self.d)\n        self.d = pd.DataFrame(scaler.transform(self.d), columns = self.features)\n</code></pre>"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.plot_timeseries","title":"<code>plot_timeseries()</code>","text":"<p>Plots timeseries data</p> Source code in <code>fpcmci/preprocessing/data.py</code> <pre><code>def plot_timeseries(self):\n\"\"\"\n    Plots timeseries data\n    \"\"\"\n    # Create grid\n    gs = gridspec.GridSpec(self.N, 1)\n\n    # Time vector\n    T = list(range(self.T))\n\n    plt.figure()\n    for i in range(0, self.d.shape[1]):\n        ax = plt.subplot(gs[i, 0])\n        plt.plot(T, self.d.values[:, i], color = 'tab:red')\n        plt.ylabel(str(self.pretty_features[i]))\n\n    plt.show()\n</code></pre>"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.shrink","title":"<code>shrink(selected_features)</code>","text":"<p>Shrinks dataframe d and dependencies based on the selected features</p> <p>Parameters:</p> Name Type Description Default <code>selected_features</code> <code>list(str</code> <p>features selected by the selector</p> required Source code in <code>fpcmci/preprocessing/data.py</code> <pre><code>def shrink(self, selected_features):\n\"\"\"\n    Shrinks dataframe d and dependencies based on the selected features\n\n    Args:\n        selected_features (list(str)): features selected by the selector\n    \"\"\"\n    self.d = self.d[selected_features]\n</code></pre>"},{"location":"subsampling_method/","title":"Subsampling","text":""},{"location":"subsampling_method/#fpcmci.preprocessing.Subsampler.Subsampler","title":"<code>Subsampler</code>","text":"<p>Subsampler class. </p> It subsamples the data by using a subsampling method chosen among <ul> <li>Static - subsamples data by taking one sample each step-samples</li> <li>WSDynamic - entropy based method with dynamic window size computed by breakpoint analysis</li> <li>WSFFTStatic - entropy based method with fixed window size computed by FFT analysis</li> <li>WSStatic - entropy base method with predefined window size</li> </ul> Source code in <code>fpcmci/preprocessing/Subsampler.py</code> <pre><code>class Subsampler():\n\"\"\"\n    Subsampler class. \n\n    It subsamples the data by using a subsampling method chosen among:\n        - Static - subsamples data by taking one sample each step-samples\n        - WSDynamic - entropy based method with dynamic window size computed by breakpoint analysis\n        - WSFFTStatic - entropy based method with fixed window size computed by FFT analysis\n        - WSStatic - entropy base method with predefined window size\n    \"\"\"\n\n    def __init__(self, \n                 df: pd.DataFrame, \n                 ss_method: SubsamplingMethod):\n\"\"\"\n        Subsampler class constructor\n\n        Args:\n            df (pd.DataFrame): dataframe to subsample\n            ss_method (SubsamplingMethod): subsampling method\n        \"\"\"\n        self.df = df\n        self.ss_method = ss_method\n        self.ss_method.initialise(df)\n\n\n    def subsample(self):\n\"\"\"\n        Runs the subsampling algorithm and returns the subsapled ndarray\n\n        Returns:\n            (ndarray): Subsampled dataframe value\n        \"\"\"\n        self.result = self.ss_method.run()\n        return self.df.values[self.result, :]\n\n\n    def plot_subsampled_data(self, dpi = 100, show = True):\n\"\"\"\n        Plot dataframe sub-sampled data\n\n        Args:\n            dpi (int, optional): image dpi. Defaults to 100.\n            show (bool, optional): if True it shows the figure and block the process. Defaults to True.\n        \"\"\"\n        n_plot = self.df.shape[1]\n\n        # Create grid\n        gs = gridspec.GridSpec(n_plot, 1)\n\n        # Time vector\n        T = list(range(0, self.df.shape[0]))\n\n        pl.figure(dpi = dpi)\n        for i in range(0, n_plot):\n            ax = pl.subplot(gs[i, 0])\n            pl.plot(T, self.df.values[:, i], color = 'tab:red')\n            pl.scatter(np.array(T)[self.result],\n                       self.df.values[self.result, i],\n                       s = 80,\n                       facecolors = 'none',\n                       edgecolors = 'b')\n            pl.gca().set(ylabel = r'$' + str(self.df.columns.values[i]) + '$')\n        if show:\n            pl.show()\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.Subsampler.Subsampler.__init__","title":"<code>__init__(df, ss_method)</code>","text":"<p>Subsampler class constructor</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>dataframe to subsample</p> required <code>ss_method</code> <code>SubsamplingMethod</code> <p>subsampling method</p> required Source code in <code>fpcmci/preprocessing/Subsampler.py</code> <pre><code>def __init__(self, \n             df: pd.DataFrame, \n             ss_method: SubsamplingMethod):\n\"\"\"\n    Subsampler class constructor\n\n    Args:\n        df (pd.DataFrame): dataframe to subsample\n        ss_method (SubsamplingMethod): subsampling method\n    \"\"\"\n    self.df = df\n    self.ss_method = ss_method\n    self.ss_method.initialise(df)\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.Subsampler.Subsampler.plot_subsampled_data","title":"<code>plot_subsampled_data(dpi=100, show=True)</code>","text":"<p>Plot dataframe sub-sampled data</p> <p>Parameters:</p> Name Type Description Default <code>dpi</code> <code>int</code> <p>image dpi. Defaults to 100.</p> <code>100</code> <code>show</code> <code>bool</code> <p>if True it shows the figure and block the process. Defaults to True.</p> <code>True</code> Source code in <code>fpcmci/preprocessing/Subsampler.py</code> <pre><code>def plot_subsampled_data(self, dpi = 100, show = True):\n\"\"\"\n    Plot dataframe sub-sampled data\n\n    Args:\n        dpi (int, optional): image dpi. Defaults to 100.\n        show (bool, optional): if True it shows the figure and block the process. Defaults to True.\n    \"\"\"\n    n_plot = self.df.shape[1]\n\n    # Create grid\n    gs = gridspec.GridSpec(n_plot, 1)\n\n    # Time vector\n    T = list(range(0, self.df.shape[0]))\n\n    pl.figure(dpi = dpi)\n    for i in range(0, n_plot):\n        ax = pl.subplot(gs[i, 0])\n        pl.plot(T, self.df.values[:, i], color = 'tab:red')\n        pl.scatter(np.array(T)[self.result],\n                   self.df.values[self.result, i],\n                   s = 80,\n                   facecolors = 'none',\n                   edgecolors = 'b')\n        pl.gca().set(ylabel = r'$' + str(self.df.columns.values[i]) + '$')\n    if show:\n        pl.show()\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.Subsampler.Subsampler.subsample","title":"<code>subsample()</code>","text":"<p>Runs the subsampling algorithm and returns the subsapled ndarray</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Subsampled dataframe value</p> Source code in <code>fpcmci/preprocessing/Subsampler.py</code> <pre><code>def subsample(self):\n\"\"\"\n    Runs the subsampling algorithm and returns the subsapled ndarray\n\n    Returns:\n        (ndarray): Subsampled dataframe value\n    \"\"\"\n    self.result = self.ss_method.run()\n    return self.df.values[self.result, :]\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.SubsamplingMethod.SubsamplingMethod","title":"<code>SubsamplingMethod</code>","text":"<p>         Bases: <code>ABC</code></p> <p>SubsamplingMethod abstract class</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/SubsamplingMethod.py</code> <pre><code>class SubsamplingMethod(ABC):\n\"\"\"\n    SubsamplingMethod abstract class\n    \"\"\"\n    def __init__(self, ssmode: SSMode):\n        self.ssmode = ssmode\n        self.df = None\n\n\n    def initialise(self, dataframe: pd.DataFrame):\n\"\"\"\n        Initialise class by setting the dataframe to subsample\n\n        Args:\n            dataframe (pd.DataFrame): _description_\n        \"\"\"\n        self.df = dataframe\n\n\n    @abstractmethod\n    def run(self):\n\"\"\"\n        Run subsampler\n        \"\"\"\n        pass\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.SubsamplingMethod.SubsamplingMethod.initialise","title":"<code>initialise(dataframe)</code>","text":"<p>Initialise class by setting the dataframe to subsample</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>pd.DataFrame</code> <p>description</p> required Source code in <code>fpcmci/preprocessing/subsampling_methods/SubsamplingMethod.py</code> <pre><code>def initialise(self, dataframe: pd.DataFrame):\n\"\"\"\n    Initialise class by setting the dataframe to subsample\n\n    Args:\n        dataframe (pd.DataFrame): _description_\n    \"\"\"\n    self.df = dataframe\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.SubsamplingMethod.SubsamplingMethod.run","title":"<code>run()</code>  <code>abstractmethod</code>","text":"<p>Run subsampler</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/SubsamplingMethod.py</code> <pre><code>@abstractmethod\ndef run(self):\n\"\"\"\n    Run subsampler\n    \"\"\"\n    pass\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod","title":"<code>EntropyBasedMethod</code>","text":"<p>         Bases: <code>ABC</code></p> <p>EntropyBasedMethod abstract class</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>class EntropyBasedMethod(ABC):\n\"\"\"\n    EntropyBasedMethod abstract class\n    \"\"\"\n    def __init__(self, threshold):\n        self.windows = list()\n        self.segments = list()\n        self.threshold = threshold\n\n\n    def create_rounded_copy(self):\n\"\"\"\n        Create deepcopy of the dataframe but with rounded values\n\n        Returns:\n            (pd.DataFrame): rounded dataframe\n        \"\"\"\n        de = deepcopy(self.df)\n        de = de.round(1)\n        return de\n\n\n    def __normalization(self):\n\"\"\"\n        Normalize entropy for each moving window\n        \"\"\"\n        max_e = max([mw.entropy for mw in self.windows])\n        for mw in self.windows:\n            mw.entropy = mw.entropy / max_e\n\n\n    def moving_window_analysis(self):\n\"\"\"\n        Compute dataframe entropy on moving windows\n        \"\"\"\n        de = self.create_rounded_copy()\n\n        for ll, rl in self.segments:\n            # Create moving window\n            mw_df = de.values[ll: rl]\n\n            # Build a Moving Window\n            mw = MovingWindow(mw_df)\n\n            # Compute entropy\n            mw.get_entropy()\n\n            # Compute optimal number of samples\n            mw.optimal_sampling(self.threshold)\n\n            # Collect result in a list\n            self.windows.append(mw)\n\n        # Entropy normalization\n        self.__normalization()\n\n\n    # def extract_data(self):\n    #     \"\"\"\n    #     Extract plottable data from moving window analysis\n    #     \"\"\"\n    #     # Entropies and samples numbers list\n    #     self.__entropy_list = [mw.entropy for mw in self.__window_list]\n    #     self.__sample_number_list = [mw.opt_size for mw in self.__window_list]\n    #     self.__original_size = [mw.T for mw in self.__window_list]\n    #     self.num_samples = sum(self.__sample_number_list)\n\n    #     # Make entropy and sample array plottable\n    #     self.__pretty_signals()\n\n\n    # def __pretty_signals(self):\n    #     \"\"\"\n    #     Make entropy list and sample number list plottable\n    #     \"\"\"\n    #     _pretty_entropy = []\n    #     _pretty_sample_number = []\n    #     _pretty_original_size = []\n    #     for i, mw in enumerate(self.__window_list):\n    #         _pretty_entropy += np.repeat(self.__entropy_list[i], mw.T).tolist()\n    #         _pretty_sample_number += np.repeat(self.__sample_number_list[i], mw.T).tolist()\n    #         _pretty_original_size += np.repeat(self.__original_size[i], mw.T).tolist()\n    #     self.__entropy_list = _pretty_entropy\n    #     self.__sample_number_list = _pretty_sample_number\n    #     self.__original_size = _pretty_original_size\n\n    #     _diff = self.df.shape[0] - len(self.__entropy_list)\n    #     if _diff != 0:\n    #         self.__entropy_list = np.append(self.__entropy_list, [self.__entropy_list[-1]] * _diff)\n    #         self.__sample_number_list = np.append(self.__sample_number_list, [self.__sample_number_list[-1]] * _diff)\n\n\n    def extract_indexes(self):\n\"\"\"\n        Extract a list of indexes corresponding to the samples\n        selected by the subsampling procedure\n        \"\"\"\n        _sample_index_list = list()\n        for i, mw in enumerate(self.windows):\n            sum_ws = sum([wind.T for wind in self.windows[:i]])\n            sample_index = [si + sum_ws for si in mw.opt_samples_index]\n            _sample_index_list += sample_index\n        return _sample_index_list\n\n\n    @abstractmethod\n    def dataset_segmentation(self):\n\"\"\"\n        abstract method\n        \"\"\"\n        pass\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.__normalization","title":"<code>__normalization()</code>","text":"<p>Normalize entropy for each moving window</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>def __normalization(self):\n\"\"\"\n    Normalize entropy for each moving window\n    \"\"\"\n    max_e = max([mw.entropy for mw in self.windows])\n    for mw in self.windows:\n        mw.entropy = mw.entropy / max_e\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.create_rounded_copy","title":"<code>create_rounded_copy()</code>","text":"<p>Create deepcopy of the dataframe but with rounded values</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>rounded dataframe</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>def create_rounded_copy(self):\n\"\"\"\n    Create deepcopy of the dataframe but with rounded values\n\n    Returns:\n        (pd.DataFrame): rounded dataframe\n    \"\"\"\n    de = deepcopy(self.df)\n    de = de.round(1)\n    return de\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.dataset_segmentation","title":"<code>dataset_segmentation()</code>  <code>abstractmethod</code>","text":"<p>abstract method</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>@abstractmethod\ndef dataset_segmentation(self):\n\"\"\"\n    abstract method\n    \"\"\"\n    pass\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.extract_indexes","title":"<code>extract_indexes()</code>","text":"<p>Extract a list of indexes corresponding to the samples selected by the subsampling procedure</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>def extract_indexes(self):\n\"\"\"\n    Extract a list of indexes corresponding to the samples\n    selected by the subsampling procedure\n    \"\"\"\n    _sample_index_list = list()\n    for i, mw in enumerate(self.windows):\n        sum_ws = sum([wind.T for wind in self.windows[:i]])\n        sample_index = [si + sum_ws for si in mw.opt_samples_index]\n        _sample_index_list += sample_index\n    return _sample_index_list\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.moving_window_analysis","title":"<code>moving_window_analysis()</code>","text":"<p>Compute dataframe entropy on moving windows</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>def moving_window_analysis(self):\n\"\"\"\n    Compute dataframe entropy on moving windows\n    \"\"\"\n    de = self.create_rounded_copy()\n\n    for ll, rl in self.segments:\n        # Create moving window\n        mw_df = de.values[ll: rl]\n\n        # Build a Moving Window\n        mw = MovingWindow(mw_df)\n\n        # Compute entropy\n        mw.get_entropy()\n\n        # Compute optimal number of samples\n        mw.optimal_sampling(self.threshold)\n\n        # Collect result in a list\n        self.windows.append(mw)\n\n    # Entropy normalization\n    self.__normalization()\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic","title":"<code>WSDynamic</code>","text":"<p>         Bases: <code>SubsamplingMethod</code>, <code>EntropyBasedMethod</code></p> <p>Subsampling method with dynamic window size based on entropy analysis</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>class WSDynamic(SubsamplingMethod, EntropyBasedMethod):\n\"\"\"\n    Subsampling method with dynamic window size based on entropy analysis\n    \"\"\"\n    def __init__(self, window_min_size, entropy_threshold):\n\"\"\"\n        WSDynamic class constructor\n\n        Args:\n            window_min_size (int): minimun window size\n            entropy_threshold (float): entropy threshold\n\n        Raises:\n            ValueError: if window_min_size == None\n        \"\"\"\n        SubsamplingMethod.__init__(self, SSMode.WSDynamic)\n        EntropyBasedMethod.__init__(self, entropy_threshold)\n        if window_min_size is None:\n            raise ValueError(\"window_type = DYNAMIC but window_min_size not specified\")\n        self.wms = window_min_size\n        self.ws = None\n\n    def dataset_segmentation(self):\n\"\"\"\n        Segments dataset based on breakpoint analysis and a min window size\n        \"\"\"\n        de = self.create_rounded_copy()\n        algo = rpt.Pelt(model = \"l2\", min_size = self.wms).fit(de)\n        seg_res = algo.predict(pen = 10)\n        self.segments = [(seg_res[i - 1], seg_res[i]) for i in range(1, len(seg_res))]\n        self.segments.insert(0, (0, seg_res[0]))\n\n\n    def run(self):\n\"\"\"\n        Run subsampler\n\n        Returns:\n            (list[int]): indexes of the remaining samples\n        \"\"\"\n        # build list of segment\n        self.dataset_segmentation()\n\n        # compute entropy moving window\n        self.moving_window_analysis()\n\n        # extracting subsampling procedure results\n        idxs = self.extract_indexes()\n\n        return idxs\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.__init__","title":"<code>__init__(window_min_size, entropy_threshold)</code>","text":"<p>WSDynamic class constructor</p> <p>Parameters:</p> Name Type Description Default <code>window_min_size</code> <code>int</code> <p>minimun window size</p> required <code>entropy_threshold</code> <code>float</code> <p>entropy threshold</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if window_min_size == None</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>def __init__(self, window_min_size, entropy_threshold):\n\"\"\"\n    WSDynamic class constructor\n\n    Args:\n        window_min_size (int): minimun window size\n        entropy_threshold (float): entropy threshold\n\n    Raises:\n        ValueError: if window_min_size == None\n    \"\"\"\n    SubsamplingMethod.__init__(self, SSMode.WSDynamic)\n    EntropyBasedMethod.__init__(self, entropy_threshold)\n    if window_min_size is None:\n        raise ValueError(\"window_type = DYNAMIC but window_min_size not specified\")\n    self.wms = window_min_size\n    self.ws = None\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.dataset_segmentation","title":"<code>dataset_segmentation()</code>","text":"<p>Segments dataset based on breakpoint analysis and a min window size</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>def dataset_segmentation(self):\n\"\"\"\n    Segments dataset based on breakpoint analysis and a min window size\n    \"\"\"\n    de = self.create_rounded_copy()\n    algo = rpt.Pelt(model = \"l2\", min_size = self.wms).fit(de)\n    seg_res = algo.predict(pen = 10)\n    self.segments = [(seg_res[i - 1], seg_res[i]) for i in range(1, len(seg_res))]\n    self.segments.insert(0, (0, seg_res[0]))\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.run","title":"<code>run()</code>","text":"<p>Run subsampler</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>indexes of the remaining samples</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>def run(self):\n\"\"\"\n    Run subsampler\n\n    Returns:\n        (list[int]): indexes of the remaining samples\n    \"\"\"\n    # build list of segment\n    self.dataset_segmentation()\n\n    # compute entropy moving window\n    self.moving_window_analysis()\n\n    # extracting subsampling procedure results\n    idxs = self.extract_indexes()\n\n    return idxs\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic","title":"<code>WSDynamic</code>","text":"<p>         Bases: <code>SubsamplingMethod</code>, <code>EntropyBasedMethod</code></p> <p>Subsampling method with dynamic window size based on entropy analysis</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>class WSDynamic(SubsamplingMethod, EntropyBasedMethod):\n\"\"\"\n    Subsampling method with dynamic window size based on entropy analysis\n    \"\"\"\n    def __init__(self, window_min_size, entropy_threshold):\n\"\"\"\n        WSDynamic class constructor\n\n        Args:\n            window_min_size (int): minimun window size\n            entropy_threshold (float): entropy threshold\n\n        Raises:\n            ValueError: if window_min_size == None\n        \"\"\"\n        SubsamplingMethod.__init__(self, SSMode.WSDynamic)\n        EntropyBasedMethod.__init__(self, entropy_threshold)\n        if window_min_size is None:\n            raise ValueError(\"window_type = DYNAMIC but window_min_size not specified\")\n        self.wms = window_min_size\n        self.ws = None\n\n    def dataset_segmentation(self):\n\"\"\"\n        Segments dataset based on breakpoint analysis and a min window size\n        \"\"\"\n        de = self.create_rounded_copy()\n        algo = rpt.Pelt(model = \"l2\", min_size = self.wms).fit(de)\n        seg_res = algo.predict(pen = 10)\n        self.segments = [(seg_res[i - 1], seg_res[i]) for i in range(1, len(seg_res))]\n        self.segments.insert(0, (0, seg_res[0]))\n\n\n    def run(self):\n\"\"\"\n        Run subsampler\n\n        Returns:\n            (list[int]): indexes of the remaining samples\n        \"\"\"\n        # build list of segment\n        self.dataset_segmentation()\n\n        # compute entropy moving window\n        self.moving_window_analysis()\n\n        # extracting subsampling procedure results\n        idxs = self.extract_indexes()\n\n        return idxs\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.__init__","title":"<code>__init__(window_min_size, entropy_threshold)</code>","text":"<p>WSDynamic class constructor</p> <p>Parameters:</p> Name Type Description Default <code>window_min_size</code> <code>int</code> <p>minimun window size</p> required <code>entropy_threshold</code> <code>float</code> <p>entropy threshold</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if window_min_size == None</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>def __init__(self, window_min_size, entropy_threshold):\n\"\"\"\n    WSDynamic class constructor\n\n    Args:\n        window_min_size (int): minimun window size\n        entropy_threshold (float): entropy threshold\n\n    Raises:\n        ValueError: if window_min_size == None\n    \"\"\"\n    SubsamplingMethod.__init__(self, SSMode.WSDynamic)\n    EntropyBasedMethod.__init__(self, entropy_threshold)\n    if window_min_size is None:\n        raise ValueError(\"window_type = DYNAMIC but window_min_size not specified\")\n    self.wms = window_min_size\n    self.ws = None\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.dataset_segmentation","title":"<code>dataset_segmentation()</code>","text":"<p>Segments dataset based on breakpoint analysis and a min window size</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>def dataset_segmentation(self):\n\"\"\"\n    Segments dataset based on breakpoint analysis and a min window size\n    \"\"\"\n    de = self.create_rounded_copy()\n    algo = rpt.Pelt(model = \"l2\", min_size = self.wms).fit(de)\n    seg_res = algo.predict(pen = 10)\n    self.segments = [(seg_res[i - 1], seg_res[i]) for i in range(1, len(seg_res))]\n    self.segments.insert(0, (0, seg_res[0]))\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.run","title":"<code>run()</code>","text":"<p>Run subsampler</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>indexes of the remaining samples</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>def run(self):\n\"\"\"\n    Run subsampler\n\n    Returns:\n        (list[int]): indexes of the remaining samples\n    \"\"\"\n    # build list of segment\n    self.dataset_segmentation()\n\n    # compute entropy moving window\n    self.moving_window_analysis()\n\n    # extracting subsampling procedure results\n    idxs = self.extract_indexes()\n\n    return idxs\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.Static.Static","title":"<code>Static</code>","text":"<p>         Bases: <code>SubsamplingMethod</code></p> <p>Subsamples data by taking one sample each step-samples</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/Static.py</code> <pre><code>class Static(SubsamplingMethod):\n\"\"\"\n    Subsamples data by taking one sample each step-samples\n    \"\"\"\n    def __init__(self, step):\n\"\"\"\n        Static class constructor\n\n        Args:\n            step (int): integer subsampling step\n\n        Raises:\n            ValueError: if step == None\n        \"\"\"\n        super().__init__(SSMode.Static)\n        if step is None:\n            raise ValueError(\"step not specified\")\n        self.step = step\n\n    def run(self):\n        return range(0, len(self.df.values), self.step)\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.Static.Static.__init__","title":"<code>__init__(step)</code>","text":"<p>Static class constructor</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>integer subsampling step</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if step == None</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/Static.py</code> <pre><code>def __init__(self, step):\n\"\"\"\n    Static class constructor\n\n    Args:\n        step (int): integer subsampling step\n\n    Raises:\n        ValueError: if step == None\n    \"\"\"\n    super().__init__(SSMode.Static)\n    if step is None:\n        raise ValueError(\"step not specified\")\n    self.step = step\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic","title":"<code>WSFFTStatic</code>","text":"<p>         Bases: <code>SubsamplingMethod</code>, <code>EntropyBasedMethod</code></p> <p>Subsampling method with static window size based on Fourier analysis</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py</code> <pre><code>class WSFFTStatic(SubsamplingMethod, EntropyBasedMethod):\n\"\"\"\n    Subsampling method with static window size based on Fourier analysis\n    \"\"\"\n    def __init__(self, sampling_time, entropy_threshold):\n\"\"\"\n        WSFFTStatic class constructor\n\n        Args:\n            sampling_time (float): timeseries sampling time\n            entropy_threshold (float): entropy threshold\n        \"\"\"\n        SubsamplingMethod.__init__(self, SSMode.WSFFTStatic)\n        EntropyBasedMethod.__init__(self, entropy_threshold)\n        self.sampling_time = sampling_time\n\n\n    def __fourier_window(self):\n\"\"\"\n        Compute window size based on Fourier analysis performed on dataframe\n\n        Returns:\n            (int): window size\n        \"\"\"\n        N, dim = self.df.shape\n        xf = rfftfreq(N, self.sampling_time)\n        w_array = list()\n        for i in range(0, dim):\n            yf = np.abs(rfft(self.df.values[:, i]))\n\n            peak_indices, _ = scipy.signal.find_peaks(yf)\n            highest_peak_index = peak_indices[np.argmax(yf[peak_indices])]\n            w_array.append(ceil(1 / (2 * xf[highest_peak_index]) / self.sampling_time))\n            # fig, ax = pl.subplots()\n            # ax.plot(xf, yf)\n            # ax.plot(xf[highest_peak_index], np.abs(yf[highest_peak_index]), \"x\")\n            # pl.show()\n        return min(w_array)\n\n\n    def dataset_segmentation(self):\n\"\"\"\n        Segments dataset with a fixed window size\n        \"\"\"\n        seg_res = [i for i in range(0, len(self.df.values), self.ws)]\n        self.segments = [(i, i + self.ws) for i in range(0, len(self.df.values) - self.ws, self.ws)]\n        if not seg_res.__contains__(len(self.df.values)):\n            self.segments.append((seg_res[-1], len(self.df.values)))\n            seg_res.append(len(self.df.values))\n\n\n    def run(self):\n\"\"\"\n        Run subsampler\n\n        Returns:\n            (list[int]): indexes of the remaining samples\n        \"\"\"\n        # define window size\n        self.ws = self.__fourier_window()\n\n        # build list of segment\n        self.dataset_segmentation()\n\n        # compute entropy moving window\n        self.moving_window_analysis()\n\n        # extracting subsampling procedure results\n        idxs = self.extract_indexes()\n\n        return idxs\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic.__fourier_window","title":"<code>__fourier_window()</code>","text":"<p>Compute window size based on Fourier analysis performed on dataframe</p> <p>Returns:</p> Type Description <code>int</code> <p>window size</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py</code> <pre><code>def __fourier_window(self):\n\"\"\"\n    Compute window size based on Fourier analysis performed on dataframe\n\n    Returns:\n        (int): window size\n    \"\"\"\n    N, dim = self.df.shape\n    xf = rfftfreq(N, self.sampling_time)\n    w_array = list()\n    for i in range(0, dim):\n        yf = np.abs(rfft(self.df.values[:, i]))\n\n        peak_indices, _ = scipy.signal.find_peaks(yf)\n        highest_peak_index = peak_indices[np.argmax(yf[peak_indices])]\n        w_array.append(ceil(1 / (2 * xf[highest_peak_index]) / self.sampling_time))\n        # fig, ax = pl.subplots()\n        # ax.plot(xf, yf)\n        # ax.plot(xf[highest_peak_index], np.abs(yf[highest_peak_index]), \"x\")\n        # pl.show()\n    return min(w_array)\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic.__init__","title":"<code>__init__(sampling_time, entropy_threshold)</code>","text":"<p>WSFFTStatic class constructor</p> <p>Parameters:</p> Name Type Description Default <code>sampling_time</code> <code>float</code> <p>timeseries sampling time</p> required <code>entropy_threshold</code> <code>float</code> <p>entropy threshold</p> required Source code in <code>fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py</code> <pre><code>def __init__(self, sampling_time, entropy_threshold):\n\"\"\"\n    WSFFTStatic class constructor\n\n    Args:\n        sampling_time (float): timeseries sampling time\n        entropy_threshold (float): entropy threshold\n    \"\"\"\n    SubsamplingMethod.__init__(self, SSMode.WSFFTStatic)\n    EntropyBasedMethod.__init__(self, entropy_threshold)\n    self.sampling_time = sampling_time\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic.dataset_segmentation","title":"<code>dataset_segmentation()</code>","text":"<p>Segments dataset with a fixed window size</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py</code> <pre><code>def dataset_segmentation(self):\n\"\"\"\n    Segments dataset with a fixed window size\n    \"\"\"\n    seg_res = [i for i in range(0, len(self.df.values), self.ws)]\n    self.segments = [(i, i + self.ws) for i in range(0, len(self.df.values) - self.ws, self.ws)]\n    if not seg_res.__contains__(len(self.df.values)):\n        self.segments.append((seg_res[-1], len(self.df.values)))\n        seg_res.append(len(self.df.values))\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic.run","title":"<code>run()</code>","text":"<p>Run subsampler</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>indexes of the remaining samples</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py</code> <pre><code>def run(self):\n\"\"\"\n    Run subsampler\n\n    Returns:\n        (list[int]): indexes of the remaining samples\n    \"\"\"\n    # define window size\n    self.ws = self.__fourier_window()\n\n    # build list of segment\n    self.dataset_segmentation()\n\n    # compute entropy moving window\n    self.moving_window_analysis()\n\n    # extracting subsampling procedure results\n    idxs = self.extract_indexes()\n\n    return idxs\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSStatic.WSStatic","title":"<code>WSStatic</code>","text":"<p>         Bases: <code>SubsamplingMethod</code>, <code>EntropyBasedMethod</code></p> <p>Entropy based subsampling method with static window size</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSStatic.py</code> <pre><code>class WSStatic(SubsamplingMethod, EntropyBasedMethod):\n\"\"\"\n    Entropy based subsampling method with static window size\n    \"\"\"\n    def __init__(self, window_size, entropy_threshold):\n\"\"\"\n        WSStatic class constructor\n\n        Args:\n            window_size (int): minimun window size\n            entropy_threshold (float): entropy threshold\n\n        Raises:\n            ValueError: if window_size == None\n        \"\"\"\n\n        SubsamplingMethod.__init__(self, SSMode.WSDynamic)\n        EntropyBasedMethod.__init__(self, entropy_threshold)\n        if window_size is None:\n            raise ValueError(\"window_type = STATIC but window_size not specified\")\n        self.ws = window_size\n\n\n    def dataset_segmentation(self):\n\"\"\"\n        Segments dataset with a fixed window size\n        \"\"\"\n        seg_res = [i for i in range(0, len(self.df.values), self.ws)]\n        self.segments = [(i, i + self.ws) for i in range(0, len(self.df.values) - self.ws, self.ws)]\n        if not seg_res.__contains__(len(self.df.values)):\n            self.segments.append((seg_res[-1], len(self.df.values)))\n            seg_res.append(len(self.df.values))\n\n\n    def run(self):\n\"\"\"\n        Run subsampler\n\n        Returns:\n            (list[int]): indexes of the remaining samples\n        \"\"\"\n        # build list of segment\n        self.dataset_segmentation()\n\n        # compute entropy moving window\n        self.moving_window_analysis()\n\n        # extracting subsampling procedure results\n        idxs = self.extract_indexes()\n\n        return idxs\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSStatic.WSStatic.__init__","title":"<code>__init__(window_size, entropy_threshold)</code>","text":"<p>WSStatic class constructor</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>minimun window size</p> required <code>entropy_threshold</code> <code>float</code> <p>entropy threshold</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if window_size == None</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSStatic.py</code> <pre><code>def __init__(self, window_size, entropy_threshold):\n\"\"\"\n    WSStatic class constructor\n\n    Args:\n        window_size (int): minimun window size\n        entropy_threshold (float): entropy threshold\n\n    Raises:\n        ValueError: if window_size == None\n    \"\"\"\n\n    SubsamplingMethod.__init__(self, SSMode.WSDynamic)\n    EntropyBasedMethod.__init__(self, entropy_threshold)\n    if window_size is None:\n        raise ValueError(\"window_type = STATIC but window_size not specified\")\n    self.ws = window_size\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSStatic.WSStatic.dataset_segmentation","title":"<code>dataset_segmentation()</code>","text":"<p>Segments dataset with a fixed window size</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSStatic.py</code> <pre><code>def dataset_segmentation(self):\n\"\"\"\n    Segments dataset with a fixed window size\n    \"\"\"\n    seg_res = [i for i in range(0, len(self.df.values), self.ws)]\n    self.segments = [(i, i + self.ws) for i in range(0, len(self.df.values) - self.ws, self.ws)]\n    if not seg_res.__contains__(len(self.df.values)):\n        self.segments.append((seg_res[-1], len(self.df.values)))\n        seg_res.append(len(self.df.values))\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSStatic.WSStatic.run","title":"<code>run()</code>","text":"<p>Run subsampler</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>indexes of the remaining samples</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSStatic.py</code> <pre><code>def run(self):\n\"\"\"\n    Run subsampler\n\n    Returns:\n        (list[int]): indexes of the remaining samples\n    \"\"\"\n    # build list of segment\n    self.dataset_segmentation()\n\n    # compute entropy moving window\n    self.moving_window_analysis()\n\n    # extracting subsampling procedure results\n    idxs = self.extract_indexes()\n\n    return idxs\n</code></pre>"}]}