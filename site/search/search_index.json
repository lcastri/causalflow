{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#causalflow-a-collection-of-causal-discovery-methods-from-time-series","title":"CausalFlow: a collection of Causal Discovery Methods from Time-series","text":"<p>CausalFlow is a python library for causal analysis from time-series data. It comprises:</p> <ul> <li> F-PCMCI - Filtered-PCMCI</li> <li> CAnDOIT - CAusal Discovery with Observational and Interventional data from Time-series</li> <li>RandomGraph</li> <li>Other causal discovery methods all within the same framework</li> </ul>"},{"location":"#useful-links","title":"Useful links","text":"<ul> <li> F-PCMCI:   L. Castri, S. Mghames, M. Hanheide and N. Bellotto (2023). Enhancing Causal Discovery from Robot Sensor Data in Dynamic Scenarios,   Proceedings of the Conference on Causal Learning and Reasoning (CLeaR). <code>@inproceedings{castri2023enhancing,     title={Enhancing Causal Discovery from Robot Sensor Data in Dynamic Scenarios},     author={Castri, Luca and Mghames, Sariah and Hanheide, Marc and Bellotto, Nicola},     booktitle={Conference on Causal Learning and Reasoning},     pages={243--258},     year={2023},     organization={PMLR}   }</code></li> <li> CAnDOIT:   L. Castri, S. Mghames, M. Hanheide and N. Bellotto (2024). CAnDOIT: Causal Discovery with Observational and Interventional Data from Time-Series,   Advanced Intelligent System. <code>BibTex coming soon!</code></li> <li>Tutorials [Coming soon..]</li> </ul>"},{"location":"#f-pcmci","title":"F-PCMCI","text":"<p>Extension of the state-of-the-art causal discovery method PCMCI, augmented with a feature-selection method based on Transfer Entropy. The algorithm, starting from a prefixed set of variables, identifies the correct subset of features and a hypothetical causal model between them. Then, using the selected features and the hypothetical causal model, the causal discovery is executed. This refined set of variables and the list of potential causal links between them contribute to achieving faster and more accurate causal discovery.</p> <p>In the following, an example demonstrating the main functionality of F-PCMCI is presented, along with a comparison between causal models obtained by PCMCI and F-PCMCI causal discovery algorithms using the same data. The dataset consists of a 7-variables system defined as follows:</p> <p> </p> <pre><code>min_lag = 1\nmax_lag = 1\nnp.random.seed(1)\nnsample = 1500\nnfeature = 7\n\nd = np.random.random(size = (nsample, feature))\nfor t in range(max_lag, nsample):\n  d[t, 0] += 2 * d[t-1, 1] + 3 * d[t-1, 3]\n  d[t, 2] += 1.1 * d[t-1, 1]**2\n  d[t, 3] += d[t-1, 3] * d[t-1, 2]\n  d[t, 4] += d[t-1, 4] + d[t-1, 5] * d[t-1, 0]\n</code></pre> Causal Model by PCMCI Causal Model by F-PCMCI Execution time ~ 8min 40sec Execution time ~ 3min 00sec <p>F-PCMCI removes the variable  from the causal graph (since isolated), and generate the correct causal model. In contrast, PCMCI retains  leading to the wrong causal structure. Specifically, a spurious link  -&gt;  appears in the causal graph derived by PCMCI.</p>"},{"location":"#candoit","title":"CAnDOIT","text":"<p>CAnDOIT extends LPCMCI, allowing the incorporation of interventional data into the causal discovery process alongside observational data. Like its predecessor, CAnDOIT can handle both lagged and contemporaneous dependencies, as well as latent variables.</p>"},{"location":"#example","title":"Example","text":"<p>In the following example, taken from one of the tigramite tutorials (this), we demonstrate CAnDOIT's ability to incorporate and leverage interventional data to improve the accuracy of causal analysis. The example involves a system of equations with four variables:</p> <p> </p> <p>Note that  is a latent confounder of  and . This system of equations generates the time-series data in the observational domain, which is then used by LPCMCI for causal discovery analysis.</p> <pre><code>tau_max = 2\npc_alpha = 0.05\nnp.random.seed(19)\nnsample_obs = 500\nnfeature = 4\n\nd = np.random.random(size = (nsample_obs, nfeature))\nfor t in range(tau_max, nsample_obs):\n  d[t, 0] += 0.9 * d[t-1, 0] + 0.6 * d[t, 1]\n  d[t, 2] += 0.9 * d[t-1, 2] + 0.4 * d[t-1, 1]\n  d[t, 3] += 0.9 * d[t-1, 3] - 0.5 * d[t-2, 2]\n\n# Remove the unobserved component time series\ndata_obs = d[:, [0, 2, 3]]\n\nvar_names = ['X_0', 'X_2', 'X_3']\nd_obs = Data(data_obs, vars = var_names)\nd_obs.plot_timeseries()\n\nlpcmci = LPCMCI(d_obs,\n                min_lag = 0,\n                max_lag = tau_max,\n                val_condtest = ParCorr(significance='analytic'),\n                alpha = pc_alpha)\n\n# Run LPCMCI\nlpcmci_cm = lpcmci.run()\nlpcmci_cm.ts_dag(node_size = 4, min_width = 1.5, max_width = 1.5, \n                 x_disp=0.5, y_disp=0.2, font_size=10)\n</code></pre> Observational Data Causal Model by LPCMCI <p>As you can see from LPCMCI's result, the method correctly identifies the bidirected link (indicating the presence of a latent confounder) between  and . However, the final causal model presents uncertainty regarding the link  o-&gt; . Specifically, the final causal model is a PAG that represents two MAGs: the first with  &lt;-&gt; , and the second with  -&gt; .</p> <p>Now, let's introduce interventional data and examine its benefits. In this case, we perform a hard intervention on the variable , meaning we replace its equation with a constant value corresponding to the intervention (in this case, ).</p> <pre><code>nsample_int = 150\nint_data = dict()\n\n# Intervention on X_2.\nd_int = np.random.random(size = (nsample_int, nfeature))\nd_int[0:tau_max, :] = d[len(d)-tau_max:,:]\nd_int[:, 2] = 3 * np.ones(shape = (nsample_int,)) \nfor t in range(tau_max, nsample_int):\n    d_int[t, 0] += 0.9 * d_int[t-1, 0] + 0.6 * d_int[t, 1]\n    d_int[t, 3] += 0.9 * d_int[t-1, 3] - 0.5 * d_int[t-2, 2]\n\ndata_int = d_int[:, [0, 2, 3]]\ndf_int = Data(data_int, vars = var_names)\nint_data['X_2'] =  df_int\n\ncandoit = CAnDOIT(d_obs, \n                  int_data,\n                  alpha = pc_alpha, \n                  min_lag = 0, \n                  max_lag = tau_max, \n                  val_condtest = ParCorr(significance='analytic'))\n\ncandoit_cm = candoit.run()\ncandoit_cm.ts_dag(node_size = 4, min_width = 1.5, max_width = 1.5, \n                  x_disp=0.5, y_disp=0.2, font_size=10)\n</code></pre> Observational &amp; Interventional Data Causal Model by CAnDOIT <p>CAnDOIT, like LPCMCI, correctly detects the bidirected link  &lt;-&gt; . Additionally, by incorporating interventional data, CAnDOIT resolves the uncertainty regarding the link  o-&gt; , resulting in a reduction of the PAG size. Specifically, the PAG found by CAnDOIT is the representaion of only one MAG.</p>"},{"location":"#robotics-application-of-candoit","title":"Robotics application of CAnDOIT","text":"<p>In this section, we discuss an application of CAnDOIT in a robotic scenario. We designed an experiment to learn the causal model in a hypothetical robot arm application equipped with a camera. For this application, we utilised Causal World, which models a TriFinger robot, a floor, and a stage. </p> <p>In our case, we use only one finger of the robot, with the finger's end effector equipped with a camera. The scenario consists of a cube placed at the centre of the floor, surrounded by a white stage.  The colour's brightness () of the cube and the floor is modelled as a function of the end-effector height (), its absolute velocity (), and the distance between the end-effector and the cube (). This model captures the shading and blurring effects on the cube. In contrast, the floor, being darker and larger than the cube, is only affected by the end effector's height.</p> <p>Note that , , and  are obtained directly from the simulator and not explicitly modelled, while the ground-truth structural causal model for the floor colour () and cube colour () is expressed as follows:</p> <p> </p> <p>This model is used to generate observational data, which is then used by LPCMCI and CAnDOIT to reconstruct the causal model. For the interventional domain instead, we substitute the equation modelling  with a constant colour (green) and collect the data for the causal analysis conducted by CAnDOIT. Note that, for both the obervational and interventional domains,  is considered as latent confounder between  and .</p> Observational dataset Interventional dataset Ground-truth Causal Model Causal Model by LPCMCI Causal Model by CAnDOIT <p>Also in this experiment, we can see the benefit of using intervention data alongside the observations. LPCMCI is unable to orient the contemporaneous (spurious) link between  and  due to the hidden confounder . This results in the ambiguous link  o-o , which does not encode the correct link &lt;-&gt;. Instead CAnDOIT, using interventional data, correctly identifies the bidirected link  &lt;-&gt; , decreasing once again the uncertainty level and increasing the accuracy of the reconstructed causal model.</p>"},{"location":"#randomgraph","title":"RandomGraph","text":"<p>RandomGraph is a random-model generator capable of creating random systems of equations with various properties: linear, nonlinear, lagged and/or contemporaneous dependencies, and hidden confounders.  This tool offers several adjustable parameters, listed as follows:</p> <ul> <li>time-series length;</li> <li>number of observable variables;</li> <li>number of observable parents per variable (link density);</li> <li>number of hidden confounders;</li> <li>number of confounded variables per hidden confounder;</li> <li>noise configuration, e.g. Gaussian noise ;</li> <li>minimum  and maximum  time delay to consider in the equations;</li> <li>coefficient range of the equations' terms;</li> <li>functional forms applied to the equations' terms: , where  stands for none;</li> <li>operators used to link various equations terms: .</li> </ul> <p>RandomGraph outputs a graph, the associated system of equations, and observational data. Additionally, it provides the option to generate interventional data.</p>"},{"location":"#example-linear-random-graph","title":"Example - Linear Random Graph","text":"<pre><code>noise_uniform = (NoiseType.Uniform, -0.5, 0.5)\nnoise_gaussian = (NoiseType.Gaussian, 0, 1)\nnoise_weibull = (NoiseType.Weibull, 2, 1)\nRG = RandomGraph(nvars = 5, \n                 nsamples = 1000, \n                 link_density = 3, \n                 coeff_range = (0.1, 0.5), \n                 max_exp = 2, \n                 min_lag = 0, \n                 max_lag = 3, \n                 noise_config = random.choice([noise_uniform, noise_gaussian, noise_weibull]),\n                 functions = [''], \n                 operators = ['+', '-'], \n                 n_hidden_confounders = 2)\nRG.gen_equations()\nRG.ts_dag(withHidden = True)\n</code></pre>"},{"location":"#example-nonlinear-random-graph","title":"Example - Nonlinear Random Graph","text":"<pre><code>noise_uniform = (NoiseType.Uniform, -0.5, 0.5)\nnoise_gaussian = (NoiseType.Gaussian, 0, 1)\nnoise_weibull = (NoiseType.Weibull, 2, 1)\nRG = RandomGraph(nvars = 5, \n                 nsamples = 1000, \n                 link_density = 3, \n                 coeff_range = (0.1, 0.5), \n                 max_exp = 2, \n                 min_lag = 0, \n                 max_lag = 3, \n                 noise_config = random.choice([noise_uniform, noise_gaussian, noise_weibull]),\n                 functions = ['','sin', 'cos', 'exp', 'abs', 'pow'], \n                 operators = ['+', '-', '*', '/'], \n                 n_hidden_confounders = 2)\nRG.gen_equations()\nRG.ts_dag(withHidden = True)\n</code></pre> Linear Random Graph Nonlinear Random Graph Linear model Nonlinear model Lagged dependencies Lagged dependencies Contemporaneous dependencies Contemporaneous dependencies 2 hidden confounders 2 hidden confounders"},{"location":"#example-random-graph-with-interventional-data","title":"Example - Random Graph with Interventional Data","text":"<pre><code>noise_gaussian = (NoiseType.Gaussian, 0, 1)\nRS = RandomGraph(nvars = 5, \n                 nsamples = 1500, \n                 link_density = 3, \n                 coeff_range = (0.1, 0.5), \n                 max_exp = 2, \n                 min_lag = 0, \n                 max_lag = 3, \n                 noise_config = noise_gaussian,\n                 functions = ['','sin', 'cos', 'exp', 'abs', 'pow'], \n                 operators = ['+', '-', '*', '/'], \n                 n_hidden_confounders = 2)\nRS.gen_equations()\n\nd_obs_wH, d_obs = RS.gen_obs_ts()\nd_obs.plot_timeseries()\n\nd_int = RS.intervene(intvar, nsample_int, random.uniform(5, 10), d_obs.d)\nd_int[intvar].plot_timeseries()\n</code></pre> Observational Data Interventional Data"},{"location":"#other-causal-discovery-algorithms","title":"Other Causal Discovery Algorithms","text":"<p>Although the main contribution of this repository is to present the CAnDOIT and F-PCMCI algorithms, other causal discovery methods have been included for benchmarking purposes. Consequently, CausalFlow offers a collection of causal discovery methods, beyond F-PCMCI and CAnDOIT, that output time-series graphs (graphs that specify the lag for each link). These methods are listed as follows:</p> <ul> <li>DYNOTEARS - from the causalnex package;</li> <li>PCMCI - from the tigramite package;</li> <li>PCMCI+ - from the tigramite package;</li> <li>LPCMCI - from the tigramite package;</li> <li>TCDF - from the causal_discovery_for_time_series package;</li> <li>tsFCI - from the causal_discovery_for_time_series package;</li> <li>VarLiNGAM - from the lingam package;</li> </ul> <p>Some algorithms are imported from other languages such as R and Java and are then wrapped in Python. Having the majority of causal discovery methods integrated into a single framework, which handles various types of inputs and outputs causal models, can facilitate the use of these algorithms. </p> Algorithm Observations Feature Selection Interventions DYNOTEARS \u2705 \u274c \u274c PCMCI \u2705 \u274c \u274c PCMCI+ \u2705 \u274c \u274c LPCMCI \u2705 \u274c \u274c TCDF \u2705 \u274c \u274c tsFCI \u2705 \u274c \u274c VarLiNGAM \u2705 \u274c \u274c F-PCMCI \u2705 \u2705 \u274c CAnDOIT \u2705 \u274c \u2705"},{"location":"#citation","title":"Citation","text":"<p>Please consider citing the following papers depending on which method you use:</p> <ul> <li> F-PCMCI:   L. Castri, S. Mghames, M. Hanheide and N. Bellotto (2023). Enhancing Causal Discovery from Robot Sensor Data in Dynamic Scenarios,   Proceedings of the Conference on Causal Learning and Reasoning (CLeaR). <code>@inproceedings{castri2023enhancing,     title={Enhancing Causal Discovery from Robot Sensor Data in Dynamic Scenarios},     author={Castri, Luca and Mghames, Sariah and Hanheide, Marc and Bellotto, Nicola},     booktitle={Conference on Causal Learning and Reasoning},     pages={243--258},     year={2023},     organization={PMLR}   }</code></li> <li> CAnDOIT:   L. Castri, S. Mghames, M. Hanheide and N. Bellotto (2024). CAnDOIT: Causal Discovery with Observational and Interventional Data from Time-Series,   Advanced Intelligent System. <code>BibTex coming soon!</code></li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>pandas&gt;=1.5.2</li> <li>netgraph&gt;=4.10.2</li> <li>networkx&gt;=2.8.6</li> <li>ruptures&gt;=1.1.7</li> <li>scikit_learn&gt;=1.1.3</li> <li>torch&gt;=1.11.0</li> <li>gpytorch&gt;=1.4</li> <li>dcor&gt;=0.5.3</li> <li>h5py&gt;=3.7.0   </li> <li>jpype1&gt;=1.5.0</li> <li>mpmath&gt;=1.3.0  </li> <li>causalnex&gt;=0.12.1</li> <li>lingam&gt;=1.8.2</li> <li>tigramite&gt;=5.1.0.3</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Before installing CausalFlow, you need to install Java and the IDTxl package used for the feature-selection process, following the guide described here. Once complete, you can install the current release of <code>CausalFlow</code> with:</p> <pre><code># COMING SOON: pip install causalflow\n</code></pre> <p>For a complete installation Java - IDTxl - CausalFlow, follow the following procedure.</p>"},{"location":"#1-java-installation","title":"1 - Java installation","text":"<p>Verify that you have not already installed Java:</p> <pre><code>java -version\n</code></pre> <p>if the latter returns <code>Command 'java' not found, ...</code>, you can install Java by the following commands, otherwise you can jump to IDTxl installation.</p> <pre><code># Java\nsudo apt-get update\nsudo apt install default-jdk\n</code></pre> <p>Then, you need to add JAVA_HOME to the environment</p> <pre><code>sudo nano /etc/environment\nJAVA_HOME=\"/lib/jvm/java-11-openjdk-amd64/bin/java\" # Paste the JAVA_HOME assignment at the bottom of the file\nsource /etc/environment\n</code></pre>"},{"location":"#2-idtxl-installation","title":"2 - IDTxl installation","text":"<pre><code># IDTxl\ngit clone https://github.com/pwollstadt/IDTxl.git\ncd IDTxl\npip install -e .\n</code></pre>"},{"location":"#3-causalflow-installation","title":"3 - CausalFlow installation","text":"<pre><code># COMING SOON: pip install causalflow\n</code></pre>"},{"location":"#recent-changes","title":"Recent changes","text":"Version Changes 4.0.0 package published"},{"location":"CAnDOIT/","title":"CAnDOIT","text":"<p>This module provides the CausalDiscoveryMethod class.</p> Classes <p>CausalDiscoveryMethod: abstract class used by all the causal discovery algorithms.</p> <p>This module provides the FPCMCI class.</p> Classes <p>FPCMCI: class containing the FPCMCI causal discovery algorithm.</p> <p>This module provides the CAnDOIT class.</p> Classes <p>CAnDOIT: class containing the CAnDOIT causal discovery algorithm.</p> <p>This module provides the DYNOTEARS class.</p> Classes <p>DYNOTEARS: class containing the DYNOTEARS causal discovery algorithm.</p> <p>This module provides the LPCMCI class.</p> Classes <p>LPCMCI: class containing the LPCMCI causal discovery algorithm.</p> <p>This module provides the PCMCI class.</p> Classes <p>PCMCI: class containing the PCMCI causal discovery algorithm.</p> <p>This module provides the PCMCI+ class.</p> Classes <p>PCMCIplus: class containing the PCMCI+ causal discovery algorithm.</p> <p>This module provides the TCDF class.</p> Classes <p>TCDF: class containing the TCDF causal discovery algorithm.</p> <p>This module provides the tsFCI class.</p> Classes <p>tsFCI: class containing the tsFCI causal discovery algorithm.</p> <p>This module provides the VarLiNGAM class.</p> Classes <p>VarLiNGAM: class containing the VarLiNGAM causal discovery algorithm.</p>"},{"location":"CAnDOIT/#causalflow.causal_discovery.CausalDiscoveryMethod.CausalDiscoveryMethod","title":"<code>CausalDiscoveryMethod</code>","text":"<p>             Bases: <code>ABC</code></p> <p>CausalDiscoveryMethod class.</p> <p>CausalDiscoveryMethod is an abstract causal discovery method for  large-scale time series datasets.</p> Source code in <code>causalflow/causal_discovery/CausalDiscoveryMethod.py</code> <pre><code>class CausalDiscoveryMethod(ABC):\n\"\"\"\n    CausalDiscoveryMethod class.\n\n    CausalDiscoveryMethod is an abstract causal discovery method for \n    large-scale time series datasets.\n    \"\"\"\n\n    def __init__(self, \n                 data: Data, \n                 min_lag, max_lag, \n                 verbosity: CPLevel, \n                 alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class contructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n\n        \"\"\"\n        self.data = data\n        self.alpha = alpha\n        self.min_lag = min_lag\n        self.max_lag = max_lag\n        self.CM = DAG(self.data.features, min_lag, max_lag, neglect_only_autodep)\n        self.neglect_only_autodep = neglect_only_autodep\n\n        self.resfolder = resfolder\n        self.respath, self.dag_path, self.ts_dag_path = None, None, None\n        if resfolder is not None:\n            logpath, self.respath, self.dag_path, self.ts_dag_path = utils.get_selectorpath(resfolder)  \n            self.logger = Logger(logpath, clean_cls)\n            sys.stdout = self.logger\n\n        CP.set_verbosity(verbosity)\n\n\n    @abstractmethod\n    def run(self) -&gt; DAG:\n\"\"\"\n        Run causal discovery method.\n\n        Returns:\n            DAG: causal model.\n        \"\"\"\n        pass\n\n\n    def load(self, res_path):\n\"\"\"\n        Load previously estimated result .\n\n        Args:\n            res_path (str): pickle file path.\n        \"\"\"\n        with open(res_path, 'rb') as f:\n            r = pickle.load(f)\n            self.CM = r['causal_model']\n            self.alpha = r['alpha']\n            self.dag_path = r['dag_path']\n            self.ts_dag_path = r['ts_dag_path']\n\n\n    def save(self):\n\"\"\"Save causal discovery result as pickle file if resfolder is set.\"\"\"\n        if self.respath is not None:\n            if self.CM:\n                res = dict()\n                res['causal_model'] = copy.deepcopy(self.CM)\n                res['alpha'] = self.alpha\n                res['dag_path'] = self.dag_path\n                res['ts_dag_path'] = self.ts_dag_path\n                with open(self.respath, 'wb') as resfile:\n                    pickle.dump(res, resfile)\n            else:\n                CP.warning(\"Causal model impossible to save\")\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.CausalDiscoveryMethod.CausalDiscoveryMethod.__init__","title":"<code>__init__(data, min_lag, max_lag, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class contructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/CausalDiscoveryMethod.py</code> <pre><code>def __init__(self, \n             data: Data, \n             min_lag, max_lag, \n             verbosity: CPLevel, \n             alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class contructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n\n    \"\"\"\n    self.data = data\n    self.alpha = alpha\n    self.min_lag = min_lag\n    self.max_lag = max_lag\n    self.CM = DAG(self.data.features, min_lag, max_lag, neglect_only_autodep)\n    self.neglect_only_autodep = neglect_only_autodep\n\n    self.resfolder = resfolder\n    self.respath, self.dag_path, self.ts_dag_path = None, None, None\n    if resfolder is not None:\n        logpath, self.respath, self.dag_path, self.ts_dag_path = utils.get_selectorpath(resfolder)  \n        self.logger = Logger(logpath, clean_cls)\n        sys.stdout = self.logger\n\n    CP.set_verbosity(verbosity)\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.CausalDiscoveryMethod.CausalDiscoveryMethod.load","title":"<code>load(res_path)</code>","text":"<p>Load previously estimated result .</p> <p>Parameters:</p> Name Type Description Default <code>res_path</code> <code>str</code> <p>pickle file path.</p> required Source code in <code>causalflow/causal_discovery/CausalDiscoveryMethod.py</code> <pre><code>def load(self, res_path):\n\"\"\"\n    Load previously estimated result .\n\n    Args:\n        res_path (str): pickle file path.\n    \"\"\"\n    with open(res_path, 'rb') as f:\n        r = pickle.load(f)\n        self.CM = r['causal_model']\n        self.alpha = r['alpha']\n        self.dag_path = r['dag_path']\n        self.ts_dag_path = r['ts_dag_path']\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.CausalDiscoveryMethod.CausalDiscoveryMethod.run","title":"<code>run()</code>  <code>abstractmethod</code>","text":"<p>Run causal discovery method.</p> <p>Returns:</p> Name Type Description <code>DAG</code> <code>DAG</code> <p>causal model.</p> Source code in <code>causalflow/causal_discovery/CausalDiscoveryMethod.py</code> <pre><code>@abstractmethod\ndef run(self) -&gt; DAG:\n\"\"\"\n    Run causal discovery method.\n\n    Returns:\n        DAG: causal model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.CausalDiscoveryMethod.CausalDiscoveryMethod.save","title":"<code>save()</code>","text":"<p>Save causal discovery result as pickle file if resfolder is set.</p> Source code in <code>causalflow/causal_discovery/CausalDiscoveryMethod.py</code> <pre><code>def save(self):\n\"\"\"Save causal discovery result as pickle file if resfolder is set.\"\"\"\n    if self.respath is not None:\n        if self.CM:\n            res = dict()\n            res['causal_model'] = copy.deepcopy(self.CM)\n            res['alpha'] = self.alpha\n            res['dag_path'] = self.dag_path\n            res['ts_dag_path'] = self.ts_dag_path\n            with open(self.respath, 'wb') as resfile:\n                pickle.dump(res, resfile)\n        else:\n            CP.warning(\"Causal model impossible to save\")\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.FPCMCI.FPCMCI","title":"<code>FPCMCI</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>F-PCMCI causal discovery method.</p> Source code in <code>causalflow/causal_discovery/FPCMCI.py</code> <pre><code>class FPCMCI(CausalDiscoveryMethod):\n\"\"\"F-PCMCI causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data: Data, \n                 min_lag, max_lag, \n                 sel_method: SelectionMethod, val_condtest: CondIndTest, \n                 verbosity: CPLevel, \n                 f_alpha = 0.05, \n                 alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class contructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            sel_method (SelectionMethod): selection method.\n            val_condtest (CondIndTest): validation method.\n            verbosity (CPLevel): verbosity level.\n            f_alpha (float, optional): filter significance level. Defaults to 0.05.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n        self.f_alpha = f_alpha\n        self.sel_method = sel_method\n\n        self.validator = myPCMCI(self.alpha, min_lag, max_lag, val_condtest, verbosity, neglect_only_autodep = neglect_only_autodep)       \n\n\n    def run_filter(self):\n\"\"\"Run filter method.\"\"\"\n        CP.info(\"\\n\")\n        CP.info(DASH)\n        CP.info(\"Selecting relevant features among: \" + str(self.data.features))\n        CP.info(\"Selection method: \" + self.sel_method.name)\n        CP.info(\"Significance level: \" + str(self.f_alpha))\n        CP.info(\"Max lag time: \" + str(self.max_lag))\n        CP.info(\"Min lag time: \" + str(self.min_lag))\n        CP.info(\"Data length: \" + str(self.data.T))\n\n        self.sel_method.initialise(self.data, self.f_alpha, self.min_lag, self.max_lag, self.CM)\n        self.CM = self.sel_method.compute_dependencies()  \n\n\n    def run(self, remove_unneeded = True, nofilter = False) -&gt; DAG:\n\"\"\"\n        Run F-PCMCI.\n\n        Args:\n            remove_unneeded (bool, optional): Bit to remove unneeded (isolated) variables. Defaults to True.\n            nofilter (bool, optional): Bit to run F-PCMCI without filter. Defaults to False.\n\n        Returns:\n            DAG: causal model.\n        \"\"\"\n        link_assumptions = None\n\n        if not nofilter:\n            ## 1. FILTER\n            self.run_filter()\n\n            # list of selected features based on filter dependencies\n            self.CM.remove_unneeded_features()\n            if not self.CM.features: return None, None\n\n            ## 2. VALIDATOR\n            # shrink dataframe d by using the filter result\n            self.data.shrink(self.CM.features)\n\n            # selected links to check by the validator\n            link_assumptions = self.CM.get_link_assumptions()\n\n            # calculate dependencies on selected links\n            f_dag = copy.deepcopy(self.CM)\n\n        if self.min_lag != 0:\n            self.CM = self.validator.run(self.data, link_assumptions)\n        else:\n            self.CM = self.validator.run_plus(self.data, link_assumptions)\n\n        # list of selected features based on validator dependencies\n        if remove_unneeded: self.CM.remove_unneeded_features()\n\n        # Saving final causal model\n        if not nofilter: self._print_differences(f_dag, self.CM)\n        self.save()\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n\n\n    def load(self, res_path):\n\"\"\"\n        Load previously estimated result.\n\n        Args:\n            res_path (str): pickle file path.\n        \"\"\"\n        with open(res_path, 'rb') as f:\n            r = pickle.load(f)\n            self.CM = r['causal_model']\n            self.f_alpha = r['filter_alpha']\n            self.alpha = r['alpha']\n            self.dag_path = r['dag_path']\n            self.ts_dag_path = r['ts_dag_path']\n\n\n    def save(self):\n\"\"\"Save causal discovery result as pickle file if resfolder is set.\"\"\"\n        if self.respath is not None:\n            if self.CM:\n                res = dict()\n                res['causal_model'] = copy.deepcopy(self.CM)\n                res['features'] = copy.deepcopy(self.CM.features)\n                res['filter_alpha'] = self.f_alpha\n                res['alpha'] = self.alpha\n                res['dag_path'] = self.dag_path\n                res['ts_dag_path'] = self.ts_dag_path\n                with open(self.respath, 'wb') as resfile:\n                    pickle.dump(res, resfile)\n            else:\n                CP.warning(\"Causal model impossible to save\")\n\n\n    def _print_differences(self, old_dag : DAG, new_dag : DAG):\n\"\"\"\n        Print difference between old and new dependencies.\n\n        Args:\n            old_dep (DAG): old dag.\n            new_dep (DAG): new dag.\n        \"\"\"\n        # Check difference(s) between validator and filter dependencies\n        list_diffs = list()\n        tmp = copy.deepcopy(old_dag)\n        for t in tmp.g:\n            if t not in new_dag.g:\n                list_diffs.append(t)\n                continue\n\n            for s in tmp.g[t].sources:\n                if s not in new_dag.g[t].sources:\n                    list_diffs.append((s[0], s[1], t))\n\n        if list_diffs:\n            CP.info(\"\\n\")\n            CP.info(DASH)\n            CP.info(\"Difference(s):\")\n            for diff in list_diffs: \n                if type(diff) is tuple:\n                    CP.info(\"Removed (\" + str(diff[0]) + \" -\" + str(diff[1]) +\") --&gt; (\" + str(diff[2]) + \")\")\n                else:\n                    CP.info(diff + \" removed\")\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.FPCMCI.FPCMCI.__init__","title":"<code>__init__(data, min_lag, max_lag, sel_method, val_condtest, verbosity, f_alpha=0.05, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class contructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>sel_method</code> <code>SelectionMethod</code> <p>selection method.</p> required <code>val_condtest</code> <code>CondIndTest</code> <p>validation method.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>f_alpha</code> <code>float</code> <p>filter significance level. Defaults to 0.05.</p> <code>0.05</code> <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/FPCMCI.py</code> <pre><code>def __init__(self, \n             data: Data, \n             min_lag, max_lag, \n             sel_method: SelectionMethod, val_condtest: CondIndTest, \n             verbosity: CPLevel, \n             f_alpha = 0.05, \n             alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class contructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        sel_method (SelectionMethod): selection method.\n        val_condtest (CondIndTest): validation method.\n        verbosity (CPLevel): verbosity level.\n        f_alpha (float, optional): filter significance level. Defaults to 0.05.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n    self.f_alpha = f_alpha\n    self.sel_method = sel_method\n\n    self.validator = myPCMCI(self.alpha, min_lag, max_lag, val_condtest, verbosity, neglect_only_autodep = neglect_only_autodep)       \n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.FPCMCI.FPCMCI.load","title":"<code>load(res_path)</code>","text":"<p>Load previously estimated result.</p> <p>Parameters:</p> Name Type Description Default <code>res_path</code> <code>str</code> <p>pickle file path.</p> required Source code in <code>causalflow/causal_discovery/FPCMCI.py</code> <pre><code>def load(self, res_path):\n\"\"\"\n    Load previously estimated result.\n\n    Args:\n        res_path (str): pickle file path.\n    \"\"\"\n    with open(res_path, 'rb') as f:\n        r = pickle.load(f)\n        self.CM = r['causal_model']\n        self.f_alpha = r['filter_alpha']\n        self.alpha = r['alpha']\n        self.dag_path = r['dag_path']\n        self.ts_dag_path = r['ts_dag_path']\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.FPCMCI.FPCMCI.run","title":"<code>run(remove_unneeded=True, nofilter=False)</code>","text":"<p>Run F-PCMCI.</p> <p>Parameters:</p> Name Type Description Default <code>remove_unneeded</code> <code>bool</code> <p>Bit to remove unneeded (isolated) variables. Defaults to True.</p> <code>True</code> <code>nofilter</code> <code>bool</code> <p>Bit to run F-PCMCI without filter. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>DAG</code> <code>DAG</code> <p>causal model.</p> Source code in <code>causalflow/causal_discovery/FPCMCI.py</code> <pre><code>def run(self, remove_unneeded = True, nofilter = False) -&gt; DAG:\n\"\"\"\n    Run F-PCMCI.\n\n    Args:\n        remove_unneeded (bool, optional): Bit to remove unneeded (isolated) variables. Defaults to True.\n        nofilter (bool, optional): Bit to run F-PCMCI without filter. Defaults to False.\n\n    Returns:\n        DAG: causal model.\n    \"\"\"\n    link_assumptions = None\n\n    if not nofilter:\n        ## 1. FILTER\n        self.run_filter()\n\n        # list of selected features based on filter dependencies\n        self.CM.remove_unneeded_features()\n        if not self.CM.features: return None, None\n\n        ## 2. VALIDATOR\n        # shrink dataframe d by using the filter result\n        self.data.shrink(self.CM.features)\n\n        # selected links to check by the validator\n        link_assumptions = self.CM.get_link_assumptions()\n\n        # calculate dependencies on selected links\n        f_dag = copy.deepcopy(self.CM)\n\n    if self.min_lag != 0:\n        self.CM = self.validator.run(self.data, link_assumptions)\n    else:\n        self.CM = self.validator.run_plus(self.data, link_assumptions)\n\n    # list of selected features based on validator dependencies\n    if remove_unneeded: self.CM.remove_unneeded_features()\n\n    # Saving final causal model\n    if not nofilter: self._print_differences(f_dag, self.CM)\n    self.save()\n\n    if self.resfolder is not None: self.logger.close()\n    return self.CM\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.FPCMCI.FPCMCI.run_filter","title":"<code>run_filter()</code>","text":"<p>Run filter method.</p> Source code in <code>causalflow/causal_discovery/FPCMCI.py</code> <pre><code>def run_filter(self):\n\"\"\"Run filter method.\"\"\"\n    CP.info(\"\\n\")\n    CP.info(DASH)\n    CP.info(\"Selecting relevant features among: \" + str(self.data.features))\n    CP.info(\"Selection method: \" + self.sel_method.name)\n    CP.info(\"Significance level: \" + str(self.f_alpha))\n    CP.info(\"Max lag time: \" + str(self.max_lag))\n    CP.info(\"Min lag time: \" + str(self.min_lag))\n    CP.info(\"Data length: \" + str(self.data.T))\n\n    self.sel_method.initialise(self.data, self.f_alpha, self.min_lag, self.max_lag, self.CM)\n    self.CM = self.sel_method.compute_dependencies()  \n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.FPCMCI.FPCMCI.save","title":"<code>save()</code>","text":"<p>Save causal discovery result as pickle file if resfolder is set.</p> Source code in <code>causalflow/causal_discovery/FPCMCI.py</code> <pre><code>def save(self):\n\"\"\"Save causal discovery result as pickle file if resfolder is set.\"\"\"\n    if self.respath is not None:\n        if self.CM:\n            res = dict()\n            res['causal_model'] = copy.deepcopy(self.CM)\n            res['features'] = copy.deepcopy(self.CM.features)\n            res['filter_alpha'] = self.f_alpha\n            res['alpha'] = self.alpha\n            res['dag_path'] = self.dag_path\n            res['ts_dag_path'] = self.ts_dag_path\n            with open(self.respath, 'wb') as resfile:\n                pickle.dump(res, resfile)\n        else:\n            CP.warning(\"Causal model impossible to save\")\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.CAnDOIT.CAnDOIT","title":"<code>CAnDOIT</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>CAnDOIT causal discovery method.</p> Source code in <code>causalflow/causal_discovery/CAnDOIT.py</code> <pre><code>class CAnDOIT(CausalDiscoveryMethod):\n\"\"\"CAnDOIT causal discovery method.\"\"\"\n\n    def __init__(self, \n                 observation_data: Data, \n                 intervention_data: dict, \n                 min_lag, max_lag,\n                 sel_method: SelectionMethod, val_condtest: CondIndTest, \n                 verbosity: CPLevel, \n                 f_alpha = 0.05, \n                 alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 exclude_context = True,\n                 plot_data = False,\n                 clean_cls = True):\n\"\"\"\n        Class contructor.\n\n        Args:\n            observation_data (Data): observational data to analyse.\n            intervention_data (dict): interventional data to analyse in the form {INTERVENTION_VARIABLE : Data (same variables of observation_data)}.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            sel_method (SelectionMethod): selection method.\n            val_condtest (CondIndTest): validation method.\n            verbosity (CPLevel): verbosity level.\n            f_alpha (float, optional): filter significance level. Defaults to 0.05.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            exclude_context (bool, optional): Bit for neglecting context variables. Defaults to False.\n            plot_data (bool, optional): Bit for plotting your data. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        self.obs_data = observation_data\n        self.systems = observation_data.features\n        self.contexts = []\n        self.sys_context = {}\n        for k in intervention_data.keys():\n            self.contexts.append(\"C\" + k)\n            self.sys_context[k] = \"C\" + k\n        self.vars = self.systems + self.contexts\n\n        self.f_alpha = f_alpha\n        self.sel_method = sel_method\n        self.val_condtest = val_condtest\n        self.exclude_context = exclude_context\n        super().__init__(self.obs_data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n        # Create filter and validator data\n        self.filter_data, self.validator_data = self._prepare_data(self.obs_data, intervention_data, plot_data)\n\n\n        CP.info(\"\\n\")\n        CP.info(DASH)\n        CP.info(\"Observational data length: \" + str(observation_data.T))\n        CP.info(\"Interventional data length: \" + str(sum([d.T for d in intervention_data.values()])))\n        CP.info(\"Min lag time: \" + str(min_lag))\n        CP.info(\"Max lag time: \" + str(max_lag))\n        CP.info(\"Filter significance level: \" + str(f_alpha))\n        CP.info(\"PCMCI significance level: \" + str(alpha))\n        CP.info(\"Selection method: \" + sel_method.name)\n\n\n    @property    \n    def isThereInterv(self) -&gt; bool:\n\"\"\"\n        Check whether an intervention is present or not.\n\n        Returns:\n            bool: flag to identify if an intervention is present or not.\n        \"\"\"\n        return len(list(self.sys_context.keys())) &gt; 0\n\n\n    def JCI_assumptions(self):\n\"\"\"Initialise the algorithm initial causal structure with the JCI assumptions.\"\"\"\n        # ! JCI Assmpution 1: No system variable causes any context variable\n        # ! JCI Assmpution 2: No context variable is confounded with a system variable\n        # ! JCI Assmpution 3: The context distribution contains no (conditional) independences\n\n        knowledge = {self.vars.index(f): dict() for f in self.vars}\n\n        # ! JCI Assmpution 1\n        for k in self.contexts:\n            for x in self.systems:\n                for tau_i in range(0, self.max_lag + 1):\n                    knowledge[self.vars.index(k)][(self.vars.index(x), -tau_i)] = ''\n\n        # ! JCI Assmpution 2\n        for k in self.contexts:\n            for x in self.systems:\n                if x not in self.sys_context or (x in self.sys_context and k != self.sys_context[x]):\n                    for tau_i in range(0, self.max_lag + 1): knowledge[self.vars.index(x)][(self.vars.index(k), -tau_i)] = ''\n                elif x in self.sys_context and k == self.sys_context[x]:\n                    knowledge[self.vars.index(x)][(self.vars.index(k), 0)] = '--&gt;'\n                    knowledge[self.vars.index(k)][(self.vars.index(x), 0)] = '&lt;--'\n                    for tau_i in range(1, self.max_lag + 1): knowledge[self.vars.index(x)][(self.vars.index(k), -tau_i)] = ''\n                    for tau_i in range(1, self.max_lag + 1): knowledge[self.vars.index(k)][(self.vars.index(x), -tau_i)] = ''\n\n        # ! JCI Assmpution 3\n        for k1 in self.contexts:\n            for k2 in remove_from_list(self.contexts, k1):\n                knowledge[self.vars.index(k1)][(self.vars.index(k2), 0)] = '&lt;-&gt;'\n                # for tau_i in range(0, self.max_lag + 1): knowledge[self.vars.index(k1)][(self.vars.index(k2), -tau_i)] = '&lt;-&gt;'\n\n        # ! This models the context variables as chain across different time steps\n        for k in self.contexts:\n            for tau_i in range(1, self.max_lag + 1):\n                knowledge[self.vars.index(k)][(self.vars.index(k), -tau_i)] = '--&gt;' if tau_i == 1 else ''\n\n\n\n\n        out = {}\n        for j in range(len(self.vars)):\n            inner_dict = {} \n\n            for i in range(len(self.vars)):\n                for tau_i in range(0, self.max_lag + 1):\n                    if tau_i &gt; 0 or i != j:\n                        value = \"o?&gt;\" if tau_i &gt; 0 else \"o?o\"\n                        inner_dict[(i, -tau_i)] = value\n\n            out[j] = inner_dict\n\n        for j, links_j in knowledge.items():\n            for (i, lag_i), link_ij in links_j.items():\n                if link_ij == \"\":\n                    del out[j][(i, lag_i)]\n                else: \n                    out[j][(i, lag_i)] = link_ij\n        return out\n\n\n    def run_filter(self):\n\"\"\"Run filter method.\"\"\"\n        CP.info(\"Selecting relevant features among: \" + str(self.filter_data.features))\n\n        self.sel_method.initialise(self.obs_data, self.f_alpha, self.min_lag, self.max_lag, self.CM)\n        self.CM = self.sel_method.compute_dependencies()\n\n\n    def run_validator(self, link_assumptions = None) -&gt; DAG:\n\"\"\"\n        Run Validator (LPCMCI).\n\n        Args:\n            link_assumptions (dict, optional): link assumption with context. Defaults to None.\n\n        Returns:\n            DAG: causal model with context.\n        \"\"\"\n        self.validator = myLPCMCI(self.validator_data,\n                                self.min_lag, self.max_lag,\n                                self.sys_context,\n                                self.val_condtest,\n                                CP.verbosity,\n                                self.alpha)\n        causal_model = self.validator.run(link_assumptions)\n        causal_model.sys_context = self.CM.sys_context      \n\n        return causal_model\n\n\n    def run(self, remove_unneeded = True, nofilter = True) -&gt; DAG:\n\"\"\"\n        Run CAnDOIT.\n\n        Returns:\n            DAG: causal model.\n        \"\"\"\n        link_assumptions = None\n\n        if not nofilter:\n            #FIXME: to include also the filter. for now this is wrong\n            ## 1. FILTER\n            self.run_filter()\n\n            # list of selected features based on filter dependencies\n            self.CM.remove_unneeded_features()\n            if not self.CM.features: return None, None\n\n            self.obs_data.shrink(self.CM.features)\n            f_dag = copy.deepcopy(self.CM)\n\n            ## 2. VALIDATOR\n            # Add dependencies corresponding to the context variables \n            # ONLY if the the related system variable is still present\n            self.CM.add_context() \n\n            # shrink dataframe d by using the filter result\n            self.validator_data.shrink(self.CM.features)\n\n            # selected links to check by the validator\n            link_assumptions = self.CM.get_link_assumptions()\n\n        else:\n            # fullg = DAG(self.validator_data.features, self.min_lag, self.max_lag, False)\n            # fullg.sys_context = self.CM.sys_context\n            link_assumptions = self.JCI_assumptions()\n\n        # calculate dependencies on selected links\n        self.CM = self.run_validator(link_assumptions)\n\n        # list of selected features based on validator dependencies\n        if remove_unneeded: self.CM.remove_unneeded_features()\n        if self.exclude_context: self.CM.remove_context()\n\n        self.save()\n\n        return self.CM\n\n\n    def load(self, res_path):\n\"\"\"\n        Load previously estimated result.\n\n        Args:\n            res_path (str): pickle file path.\n        \"\"\"\n        with open(res_path, 'rb') as f:\n            r = pickle.load(f)\n            self.CM = r['causal_model']\n            self.f_alpha = r['filter_alpha']\n            self.alpha = r['alpha']\n            self.dag_path = r['dag_path']\n            self.ts_dag_path = r['ts_dag_path']\n\n\n    def save(self):\n\"\"\"Save causal discovery result as pickle file if resfolder is set.\"\"\"\n        if self.respath is not None:\n            if self.CM:\n                res = dict()\n                res['causal_model'] = copy.deepcopy(self.CM)\n                res['features'] = copy.deepcopy(self.CM.features)\n                res['filter_alpha'] = self.f_alpha\n                res['alpha'] = self.alpha\n                res['dag_path'] = self.dag_path\n                res['ts_dag_path'] = self.ts_dag_path\n                with open(self.respath, 'wb') as resfile:\n                    pickle.dump(res, resfile)\n            else:\n                CP.warning(\"Causal model impossible to save\")\n\n\n    def _prepare_data(self, obser_data, inter_data, plot_data):\n\"\"\"\n        Prepare data for filter and validator phases.\n\n        Args:\n            obser_data (Data): observational data.\n            inter_data (Data): interventional data.\n            plot_data (bool): boolean bit to plot the generated data.\n\n        Returns:\n            Data, Data: filter data obj and validator data obj.\n        \"\"\"\n        # Filter phase data preparation\n        filter_data = copy.deepcopy(obser_data.d)\n        for int_data in inter_data.values(): filter_data = pd.concat([filter_data, int_data.d], axis = 0, ignore_index = True)\n        filter_data = Data(filter_data, vars = obser_data.features)\n\n        # Validator phase data preparation\n        validator_data = copy.deepcopy(obser_data.d)\n        context_vars = dict()\n        for int_var, int_data in inter_data.items():\n\n            # Create context variable name\n            context_varname = 'C' + int_var\n\n            # Store a dict of context variable and system variable corresponding to an intervention\n            self.CM.sys_context[int_var] = context_varname\n\n            # Create context variable data\n            # context_data = np.ones(shape=int_data.d[int_var].shape)\n            context_data = int_data.d[int_var]\n            context_start = len(validator_data)\n            context_end = context_start + len(context_data)\n            context_vars[context_varname] = {'data': context_data, 'start': context_start, 'end': context_end}\n\n            validator_data = pd.concat([validator_data, int_data.d], axis = 0, ignore_index = True)\n\n        for var in context_vars:\n            new_column = np.zeros(shape = (len(validator_data),))\n            new_column[context_vars[var]['start']: context_vars[var]['end']] = context_vars[var]['data']\n            validator_data[var] = new_column\n\n        validator_data = Data(validator_data, vars = list(validator_data.columns))\n\n        if plot_data: validator_data.plot_timeseries()\n        return filter_data, validator_data\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.CAnDOIT.CAnDOIT.isThereInterv","title":"<code>isThereInterv: bool</code>  <code>property</code>","text":"<p>Check whether an intervention is present or not.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>flag to identify if an intervention is present or not.</p>"},{"location":"CAnDOIT/#causalflow.causal_discovery.CAnDOIT.CAnDOIT.JCI_assumptions","title":"<code>JCI_assumptions()</code>","text":"<p>Initialise the algorithm initial causal structure with the JCI assumptions.</p> Source code in <code>causalflow/causal_discovery/CAnDOIT.py</code> <pre><code>def JCI_assumptions(self):\n\"\"\"Initialise the algorithm initial causal structure with the JCI assumptions.\"\"\"\n    # ! JCI Assmpution 1: No system variable causes any context variable\n    # ! JCI Assmpution 2: No context variable is confounded with a system variable\n    # ! JCI Assmpution 3: The context distribution contains no (conditional) independences\n\n    knowledge = {self.vars.index(f): dict() for f in self.vars}\n\n    # ! JCI Assmpution 1\n    for k in self.contexts:\n        for x in self.systems:\n            for tau_i in range(0, self.max_lag + 1):\n                knowledge[self.vars.index(k)][(self.vars.index(x), -tau_i)] = ''\n\n    # ! JCI Assmpution 2\n    for k in self.contexts:\n        for x in self.systems:\n            if x not in self.sys_context or (x in self.sys_context and k != self.sys_context[x]):\n                for tau_i in range(0, self.max_lag + 1): knowledge[self.vars.index(x)][(self.vars.index(k), -tau_i)] = ''\n            elif x in self.sys_context and k == self.sys_context[x]:\n                knowledge[self.vars.index(x)][(self.vars.index(k), 0)] = '--&gt;'\n                knowledge[self.vars.index(k)][(self.vars.index(x), 0)] = '&lt;--'\n                for tau_i in range(1, self.max_lag + 1): knowledge[self.vars.index(x)][(self.vars.index(k), -tau_i)] = ''\n                for tau_i in range(1, self.max_lag + 1): knowledge[self.vars.index(k)][(self.vars.index(x), -tau_i)] = ''\n\n    # ! JCI Assmpution 3\n    for k1 in self.contexts:\n        for k2 in remove_from_list(self.contexts, k1):\n            knowledge[self.vars.index(k1)][(self.vars.index(k2), 0)] = '&lt;-&gt;'\n            # for tau_i in range(0, self.max_lag + 1): knowledge[self.vars.index(k1)][(self.vars.index(k2), -tau_i)] = '&lt;-&gt;'\n\n    # ! This models the context variables as chain across different time steps\n    for k in self.contexts:\n        for tau_i in range(1, self.max_lag + 1):\n            knowledge[self.vars.index(k)][(self.vars.index(k), -tau_i)] = '--&gt;' if tau_i == 1 else ''\n\n\n\n\n    out = {}\n    for j in range(len(self.vars)):\n        inner_dict = {} \n\n        for i in range(len(self.vars)):\n            for tau_i in range(0, self.max_lag + 1):\n                if tau_i &gt; 0 or i != j:\n                    value = \"o?&gt;\" if tau_i &gt; 0 else \"o?o\"\n                    inner_dict[(i, -tau_i)] = value\n\n        out[j] = inner_dict\n\n    for j, links_j in knowledge.items():\n        for (i, lag_i), link_ij in links_j.items():\n            if link_ij == \"\":\n                del out[j][(i, lag_i)]\n            else: \n                out[j][(i, lag_i)] = link_ij\n    return out\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.CAnDOIT.CAnDOIT.__init__","title":"<code>__init__(observation_data, intervention_data, min_lag, max_lag, sel_method, val_condtest, verbosity, f_alpha=0.05, alpha=0.05, resfolder=None, neglect_only_autodep=False, exclude_context=True, plot_data=False, clean_cls=True)</code>","text":"<p>Class contructor.</p> <p>Parameters:</p> Name Type Description Default <code>observation_data</code> <code>Data</code> <p>observational data to analyse.</p> required <code>intervention_data</code> <code>dict</code> <p>interventional data to analyse in the form {INTERVENTION_VARIABLE : Data (same variables of observation_data)}.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>sel_method</code> <code>SelectionMethod</code> <p>selection method.</p> required <code>val_condtest</code> <code>CondIndTest</code> <p>validation method.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>f_alpha</code> <code>float</code> <p>filter significance level. Defaults to 0.05.</p> <code>0.05</code> <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>exclude_context</code> <code>bool</code> <p>Bit for neglecting context variables. Defaults to False.</p> <code>True</code> <code>plot_data</code> <code>bool</code> <p>Bit for plotting your data. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/CAnDOIT.py</code> <pre><code>def __init__(self, \n             observation_data: Data, \n             intervention_data: dict, \n             min_lag, max_lag,\n             sel_method: SelectionMethod, val_condtest: CondIndTest, \n             verbosity: CPLevel, \n             f_alpha = 0.05, \n             alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False,\n             exclude_context = True,\n             plot_data = False,\n             clean_cls = True):\n\"\"\"\n    Class contructor.\n\n    Args:\n        observation_data (Data): observational data to analyse.\n        intervention_data (dict): interventional data to analyse in the form {INTERVENTION_VARIABLE : Data (same variables of observation_data)}.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        sel_method (SelectionMethod): selection method.\n        val_condtest (CondIndTest): validation method.\n        verbosity (CPLevel): verbosity level.\n        f_alpha (float, optional): filter significance level. Defaults to 0.05.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        exclude_context (bool, optional): Bit for neglecting context variables. Defaults to False.\n        plot_data (bool, optional): Bit for plotting your data. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    self.obs_data = observation_data\n    self.systems = observation_data.features\n    self.contexts = []\n    self.sys_context = {}\n    for k in intervention_data.keys():\n        self.contexts.append(\"C\" + k)\n        self.sys_context[k] = \"C\" + k\n    self.vars = self.systems + self.contexts\n\n    self.f_alpha = f_alpha\n    self.sel_method = sel_method\n    self.val_condtest = val_condtest\n    self.exclude_context = exclude_context\n    super().__init__(self.obs_data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n    # Create filter and validator data\n    self.filter_data, self.validator_data = self._prepare_data(self.obs_data, intervention_data, plot_data)\n\n\n    CP.info(\"\\n\")\n    CP.info(DASH)\n    CP.info(\"Observational data length: \" + str(observation_data.T))\n    CP.info(\"Interventional data length: \" + str(sum([d.T for d in intervention_data.values()])))\n    CP.info(\"Min lag time: \" + str(min_lag))\n    CP.info(\"Max lag time: \" + str(max_lag))\n    CP.info(\"Filter significance level: \" + str(f_alpha))\n    CP.info(\"PCMCI significance level: \" + str(alpha))\n    CP.info(\"Selection method: \" + sel_method.name)\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.CAnDOIT.CAnDOIT.load","title":"<code>load(res_path)</code>","text":"<p>Load previously estimated result.</p> <p>Parameters:</p> Name Type Description Default <code>res_path</code> <code>str</code> <p>pickle file path.</p> required Source code in <code>causalflow/causal_discovery/CAnDOIT.py</code> <pre><code>def load(self, res_path):\n\"\"\"\n    Load previously estimated result.\n\n    Args:\n        res_path (str): pickle file path.\n    \"\"\"\n    with open(res_path, 'rb') as f:\n        r = pickle.load(f)\n        self.CM = r['causal_model']\n        self.f_alpha = r['filter_alpha']\n        self.alpha = r['alpha']\n        self.dag_path = r['dag_path']\n        self.ts_dag_path = r['ts_dag_path']\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.CAnDOIT.CAnDOIT.run","title":"<code>run(remove_unneeded=True, nofilter=True)</code>","text":"<p>Run CAnDOIT.</p> <p>Returns:</p> Name Type Description <code>DAG</code> <code>DAG</code> <p>causal model.</p> Source code in <code>causalflow/causal_discovery/CAnDOIT.py</code> <pre><code>def run(self, remove_unneeded = True, nofilter = True) -&gt; DAG:\n\"\"\"\n    Run CAnDOIT.\n\n    Returns:\n        DAG: causal model.\n    \"\"\"\n    link_assumptions = None\n\n    if not nofilter:\n        #FIXME: to include also the filter. for now this is wrong\n        ## 1. FILTER\n        self.run_filter()\n\n        # list of selected features based on filter dependencies\n        self.CM.remove_unneeded_features()\n        if not self.CM.features: return None, None\n\n        self.obs_data.shrink(self.CM.features)\n        f_dag = copy.deepcopy(self.CM)\n\n        ## 2. VALIDATOR\n        # Add dependencies corresponding to the context variables \n        # ONLY if the the related system variable is still present\n        self.CM.add_context() \n\n        # shrink dataframe d by using the filter result\n        self.validator_data.shrink(self.CM.features)\n\n        # selected links to check by the validator\n        link_assumptions = self.CM.get_link_assumptions()\n\n    else:\n        # fullg = DAG(self.validator_data.features, self.min_lag, self.max_lag, False)\n        # fullg.sys_context = self.CM.sys_context\n        link_assumptions = self.JCI_assumptions()\n\n    # calculate dependencies on selected links\n    self.CM = self.run_validator(link_assumptions)\n\n    # list of selected features based on validator dependencies\n    if remove_unneeded: self.CM.remove_unneeded_features()\n    if self.exclude_context: self.CM.remove_context()\n\n    self.save()\n\n    return self.CM\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.CAnDOIT.CAnDOIT.run_filter","title":"<code>run_filter()</code>","text":"<p>Run filter method.</p> Source code in <code>causalflow/causal_discovery/CAnDOIT.py</code> <pre><code>def run_filter(self):\n\"\"\"Run filter method.\"\"\"\n    CP.info(\"Selecting relevant features among: \" + str(self.filter_data.features))\n\n    self.sel_method.initialise(self.obs_data, self.f_alpha, self.min_lag, self.max_lag, self.CM)\n    self.CM = self.sel_method.compute_dependencies()\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.CAnDOIT.CAnDOIT.run_validator","title":"<code>run_validator(link_assumptions=None)</code>","text":"<p>Run Validator (LPCMCI).</p> <p>Parameters:</p> Name Type Description Default <code>link_assumptions</code> <code>dict</code> <p>link assumption with context. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DAG</code> <code>DAG</code> <p>causal model with context.</p> Source code in <code>causalflow/causal_discovery/CAnDOIT.py</code> <pre><code>def run_validator(self, link_assumptions = None) -&gt; DAG:\n\"\"\"\n    Run Validator (LPCMCI).\n\n    Args:\n        link_assumptions (dict, optional): link assumption with context. Defaults to None.\n\n    Returns:\n        DAG: causal model with context.\n    \"\"\"\n    self.validator = myLPCMCI(self.validator_data,\n                            self.min_lag, self.max_lag,\n                            self.sys_context,\n                            self.val_condtest,\n                            CP.verbosity,\n                            self.alpha)\n    causal_model = self.validator.run(link_assumptions)\n    causal_model.sys_context = self.CM.sys_context      \n\n    return causal_model\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.CAnDOIT.CAnDOIT.save","title":"<code>save()</code>","text":"<p>Save causal discovery result as pickle file if resfolder is set.</p> Source code in <code>causalflow/causal_discovery/CAnDOIT.py</code> <pre><code>def save(self):\n\"\"\"Save causal discovery result as pickle file if resfolder is set.\"\"\"\n    if self.respath is not None:\n        if self.CM:\n            res = dict()\n            res['causal_model'] = copy.deepcopy(self.CM)\n            res['features'] = copy.deepcopy(self.CM.features)\n            res['filter_alpha'] = self.f_alpha\n            res['alpha'] = self.alpha\n            res['dag_path'] = self.dag_path\n            res['ts_dag_path'] = self.ts_dag_path\n            with open(self.respath, 'wb') as resfile:\n                pickle.dump(res, resfile)\n        else:\n            CP.warning(\"Causal model impossible to save\")\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.DYNOTEARS.DYNOTEARS","title":"<code>DYNOTEARS</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>DYNOTEARS causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/DYNOTEARS.py</code> <pre><code>class DYNOTEARS(CausalDiscoveryMethod):\n\"\"\"DYNOTEARS causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data, \n                 min_lag,\n                 max_lag, \n                 verbosity, \n                 alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n    def run(self) -&gt; DAG:\n\"\"\"\n        Run DYNOTEARS algorithm.\n\n        Returns:\n            DAG: causal discovery result.\n        \"\"\"\n        graph_dict = dict()\n        for name in self.data.features:\n            graph_dict[name] = []\n        sm = from_pandas_dynamic(self.data.d, p=self.max_lag)\n\n        tname_to_name_dict = dict()\n        count_lag = 0\n        idx_name = 0\n        for tname in sm.nodes:\n            tname_to_name_dict[tname] = self.data.features[idx_name]\n            if count_lag == self.max_lag:\n                idx_name = idx_name +1\n                count_lag = -1\n            count_lag = count_lag +1\n\n        for ce in sm.edges:\n            c = ce[0]\n            e = ce[1]\n            w = sm.adj[c][e][\"weight\"]\n            tc = int(c.partition(\"lag\")[2])\n            te = int(e.partition(\"lag\")[2])\n            t = tc - te\n            if (tname_to_name_dict[c], -t) not in graph_dict[tname_to_name_dict[e]]:\n                graph_dict[tname_to_name_dict[e]].append((tname_to_name_dict[c], w, -t))\n\n        self.CM = self._to_DAG(graph_dict)\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n\n\n    def _to_DAG(self, graph):\n\"\"\"\n        Re-elaborate the result in a DAG.\n\n        Returns:\n            (DAG): result re-elaborated.\n        \"\"\"\n        tmp_dag = DAG(self.data.features, self.min_lag, self.max_lag, self.neglect_only_autodep)\n        tmp_dag.sys_context = dict()\n        for t in graph.keys():\n            for s in graph[t]:\n                lag = abs(s[2])\n                if lag &gt;= self.min_lag and lag &lt;= self.max_lag:\n                    tmp_dag.add_source(t, s[0], abs(s[1]), 0, s[2])\n        return tmp_dag\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.DYNOTEARS.DYNOTEARS.__init__","title":"<code>__init__(data, min_lag, max_lag, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/DYNOTEARS.py</code> <pre><code>def __init__(self, \n             data, \n             min_lag,\n             max_lag, \n             verbosity, \n             alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.DYNOTEARS.DYNOTEARS.run","title":"<code>run()</code>","text":"<p>Run DYNOTEARS algorithm.</p> <p>Returns:</p> Name Type Description <code>DAG</code> <code>DAG</code> <p>causal discovery result.</p> Source code in <code>causalflow/causal_discovery/baseline/DYNOTEARS.py</code> <pre><code>def run(self) -&gt; DAG:\n\"\"\"\n    Run DYNOTEARS algorithm.\n\n    Returns:\n        DAG: causal discovery result.\n    \"\"\"\n    graph_dict = dict()\n    for name in self.data.features:\n        graph_dict[name] = []\n    sm = from_pandas_dynamic(self.data.d, p=self.max_lag)\n\n    tname_to_name_dict = dict()\n    count_lag = 0\n    idx_name = 0\n    for tname in sm.nodes:\n        tname_to_name_dict[tname] = self.data.features[idx_name]\n        if count_lag == self.max_lag:\n            idx_name = idx_name +1\n            count_lag = -1\n        count_lag = count_lag +1\n\n    for ce in sm.edges:\n        c = ce[0]\n        e = ce[1]\n        w = sm.adj[c][e][\"weight\"]\n        tc = int(c.partition(\"lag\")[2])\n        te = int(e.partition(\"lag\")[2])\n        t = tc - te\n        if (tname_to_name_dict[c], -t) not in graph_dict[tname_to_name_dict[e]]:\n            graph_dict[tname_to_name_dict[e]].append((tname_to_name_dict[c], w, -t))\n\n    self.CM = self._to_DAG(graph_dict)\n\n    if self.resfolder is not None: self.logger.close()\n    return self.CM\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.LPCMCI.LPCMCI","title":"<code>LPCMCI</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>LPCMCI causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/LPCMCI.py</code> <pre><code>class LPCMCI(CausalDiscoveryMethod):\n\"\"\"LPCMCI causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data: Data,\n                 min_lag, max_lag, \n                 val_condtest: CondIndTest, \n                 verbosity: CPLevel,\n                 alpha = 0.05, \n                 resfolder = None, \n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            val_condtest (CondIndTest): validation method.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n        # build tigramite dataset\n        vector = np.vectorize(float)\n        d = vector(data.d)\n\n        # init pcmci\n        self.lpcmci = lpcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                             cond_ind_test = val_condtest,\n                             verbosity = verbosity.value)\n\n\n    def run(self, link_assumptions = None) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        CP.info('\\n')\n        CP.info(DASH)\n        CP.info(\"Running Causal Discovery Algorithm\")\n        self.result = self.lpcmci.run_lpcmci(link_assumptions = link_assumptions,\n                                             tau_max = self.max_lag,\n                                             tau_min = self.min_lag,\n                                             pc_alpha = self.alpha)\n\n        self.CM = self._to_DAG()\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n\n\n    def _to_DAG(self):\n\"\"\"\n        Re-elaborate the PCMCI result in a new dictionary.\n\n        Returns:\n            (DAG): lpcmci result re-elaborated.\n        \"\"\"\n        vars = self.data.features\n        tmp_dag = DAG(vars, self.min_lag, self.max_lag)\n        tmp_dag.sys_context = dict()\n        N, lags = self.result['graph'][0].shape\n        for s in range(len(self.result['graph'])):\n            for t in range(N):\n                for lag in range(lags):\n                    if self.result['graph'][s][t,lag] != '':\n                        arrowtype = self.result['graph'][s][t,lag]\n\n                        if arrowtype == LinkType.Bidirected.value:\n                            if ((vars[s], abs(lag)) in tmp_dag.g[vars[t]].sources and \n                                tmp_dag.g[t].sources[(vars[s], abs(lag))][TYPE] == LinkType.Bidirected.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif arrowtype == LinkType.Uncertain.value:\n                            if ((vars[t], abs(lag)) in tmp_dag.g[vars[s]].sources and \n                                tmp_dag.g[vars[s]].sources[(vars[t], abs(lag))][TYPE] == LinkType.Uncertain.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif (arrowtype == LinkType.Directed.value or\n                              arrowtype == LinkType.HalfUncertain.value):\n                            tmp_dag.add_source(vars[t], \n                                            vars[s],\n                                            self.result['val_matrix'][s][t,lag],\n                                            self.result['p_matrix'][s][t,lag],\n                                            lag,\n                                            arrowtype)\n        return tmp_dag\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.LPCMCI.LPCMCI.__init__","title":"<code>__init__(data, min_lag, max_lag, val_condtest, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>val_condtest</code> <code>CondIndTest</code> <p>validation method.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/LPCMCI.py</code> <pre><code>def __init__(self, \n             data: Data,\n             min_lag, max_lag, \n             val_condtest: CondIndTest, \n             verbosity: CPLevel,\n             alpha = 0.05, \n             resfolder = None, \n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        val_condtest (CondIndTest): validation method.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n    # build tigramite dataset\n    vector = np.vectorize(float)\n    d = vector(data.d)\n\n    # init pcmci\n    self.lpcmci = lpcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                         cond_ind_test = val_condtest,\n                         verbosity = verbosity.value)\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.LPCMCI.LPCMCI.run","title":"<code>run(link_assumptions=None)</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/LPCMCI.py</code> <pre><code>def run(self, link_assumptions = None) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    CP.info('\\n')\n    CP.info(DASH)\n    CP.info(\"Running Causal Discovery Algorithm\")\n    self.result = self.lpcmci.run_lpcmci(link_assumptions = link_assumptions,\n                                         tau_max = self.max_lag,\n                                         tau_min = self.min_lag,\n                                         pc_alpha = self.alpha)\n\n    self.CM = self._to_DAG()\n\n    if self.resfolder is not None: self.logger.close()\n    return self.CM\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.PCMCI.PCMCI","title":"<code>PCMCI</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>PCMCI causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/PCMCI.py</code> <pre><code>class PCMCI(CausalDiscoveryMethod):\n\"\"\"PCMCI causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data: Data, \n                 min_lag, max_lag, \n                 val_condtest: CondIndTest, \n                 verbosity: CPLevel,\n                 pc_alpha = 0.05, \n                 alpha = 0.05, \n                 resfolder = None, \n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            val_condtest (CondIndTest): validation method.\n            verbosity (CPLevel): verbosity level.\n            pc_alpha (float, optional): PC significance level. Defaults to 0.05.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n        self.pc_alpha = pc_alpha\n\n        # build tigramite dataset\n        vector = np.vectorize(float)\n        d = vector(data.d)\n\n        # init pcmci\n        self.pcmci = pcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                           cond_ind_test = val_condtest,\n                           verbosity = verbosity.value)\n\n\n    def run(self) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        CP.info('\\n')\n        CP.info(DASH)\n        CP.info(\"Running Causal Discovery Algorithm\")\n\n        self.result = self.pcmci.run_pcmci(tau_max = self.max_lag,\n                                           tau_min = self.min_lag,\n                                           alpha_level = self.alpha,\n                                           pc_alpha = self.pc_alpha)\n\n        self.CM = self._to_DAG()\n\n        if self.resfolder is not None: self.logger.close()        \n        return self.CM\n\n\n    def _to_DAG(self):\n\"\"\"\n        Re-elaborates the PCMCI result in a new dictionary.\n\n        Returns:\n            (DAG): pcmci result re-elaborated.\n        \"\"\"\n        vars = self.data.features\n        tmp_dag = DAG(vars, self.min_lag, self.max_lag)\n        tmp_dag.sys_context = dict()\n        N, lags = self.result['graph'][0].shape\n        for s in range(len(self.result['graph'])):\n            for t in range(N):\n                for lag in range(lags):\n                    if self.result['graph'][s][t,lag] != '':\n                        arrowtype = self.result['graph'][s][t,lag]\n\n                        if arrowtype == LinkType.Bidirected.value:\n                            if ((vars[s], abs(lag)) in tmp_dag.g[vars[t]].sources and \n                                tmp_dag.g[t].sources[(vars[s], abs(lag))][TYPE] == LinkType.Bidirected.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif arrowtype == LinkType.Uncertain.value:\n                            if ((vars[t], abs(lag)) in tmp_dag.g[vars[s]].sources and \n                                tmp_dag.g[vars[s]].sources[(vars[t], abs(lag))][TYPE] == LinkType.Uncertain.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif (arrowtype == LinkType.Directed.value or\n                              arrowtype == LinkType.HalfUncertain.value):\n                            tmp_dag.add_source(vars[t], \n                                            vars[s],\n                                            self.result['val_matrix'][s][t,lag],\n                                            self.result['p_matrix'][s][t,lag],\n                                            lag,\n                                            arrowtype)\n        return tmp_dag\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.PCMCI.PCMCI.__init__","title":"<code>__init__(data, min_lag, max_lag, val_condtest, verbosity, pc_alpha=0.05, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>val_condtest</code> <code>CondIndTest</code> <p>validation method.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>pc_alpha</code> <code>float</code> <p>PC significance level. Defaults to 0.05.</p> <code>0.05</code> <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/PCMCI.py</code> <pre><code>def __init__(self, \n             data: Data, \n             min_lag, max_lag, \n             val_condtest: CondIndTest, \n             verbosity: CPLevel,\n             pc_alpha = 0.05, \n             alpha = 0.05, \n             resfolder = None, \n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        val_condtest (CondIndTest): validation method.\n        verbosity (CPLevel): verbosity level.\n        pc_alpha (float, optional): PC significance level. Defaults to 0.05.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n    self.pc_alpha = pc_alpha\n\n    # build tigramite dataset\n    vector = np.vectorize(float)\n    d = vector(data.d)\n\n    # init pcmci\n    self.pcmci = pcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                       cond_ind_test = val_condtest,\n                       verbosity = verbosity.value)\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.PCMCI.PCMCI.run","title":"<code>run()</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/PCMCI.py</code> <pre><code>def run(self) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    CP.info('\\n')\n    CP.info(DASH)\n    CP.info(\"Running Causal Discovery Algorithm\")\n\n    self.result = self.pcmci.run_pcmci(tau_max = self.max_lag,\n                                       tau_min = self.min_lag,\n                                       alpha_level = self.alpha,\n                                       pc_alpha = self.pc_alpha)\n\n    self.CM = self._to_DAG()\n\n    if self.resfolder is not None: self.logger.close()        \n    return self.CM\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.PCMCIplus.PCMCIplus","title":"<code>PCMCIplus</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>PCMCI+ causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/PCMCIplus.py</code> <pre><code>class PCMCIplus(CausalDiscoveryMethod):\n\"\"\"PCMCI+ causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data: Data, \n                 min_lag, max_lag, \n                 val_condtest: CondIndTest, \n                 verbosity: CPLevel,\n                 alpha = 0.05, \n                 resfolder = None, \n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            val_condtest (CondIndTest): validation method.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n        # build tigramite dataset\n        vector = np.vectorize(float)\n        d = vector(data.d)\n\n        # init pcmci\n        self.pcmci = pcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                           cond_ind_test = val_condtest,\n                           verbosity = verbosity.value)\n\n\n    def run(self, link_assumptions=None) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        CP.info('\\n')\n        CP.info(DASH)\n        CP.info(\"Running Causal Discovery Algorithm\")\n\n        self.result = self.pcmci.run_pcmciplus(link_assumptions=link_assumptions,\n                                               tau_max = self.max_lag,\n                                               tau_min = 0,\n                                               pc_alpha = self.alpha)\n\n        self.CM = self._to_DAG()\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n\n\n    def _to_DAG(self):\n\"\"\"\n        Re-elaborates the PCMCI result in a new dictionary.\n\n        Returns:\n            (DAG): pcmci result re-elaborated.\n        \"\"\"\n        vars = self.data.features\n        tmp_dag = DAG(vars, self.min_lag, self.max_lag)\n        tmp_dag.sys_context = dict()\n        N, lags = self.result['graph'][0].shape\n        for s in range(len(self.result['graph'])):\n            for t in range(N):\n                for lag in range(lags):\n                    if self.result['graph'][s][t,lag] != '':\n                        arrowtype = self.result['graph'][s][t,lag]\n\n                        if arrowtype == LinkType.Bidirected.value:\n                            if ((vars[s], abs(lag)) in tmp_dag.g[vars[t]].sources and \n                                tmp_dag.g[t].sources[(vars[s], abs(lag))][TYPE] == LinkType.Bidirected.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif arrowtype == LinkType.Uncertain.value:\n                            if ((vars[t], abs(lag)) in tmp_dag.g[vars[s]].sources and \n                                tmp_dag.g[vars[s]].sources[(vars[t], abs(lag))][TYPE] == LinkType.Uncertain.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif (arrowtype == LinkType.Directed.value or\n                              arrowtype == LinkType.HalfUncertain.value):\n                            tmp_dag.add_source(vars[t], \n                                            vars[s],\n                                            self.result['val_matrix'][s][t,lag],\n                                            self.result['p_matrix'][s][t,lag],\n                                            lag,\n                                            arrowtype)\n        return tmp_dag\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.PCMCIplus.PCMCIplus.__init__","title":"<code>__init__(data, min_lag, max_lag, val_condtest, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>val_condtest</code> <code>CondIndTest</code> <p>validation method.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/PCMCIplus.py</code> <pre><code>def __init__(self, \n             data: Data, \n             min_lag, max_lag, \n             val_condtest: CondIndTest, \n             verbosity: CPLevel,\n             alpha = 0.05, \n             resfolder = None, \n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        val_condtest (CondIndTest): validation method.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n    # build tigramite dataset\n    vector = np.vectorize(float)\n    d = vector(data.d)\n\n    # init pcmci\n    self.pcmci = pcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                       cond_ind_test = val_condtest,\n                       verbosity = verbosity.value)\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.PCMCIplus.PCMCIplus.run","title":"<code>run(link_assumptions=None)</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/PCMCIplus.py</code> <pre><code>def run(self, link_assumptions=None) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    CP.info('\\n')\n    CP.info(DASH)\n    CP.info(\"Running Causal Discovery Algorithm\")\n\n    self.result = self.pcmci.run_pcmciplus(link_assumptions=link_assumptions,\n                                           tau_max = self.max_lag,\n                                           tau_min = 0,\n                                           pc_alpha = self.alpha)\n\n    self.CM = self._to_DAG()\n\n    if self.resfolder is not None: self.logger.close()\n    return self.CM\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.TCDF.TCDF","title":"<code>TCDF</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>TCDF causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/TCDF.py</code> <pre><code>class TCDF(CausalDiscoveryMethod):\n\"\"\"TCDF causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data, \n                 min_lag,\n                 max_lag, \n                 verbosity, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            verbosity (CPLevel): verbosity level.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, resfolder=resfolder, neglect_only_autodep=neglect_only_autodep, clean_cls=clean_cls)\n\n\n    def run(self, \n            epochs=1000,  \n            kernel_size=4, \n            dilation_coefficient=4, \n            hidden_layers=0, \n            learning_rate=0.01,\n            cuda=False) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        # Remove all arguments from directory\n        dir_path = os.path.dirname(os.path.realpath(__file__))\n        Path(dir_path+\"/args\").mkdir(exist_ok=True)\n        Path(dir_path+\"/results\").mkdir(exist_ok=True)\n        script = dir_path + \"/pkgs/TCDF-master/runTCDF\" + \".py\"\n        r_arg_list = []\n        r_arg_list.append(\"--epochs\")\n        r_arg_list.append(str(epochs))\n        r_arg_list.append(\"--kernel_size\")\n        r_arg_list.append(str(kernel_size))\n        r_arg_list.append(\"--dilation_coefficient\")\n        r_arg_list.append(str(dilation_coefficient))\n        r_arg_list.append(\"--hidden_layers\")\n        r_arg_list.append(str(hidden_layers))\n        r_arg_list.append(\"--learning_rate\")\n        r_arg_list.append(str(learning_rate))\n        r_arg_list.append(\"--significance\")\n        r_arg_list.append(str(0.8))\n        self.data.d.to_csv(dir_path + \"/args/data.csv\", index=False)\n        r_arg_list.append(\"--data\")\n        r_arg_list.append(dir_path + \"/args/data.csv\")            \n        if cuda: r_arg_list.append(\"--cuda\")\n        r_arg_list.append(\"--path\")\n        r_arg_list.append(dir_path)\n\n        cmd = [\"python\", script] + r_arg_list\n        p = Popen(cmd, cwd=\"./\", stdin=PIPE, stdout=PIPE, stderr=PIPE)\n\n        # Return R output or error\n        output, error = p.communicate()\n        CP.info(output.decode('utf-8'))\n        if p.returncode == 0:\n            g_dict = json.load(open(dir_path + \"/results/tcdf_result.txt\"))\n            for key in g_dict.keys():\n                key_list = []\n                for elem in g_dict[key]:\n                    key_list.append(tuple(elem))\n                g_dict[key] = key_list\n            utils.clean(dir_path)\n            self.CM = self._to_DAG(g_dict)\n\n            if self.resfolder is not None: self.logger.close()\n            return self.CM\n        else:\n            utils.clean(dir_path)\n            CP.warning('Python Error:\\n {0}'.format(error))\n            exit(0)\n\n\n    def _to_DAG(self, graph):\n\"\"\"\n        Re-elaborate the result in a DAG.\n\n        Returns:\n            (DAG): result re-elaborated.\n        \"\"\"\n        tmp_dag = DAG(self.data.features, self.min_lag, self.max_lag, self.neglect_only_autodep)\n        tmp_dag.sys_context = dict()\n        for t in graph.keys():\n            for s in graph[t]:\n                lag = abs(s[1])\n                if lag &gt;= self.min_lag and lag &lt;= self.max_lag:\n                    tmp_dag.add_source(t, s[0], utils.DSCORE, 0, s[1])\n        return tmp_dag\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.TCDF.TCDF.__init__","title":"<code>__init__(data, min_lag, max_lag, verbosity, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/TCDF.py</code> <pre><code>def __init__(self, \n             data, \n             min_lag,\n             max_lag, \n             verbosity, \n             resfolder = None,\n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        verbosity (CPLevel): verbosity level.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, resfolder=resfolder, neglect_only_autodep=neglect_only_autodep, clean_cls=clean_cls)\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.TCDF.TCDF.run","title":"<code>run(epochs=1000, kernel_size=4, dilation_coefficient=4, hidden_layers=0, learning_rate=0.01, cuda=False)</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/TCDF.py</code> <pre><code>def run(self, \n        epochs=1000,  \n        kernel_size=4, \n        dilation_coefficient=4, \n        hidden_layers=0, \n        learning_rate=0.01,\n        cuda=False) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    # Remove all arguments from directory\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    Path(dir_path+\"/args\").mkdir(exist_ok=True)\n    Path(dir_path+\"/results\").mkdir(exist_ok=True)\n    script = dir_path + \"/pkgs/TCDF-master/runTCDF\" + \".py\"\n    r_arg_list = []\n    r_arg_list.append(\"--epochs\")\n    r_arg_list.append(str(epochs))\n    r_arg_list.append(\"--kernel_size\")\n    r_arg_list.append(str(kernel_size))\n    r_arg_list.append(\"--dilation_coefficient\")\n    r_arg_list.append(str(dilation_coefficient))\n    r_arg_list.append(\"--hidden_layers\")\n    r_arg_list.append(str(hidden_layers))\n    r_arg_list.append(\"--learning_rate\")\n    r_arg_list.append(str(learning_rate))\n    r_arg_list.append(\"--significance\")\n    r_arg_list.append(str(0.8))\n    self.data.d.to_csv(dir_path + \"/args/data.csv\", index=False)\n    r_arg_list.append(\"--data\")\n    r_arg_list.append(dir_path + \"/args/data.csv\")            \n    if cuda: r_arg_list.append(\"--cuda\")\n    r_arg_list.append(\"--path\")\n    r_arg_list.append(dir_path)\n\n    cmd = [\"python\", script] + r_arg_list\n    p = Popen(cmd, cwd=\"./\", stdin=PIPE, stdout=PIPE, stderr=PIPE)\n\n    # Return R output or error\n    output, error = p.communicate()\n    CP.info(output.decode('utf-8'))\n    if p.returncode == 0:\n        g_dict = json.load(open(dir_path + \"/results/tcdf_result.txt\"))\n        for key in g_dict.keys():\n            key_list = []\n            for elem in g_dict[key]:\n                key_list.append(tuple(elem))\n            g_dict[key] = key_list\n        utils.clean(dir_path)\n        self.CM = self._to_DAG(g_dict)\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n    else:\n        utils.clean(dir_path)\n        CP.warning('Python Error:\\n {0}'.format(error))\n        exit(0)\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.tsFCI.tsFCI","title":"<code>tsFCI</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>tsFCI causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/tsFCI.py</code> <pre><code>class tsFCI(CausalDiscoveryMethod):\n\"\"\"tsFCI causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data, \n                 min_lag,\n                 max_lag, \n                 verbosity, \n                 alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n\n    def run(self) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        # Remove all arguments from directory\n        dir_path = os.path.dirname(os.path.realpath(__file__))\n        Path(dir_path + \"/args\").mkdir(exist_ok=True)\n        Path(dir_path + \"/results\").mkdir(exist_ok=True)\n\n        script = dir_path + \"/pkgs/tsfci.R\"\n        r_arg_list = []\n\n        # COMMAND WITH ARGUMENTS\n        self.data.d.to_csv(dir_path + \"/args/data.csv\", index=False)\n        r_arg_list.append(dir_path + \"/args/data.csv\")\n        r_arg_list.append(str(self.alpha))\n        r_arg_list.append(str(self.max_lag))\n\n        r_arg_list.append(dir_path)\n        cmd = [\"Rscript\", script] + r_arg_list\n\n        p = Popen(cmd, cwd=\"./\", stdin=PIPE, stdout=PIPE, stderr=PIPE)\n\n        # Return R output or error\n        output, error = p.communicate()\n        print(output.decode('utf-8'))\n        if p.returncode == 0:\n            g_df = pd.read_csv(dir_path + \"/results/result.csv\", header=0, index_col=0)\n            g_dict = self.ts_fci_dataframe_to_dict(g_df, self.data.features, self.max_lag)\n            self.CM = self._to_DAG(g_dict)\n            utils.clean(dir_path)\n\n            if self.resfolder is not None: self.logger.close()\n            return self.CM\n\n        else:\n            utils.clean(dir_path)\n            print('R Error:\\n {0}'.format(error.decode('utf-8')))\n            exit(0)\n\n\n    def _to_DAG(self, graph):\n\"\"\"\n        Re-elaborate the result in a DAG.\n\n        Returns:\n            (DAG): result re-elaborated.\n        \"\"\"\n        tmp_dag = DAG(self.data.features, self.min_lag, self.max_lag, self.neglect_only_autodep)\n        tmp_dag.sys_context = dict()\n        for t in graph.keys():\n            for s in graph[t]:\n                lag = abs(s[1])\n                if lag &gt;= self.min_lag and lag &lt;= self.max_lag:\n                    tmp_dag.add_source(t, s[0], utils.DSCORE, 0, s[1])\n        return tmp_dag\n\n\n    def ts_fci_dataframe_to_dict(self, df, names, nlags) -&gt; dict:\n\"\"\"\n        Convert tsFCI result into a dict for _to_DAG.\n\n        Args:\n            df (DataFrame): graph.\n            names (list[str]): variables' name.\n            nlags (int): max time lag.\n\n        Returns:\n            dict: dict graph.\n        \"\"\"\n        # todo: check if its correct\n        for i in range(df.shape[1]):\n            for j in range(i+1, df.shape[1]):\n                if df[df.columns[i]].loc[df.columns[j]] == 2:\n                    if df[df.columns[j]].loc[df.columns[i]] == 2:\n                        print(df.columns[i] + \" &lt;-&gt; \" + df.columns[j])\n\n        g_dict = dict()\n        for name_y in names:\n            g_dict[name_y] = []\n        for ty in range(nlags):\n            for name_y in names:\n                t_name_y = df.columns[ty*len(names)+names.index(name_y)]\n                for tx in range(nlags):\n                    for name_x in names:\n                        t_name_x = df.columns[tx * len(names) + names.index(name_x)]\n                        if df[t_name_y].loc[t_name_x] == 2:\n                            if (name_x, tx-ty) not in g_dict[name_y]:\n                                g_dict[name_y].append((name_x, tx - ty))\n        print(g_dict)\n        return g_dict\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.tsFCI.tsFCI.__init__","title":"<code>__init__(data, min_lag, max_lag, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/tsFCI.py</code> <pre><code>def __init__(self, \n             data, \n             min_lag,\n             max_lag, \n             verbosity, \n             alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.tsFCI.tsFCI.run","title":"<code>run()</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/tsFCI.py</code> <pre><code>def run(self) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    # Remove all arguments from directory\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    Path(dir_path + \"/args\").mkdir(exist_ok=True)\n    Path(dir_path + \"/results\").mkdir(exist_ok=True)\n\n    script = dir_path + \"/pkgs/tsfci.R\"\n    r_arg_list = []\n\n    # COMMAND WITH ARGUMENTS\n    self.data.d.to_csv(dir_path + \"/args/data.csv\", index=False)\n    r_arg_list.append(dir_path + \"/args/data.csv\")\n    r_arg_list.append(str(self.alpha))\n    r_arg_list.append(str(self.max_lag))\n\n    r_arg_list.append(dir_path)\n    cmd = [\"Rscript\", script] + r_arg_list\n\n    p = Popen(cmd, cwd=\"./\", stdin=PIPE, stdout=PIPE, stderr=PIPE)\n\n    # Return R output or error\n    output, error = p.communicate()\n    print(output.decode('utf-8'))\n    if p.returncode == 0:\n        g_df = pd.read_csv(dir_path + \"/results/result.csv\", header=0, index_col=0)\n        g_dict = self.ts_fci_dataframe_to_dict(g_df, self.data.features, self.max_lag)\n        self.CM = self._to_DAG(g_dict)\n        utils.clean(dir_path)\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n\n    else:\n        utils.clean(dir_path)\n        print('R Error:\\n {0}'.format(error.decode('utf-8')))\n        exit(0)\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.tsFCI.tsFCI.ts_fci_dataframe_to_dict","title":"<code>ts_fci_dataframe_to_dict(df, names, nlags)</code>","text":"<p>Convert tsFCI result into a dict for _to_DAG.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>graph.</p> required <code>names</code> <code>list[str]</code> <p>variables' name.</p> required <code>nlags</code> <code>int</code> <p>max time lag.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dict graph.</p> Source code in <code>causalflow/causal_discovery/baseline/tsFCI.py</code> <pre><code>def ts_fci_dataframe_to_dict(self, df, names, nlags) -&gt; dict:\n\"\"\"\n    Convert tsFCI result into a dict for _to_DAG.\n\n    Args:\n        df (DataFrame): graph.\n        names (list[str]): variables' name.\n        nlags (int): max time lag.\n\n    Returns:\n        dict: dict graph.\n    \"\"\"\n    # todo: check if its correct\n    for i in range(df.shape[1]):\n        for j in range(i+1, df.shape[1]):\n            if df[df.columns[i]].loc[df.columns[j]] == 2:\n                if df[df.columns[j]].loc[df.columns[i]] == 2:\n                    print(df.columns[i] + \" &lt;-&gt; \" + df.columns[j])\n\n    g_dict = dict()\n    for name_y in names:\n        g_dict[name_y] = []\n    for ty in range(nlags):\n        for name_y in names:\n            t_name_y = df.columns[ty*len(names)+names.index(name_y)]\n            for tx in range(nlags):\n                for name_x in names:\n                    t_name_x = df.columns[tx * len(names) + names.index(name_x)]\n                    if df[t_name_y].loc[t_name_x] == 2:\n                        if (name_x, tx-ty) not in g_dict[name_y]:\n                            g_dict[name_y].append((name_x, tx - ty))\n    print(g_dict)\n    return g_dict\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.VarLiNGAM.VarLiNGAM","title":"<code>VarLiNGAM</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>VarLiNGAM causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/VarLiNGAM.py</code> <pre><code>class VarLiNGAM(CausalDiscoveryMethod):\n\"\"\"VarLiNGAM causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data, \n                 min_lag,\n                 max_lag, \n                 verbosity, \n                 alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n\n    def run(self) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        split_by_causal_effect_sign = True\n\n        model = VARLiNGAM(lags = self.max_lag, criterion='bic', prune=True)\n        model.fit(self.data.d)\n\n        m = model._adjacency_matrices\n        am = np.concatenate([*m], axis=1)\n\n        dag = np.abs(am) &gt; self.alpha\n\n        if split_by_causal_effect_sign:\n            direction = np.array(np.where(dag))\n            signs = np.zeros_like(dag).astype('int64')\n            for i, j in direction.T:\n                signs[i][j] = np.sign(am[i][j]).astype('int64')\n            dag = signs\n\n        dag = np.abs(dag)\n        names = self.data.features\n        res_dict = dict()\n        for e in range(dag.shape[0]):\n            res_dict[names[e]] = []\n        for c in range(dag.shape[0]):\n            for te in range(dag.shape[1]):\n                if dag[c][te] == 1:\n                    e = te%dag.shape[0]\n                    t = te//dag.shape[0]\n                    res_dict[names[e]].append((names[c], -t))\n        self.CM = self._to_DAG(res_dict)\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n\n    def _to_DAG(self, graph):\n\"\"\"\n        Re-elaborates the result in a DAG.\n\n        Returns:\n            (DAG): result re-elaborated.\n        \"\"\"\n        tmp_dag = DAG(self.data.features, self.min_lag, self.max_lag, self.neglect_only_autodep)\n        tmp_dag.sys_context = dict()\n        for t in graph.keys():\n            for s in graph[t]:\n                lag = abs(s[1])\n                if lag &gt;= self.min_lag and lag &lt;= self.max_lag:\n                    tmp_dag.add_source(t, s[0], utils.DSCORE, 0, s[1])\n        return tmp_dag\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.VarLiNGAM.VarLiNGAM.__init__","title":"<code>__init__(data, min_lag, max_lag, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/VarLiNGAM.py</code> <pre><code>def __init__(self, \n             data, \n             min_lag,\n             max_lag, \n             verbosity, \n             alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n</code></pre>"},{"location":"CAnDOIT/#causalflow.causal_discovery.baseline.VarLiNGAM.VarLiNGAM.run","title":"<code>run()</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/VarLiNGAM.py</code> <pre><code>def run(self) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    split_by_causal_effect_sign = True\n\n    model = VARLiNGAM(lags = self.max_lag, criterion='bic', prune=True)\n    model.fit(self.data.d)\n\n    m = model._adjacency_matrices\n    am = np.concatenate([*m], axis=1)\n\n    dag = np.abs(am) &gt; self.alpha\n\n    if split_by_causal_effect_sign:\n        direction = np.array(np.where(dag))\n        signs = np.zeros_like(dag).astype('int64')\n        for i, j in direction.T:\n            signs[i][j] = np.sign(am[i][j]).astype('int64')\n        dag = signs\n\n    dag = np.abs(dag)\n    names = self.data.features\n    res_dict = dict()\n    for e in range(dag.shape[0]):\n        res_dict[names[e]] = []\n    for c in range(dag.shape[0]):\n        for te in range(dag.shape[1]):\n            if dag[c][te] == 1:\n                e = te%dag.shape[0]\n                t = te//dag.shape[0]\n                res_dict[names[e]].append((names[c], -t))\n    self.CM = self._to_DAG(res_dict)\n\n    if self.resfolder is not None: self.logger.close()\n    return self.CM\n</code></pre>"},{"location":"FPCMCI/","title":"F-PCMCI","text":"<p>This module provides the CausalDiscoveryMethod class.</p> Classes <p>CausalDiscoveryMethod: abstract class used by all the causal discovery algorithms.</p> <p>This module provides the FPCMCI class.</p> Classes <p>FPCMCI: class containing the FPCMCI causal discovery algorithm.</p> <p>This module provides the CAnDOIT class.</p> Classes <p>CAnDOIT: class containing the CAnDOIT causal discovery algorithm.</p> <p>This module provides the DYNOTEARS class.</p> Classes <p>DYNOTEARS: class containing the DYNOTEARS causal discovery algorithm.</p> <p>This module provides the LPCMCI class.</p> Classes <p>LPCMCI: class containing the LPCMCI causal discovery algorithm.</p> <p>This module provides the PCMCI class.</p> Classes <p>PCMCI: class containing the PCMCI causal discovery algorithm.</p> <p>This module provides the PCMCI+ class.</p> Classes <p>PCMCIplus: class containing the PCMCI+ causal discovery algorithm.</p> <p>This module provides the TCDF class.</p> Classes <p>TCDF: class containing the TCDF causal discovery algorithm.</p> <p>This module provides the tsFCI class.</p> Classes <p>tsFCI: class containing the tsFCI causal discovery algorithm.</p> <p>This module provides the VarLiNGAM class.</p> Classes <p>VarLiNGAM: class containing the VarLiNGAM causal discovery algorithm.</p>"},{"location":"FPCMCI/#causalflow.causal_discovery.CausalDiscoveryMethod.CausalDiscoveryMethod","title":"<code>CausalDiscoveryMethod</code>","text":"<p>             Bases: <code>ABC</code></p> <p>CausalDiscoveryMethod class.</p> <p>CausalDiscoveryMethod is an abstract causal discovery method for  large-scale time series datasets.</p> Source code in <code>causalflow/causal_discovery/CausalDiscoveryMethod.py</code> <pre><code>class CausalDiscoveryMethod(ABC):\n\"\"\"\n    CausalDiscoveryMethod class.\n\n    CausalDiscoveryMethod is an abstract causal discovery method for \n    large-scale time series datasets.\n    \"\"\"\n\n    def __init__(self, \n                 data: Data, \n                 min_lag, max_lag, \n                 verbosity: CPLevel, \n                 alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class contructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n\n        \"\"\"\n        self.data = data\n        self.alpha = alpha\n        self.min_lag = min_lag\n        self.max_lag = max_lag\n        self.CM = DAG(self.data.features, min_lag, max_lag, neglect_only_autodep)\n        self.neglect_only_autodep = neglect_only_autodep\n\n        self.resfolder = resfolder\n        self.respath, self.dag_path, self.ts_dag_path = None, None, None\n        if resfolder is not None:\n            logpath, self.respath, self.dag_path, self.ts_dag_path = utils.get_selectorpath(resfolder)  \n            self.logger = Logger(logpath, clean_cls)\n            sys.stdout = self.logger\n\n        CP.set_verbosity(verbosity)\n\n\n    @abstractmethod\n    def run(self) -&gt; DAG:\n\"\"\"\n        Run causal discovery method.\n\n        Returns:\n            DAG: causal model.\n        \"\"\"\n        pass\n\n\n    def load(self, res_path):\n\"\"\"\n        Load previously estimated result .\n\n        Args:\n            res_path (str): pickle file path.\n        \"\"\"\n        with open(res_path, 'rb') as f:\n            r = pickle.load(f)\n            self.CM = r['causal_model']\n            self.alpha = r['alpha']\n            self.dag_path = r['dag_path']\n            self.ts_dag_path = r['ts_dag_path']\n\n\n    def save(self):\n\"\"\"Save causal discovery result as pickle file if resfolder is set.\"\"\"\n        if self.respath is not None:\n            if self.CM:\n                res = dict()\n                res['causal_model'] = copy.deepcopy(self.CM)\n                res['alpha'] = self.alpha\n                res['dag_path'] = self.dag_path\n                res['ts_dag_path'] = self.ts_dag_path\n                with open(self.respath, 'wb') as resfile:\n                    pickle.dump(res, resfile)\n            else:\n                CP.warning(\"Causal model impossible to save\")\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.CausalDiscoveryMethod.CausalDiscoveryMethod.__init__","title":"<code>__init__(data, min_lag, max_lag, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class contructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/CausalDiscoveryMethod.py</code> <pre><code>def __init__(self, \n             data: Data, \n             min_lag, max_lag, \n             verbosity: CPLevel, \n             alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class contructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n\n    \"\"\"\n    self.data = data\n    self.alpha = alpha\n    self.min_lag = min_lag\n    self.max_lag = max_lag\n    self.CM = DAG(self.data.features, min_lag, max_lag, neglect_only_autodep)\n    self.neglect_only_autodep = neglect_only_autodep\n\n    self.resfolder = resfolder\n    self.respath, self.dag_path, self.ts_dag_path = None, None, None\n    if resfolder is not None:\n        logpath, self.respath, self.dag_path, self.ts_dag_path = utils.get_selectorpath(resfolder)  \n        self.logger = Logger(logpath, clean_cls)\n        sys.stdout = self.logger\n\n    CP.set_verbosity(verbosity)\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.CausalDiscoveryMethod.CausalDiscoveryMethod.load","title":"<code>load(res_path)</code>","text":"<p>Load previously estimated result .</p> <p>Parameters:</p> Name Type Description Default <code>res_path</code> <code>str</code> <p>pickle file path.</p> required Source code in <code>causalflow/causal_discovery/CausalDiscoveryMethod.py</code> <pre><code>def load(self, res_path):\n\"\"\"\n    Load previously estimated result .\n\n    Args:\n        res_path (str): pickle file path.\n    \"\"\"\n    with open(res_path, 'rb') as f:\n        r = pickle.load(f)\n        self.CM = r['causal_model']\n        self.alpha = r['alpha']\n        self.dag_path = r['dag_path']\n        self.ts_dag_path = r['ts_dag_path']\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.CausalDiscoveryMethod.CausalDiscoveryMethod.run","title":"<code>run()</code>  <code>abstractmethod</code>","text":"<p>Run causal discovery method.</p> <p>Returns:</p> Name Type Description <code>DAG</code> <code>DAG</code> <p>causal model.</p> Source code in <code>causalflow/causal_discovery/CausalDiscoveryMethod.py</code> <pre><code>@abstractmethod\ndef run(self) -&gt; DAG:\n\"\"\"\n    Run causal discovery method.\n\n    Returns:\n        DAG: causal model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.CausalDiscoveryMethod.CausalDiscoveryMethod.save","title":"<code>save()</code>","text":"<p>Save causal discovery result as pickle file if resfolder is set.</p> Source code in <code>causalflow/causal_discovery/CausalDiscoveryMethod.py</code> <pre><code>def save(self):\n\"\"\"Save causal discovery result as pickle file if resfolder is set.\"\"\"\n    if self.respath is not None:\n        if self.CM:\n            res = dict()\n            res['causal_model'] = copy.deepcopy(self.CM)\n            res['alpha'] = self.alpha\n            res['dag_path'] = self.dag_path\n            res['ts_dag_path'] = self.ts_dag_path\n            with open(self.respath, 'wb') as resfile:\n                pickle.dump(res, resfile)\n        else:\n            CP.warning(\"Causal model impossible to save\")\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.FPCMCI.FPCMCI","title":"<code>FPCMCI</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>F-PCMCI causal discovery method.</p> Source code in <code>causalflow/causal_discovery/FPCMCI.py</code> <pre><code>class FPCMCI(CausalDiscoveryMethod):\n\"\"\"F-PCMCI causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data: Data, \n                 min_lag, max_lag, \n                 sel_method: SelectionMethod, val_condtest: CondIndTest, \n                 verbosity: CPLevel, \n                 f_alpha = 0.05, \n                 alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class contructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            sel_method (SelectionMethod): selection method.\n            val_condtest (CondIndTest): validation method.\n            verbosity (CPLevel): verbosity level.\n            f_alpha (float, optional): filter significance level. Defaults to 0.05.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n        self.f_alpha = f_alpha\n        self.sel_method = sel_method\n\n        self.validator = myPCMCI(self.alpha, min_lag, max_lag, val_condtest, verbosity, neglect_only_autodep = neglect_only_autodep)       \n\n\n    def run_filter(self):\n\"\"\"Run filter method.\"\"\"\n        CP.info(\"\\n\")\n        CP.info(DASH)\n        CP.info(\"Selecting relevant features among: \" + str(self.data.features))\n        CP.info(\"Selection method: \" + self.sel_method.name)\n        CP.info(\"Significance level: \" + str(self.f_alpha))\n        CP.info(\"Max lag time: \" + str(self.max_lag))\n        CP.info(\"Min lag time: \" + str(self.min_lag))\n        CP.info(\"Data length: \" + str(self.data.T))\n\n        self.sel_method.initialise(self.data, self.f_alpha, self.min_lag, self.max_lag, self.CM)\n        self.CM = self.sel_method.compute_dependencies()  \n\n\n    def run(self, remove_unneeded = True, nofilter = False) -&gt; DAG:\n\"\"\"\n        Run F-PCMCI.\n\n        Args:\n            remove_unneeded (bool, optional): Bit to remove unneeded (isolated) variables. Defaults to True.\n            nofilter (bool, optional): Bit to run F-PCMCI without filter. Defaults to False.\n\n        Returns:\n            DAG: causal model.\n        \"\"\"\n        link_assumptions = None\n\n        if not nofilter:\n            ## 1. FILTER\n            self.run_filter()\n\n            # list of selected features based on filter dependencies\n            self.CM.remove_unneeded_features()\n            if not self.CM.features: return None, None\n\n            ## 2. VALIDATOR\n            # shrink dataframe d by using the filter result\n            self.data.shrink(self.CM.features)\n\n            # selected links to check by the validator\n            link_assumptions = self.CM.get_link_assumptions()\n\n            # calculate dependencies on selected links\n            f_dag = copy.deepcopy(self.CM)\n\n        if self.min_lag != 0:\n            self.CM = self.validator.run(self.data, link_assumptions)\n        else:\n            self.CM = self.validator.run_plus(self.data, link_assumptions)\n\n        # list of selected features based on validator dependencies\n        if remove_unneeded: self.CM.remove_unneeded_features()\n\n        # Saving final causal model\n        if not nofilter: self._print_differences(f_dag, self.CM)\n        self.save()\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n\n\n    def load(self, res_path):\n\"\"\"\n        Load previously estimated result.\n\n        Args:\n            res_path (str): pickle file path.\n        \"\"\"\n        with open(res_path, 'rb') as f:\n            r = pickle.load(f)\n            self.CM = r['causal_model']\n            self.f_alpha = r['filter_alpha']\n            self.alpha = r['alpha']\n            self.dag_path = r['dag_path']\n            self.ts_dag_path = r['ts_dag_path']\n\n\n    def save(self):\n\"\"\"Save causal discovery result as pickle file if resfolder is set.\"\"\"\n        if self.respath is not None:\n            if self.CM:\n                res = dict()\n                res['causal_model'] = copy.deepcopy(self.CM)\n                res['features'] = copy.deepcopy(self.CM.features)\n                res['filter_alpha'] = self.f_alpha\n                res['alpha'] = self.alpha\n                res['dag_path'] = self.dag_path\n                res['ts_dag_path'] = self.ts_dag_path\n                with open(self.respath, 'wb') as resfile:\n                    pickle.dump(res, resfile)\n            else:\n                CP.warning(\"Causal model impossible to save\")\n\n\n    def _print_differences(self, old_dag : DAG, new_dag : DAG):\n\"\"\"\n        Print difference between old and new dependencies.\n\n        Args:\n            old_dep (DAG): old dag.\n            new_dep (DAG): new dag.\n        \"\"\"\n        # Check difference(s) between validator and filter dependencies\n        list_diffs = list()\n        tmp = copy.deepcopy(old_dag)\n        for t in tmp.g:\n            if t not in new_dag.g:\n                list_diffs.append(t)\n                continue\n\n            for s in tmp.g[t].sources:\n                if s not in new_dag.g[t].sources:\n                    list_diffs.append((s[0], s[1], t))\n\n        if list_diffs:\n            CP.info(\"\\n\")\n            CP.info(DASH)\n            CP.info(\"Difference(s):\")\n            for diff in list_diffs: \n                if type(diff) is tuple:\n                    CP.info(\"Removed (\" + str(diff[0]) + \" -\" + str(diff[1]) +\") --&gt; (\" + str(diff[2]) + \")\")\n                else:\n                    CP.info(diff + \" removed\")\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.FPCMCI.FPCMCI.__init__","title":"<code>__init__(data, min_lag, max_lag, sel_method, val_condtest, verbosity, f_alpha=0.05, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class contructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>sel_method</code> <code>SelectionMethod</code> <p>selection method.</p> required <code>val_condtest</code> <code>CondIndTest</code> <p>validation method.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>f_alpha</code> <code>float</code> <p>filter significance level. Defaults to 0.05.</p> <code>0.05</code> <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/FPCMCI.py</code> <pre><code>def __init__(self, \n             data: Data, \n             min_lag, max_lag, \n             sel_method: SelectionMethod, val_condtest: CondIndTest, \n             verbosity: CPLevel, \n             f_alpha = 0.05, \n             alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class contructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        sel_method (SelectionMethod): selection method.\n        val_condtest (CondIndTest): validation method.\n        verbosity (CPLevel): verbosity level.\n        f_alpha (float, optional): filter significance level. Defaults to 0.05.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n    self.f_alpha = f_alpha\n    self.sel_method = sel_method\n\n    self.validator = myPCMCI(self.alpha, min_lag, max_lag, val_condtest, verbosity, neglect_only_autodep = neglect_only_autodep)       \n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.FPCMCI.FPCMCI.load","title":"<code>load(res_path)</code>","text":"<p>Load previously estimated result.</p> <p>Parameters:</p> Name Type Description Default <code>res_path</code> <code>str</code> <p>pickle file path.</p> required Source code in <code>causalflow/causal_discovery/FPCMCI.py</code> <pre><code>def load(self, res_path):\n\"\"\"\n    Load previously estimated result.\n\n    Args:\n        res_path (str): pickle file path.\n    \"\"\"\n    with open(res_path, 'rb') as f:\n        r = pickle.load(f)\n        self.CM = r['causal_model']\n        self.f_alpha = r['filter_alpha']\n        self.alpha = r['alpha']\n        self.dag_path = r['dag_path']\n        self.ts_dag_path = r['ts_dag_path']\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.FPCMCI.FPCMCI.run","title":"<code>run(remove_unneeded=True, nofilter=False)</code>","text":"<p>Run F-PCMCI.</p> <p>Parameters:</p> Name Type Description Default <code>remove_unneeded</code> <code>bool</code> <p>Bit to remove unneeded (isolated) variables. Defaults to True.</p> <code>True</code> <code>nofilter</code> <code>bool</code> <p>Bit to run F-PCMCI without filter. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>DAG</code> <code>DAG</code> <p>causal model.</p> Source code in <code>causalflow/causal_discovery/FPCMCI.py</code> <pre><code>def run(self, remove_unneeded = True, nofilter = False) -&gt; DAG:\n\"\"\"\n    Run F-PCMCI.\n\n    Args:\n        remove_unneeded (bool, optional): Bit to remove unneeded (isolated) variables. Defaults to True.\n        nofilter (bool, optional): Bit to run F-PCMCI without filter. Defaults to False.\n\n    Returns:\n        DAG: causal model.\n    \"\"\"\n    link_assumptions = None\n\n    if not nofilter:\n        ## 1. FILTER\n        self.run_filter()\n\n        # list of selected features based on filter dependencies\n        self.CM.remove_unneeded_features()\n        if not self.CM.features: return None, None\n\n        ## 2. VALIDATOR\n        # shrink dataframe d by using the filter result\n        self.data.shrink(self.CM.features)\n\n        # selected links to check by the validator\n        link_assumptions = self.CM.get_link_assumptions()\n\n        # calculate dependencies on selected links\n        f_dag = copy.deepcopy(self.CM)\n\n    if self.min_lag != 0:\n        self.CM = self.validator.run(self.data, link_assumptions)\n    else:\n        self.CM = self.validator.run_plus(self.data, link_assumptions)\n\n    # list of selected features based on validator dependencies\n    if remove_unneeded: self.CM.remove_unneeded_features()\n\n    # Saving final causal model\n    if not nofilter: self._print_differences(f_dag, self.CM)\n    self.save()\n\n    if self.resfolder is not None: self.logger.close()\n    return self.CM\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.FPCMCI.FPCMCI.run_filter","title":"<code>run_filter()</code>","text":"<p>Run filter method.</p> Source code in <code>causalflow/causal_discovery/FPCMCI.py</code> <pre><code>def run_filter(self):\n\"\"\"Run filter method.\"\"\"\n    CP.info(\"\\n\")\n    CP.info(DASH)\n    CP.info(\"Selecting relevant features among: \" + str(self.data.features))\n    CP.info(\"Selection method: \" + self.sel_method.name)\n    CP.info(\"Significance level: \" + str(self.f_alpha))\n    CP.info(\"Max lag time: \" + str(self.max_lag))\n    CP.info(\"Min lag time: \" + str(self.min_lag))\n    CP.info(\"Data length: \" + str(self.data.T))\n\n    self.sel_method.initialise(self.data, self.f_alpha, self.min_lag, self.max_lag, self.CM)\n    self.CM = self.sel_method.compute_dependencies()  \n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.FPCMCI.FPCMCI.save","title":"<code>save()</code>","text":"<p>Save causal discovery result as pickle file if resfolder is set.</p> Source code in <code>causalflow/causal_discovery/FPCMCI.py</code> <pre><code>def save(self):\n\"\"\"Save causal discovery result as pickle file if resfolder is set.\"\"\"\n    if self.respath is not None:\n        if self.CM:\n            res = dict()\n            res['causal_model'] = copy.deepcopy(self.CM)\n            res['features'] = copy.deepcopy(self.CM.features)\n            res['filter_alpha'] = self.f_alpha\n            res['alpha'] = self.alpha\n            res['dag_path'] = self.dag_path\n            res['ts_dag_path'] = self.ts_dag_path\n            with open(self.respath, 'wb') as resfile:\n                pickle.dump(res, resfile)\n        else:\n            CP.warning(\"Causal model impossible to save\")\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.CAnDOIT.CAnDOIT","title":"<code>CAnDOIT</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>CAnDOIT causal discovery method.</p> Source code in <code>causalflow/causal_discovery/CAnDOIT.py</code> <pre><code>class CAnDOIT(CausalDiscoveryMethod):\n\"\"\"CAnDOIT causal discovery method.\"\"\"\n\n    def __init__(self, \n                 observation_data: Data, \n                 intervention_data: dict, \n                 min_lag, max_lag,\n                 sel_method: SelectionMethod, val_condtest: CondIndTest, \n                 verbosity: CPLevel, \n                 f_alpha = 0.05, \n                 alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 exclude_context = True,\n                 plot_data = False,\n                 clean_cls = True):\n\"\"\"\n        Class contructor.\n\n        Args:\n            observation_data (Data): observational data to analyse.\n            intervention_data (dict): interventional data to analyse in the form {INTERVENTION_VARIABLE : Data (same variables of observation_data)}.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            sel_method (SelectionMethod): selection method.\n            val_condtest (CondIndTest): validation method.\n            verbosity (CPLevel): verbosity level.\n            f_alpha (float, optional): filter significance level. Defaults to 0.05.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            exclude_context (bool, optional): Bit for neglecting context variables. Defaults to False.\n            plot_data (bool, optional): Bit for plotting your data. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        self.obs_data = observation_data\n        self.systems = observation_data.features\n        self.contexts = []\n        self.sys_context = {}\n        for k in intervention_data.keys():\n            self.contexts.append(\"C\" + k)\n            self.sys_context[k] = \"C\" + k\n        self.vars = self.systems + self.contexts\n\n        self.f_alpha = f_alpha\n        self.sel_method = sel_method\n        self.val_condtest = val_condtest\n        self.exclude_context = exclude_context\n        super().__init__(self.obs_data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n        # Create filter and validator data\n        self.filter_data, self.validator_data = self._prepare_data(self.obs_data, intervention_data, plot_data)\n\n\n        CP.info(\"\\n\")\n        CP.info(DASH)\n        CP.info(\"Observational data length: \" + str(observation_data.T))\n        CP.info(\"Interventional data length: \" + str(sum([d.T for d in intervention_data.values()])))\n        CP.info(\"Min lag time: \" + str(min_lag))\n        CP.info(\"Max lag time: \" + str(max_lag))\n        CP.info(\"Filter significance level: \" + str(f_alpha))\n        CP.info(\"PCMCI significance level: \" + str(alpha))\n        CP.info(\"Selection method: \" + sel_method.name)\n\n\n    @property    \n    def isThereInterv(self) -&gt; bool:\n\"\"\"\n        Check whether an intervention is present or not.\n\n        Returns:\n            bool: flag to identify if an intervention is present or not.\n        \"\"\"\n        return len(list(self.sys_context.keys())) &gt; 0\n\n\n    def JCI_assumptions(self):\n\"\"\"Initialise the algorithm initial causal structure with the JCI assumptions.\"\"\"\n        # ! JCI Assmpution 1: No system variable causes any context variable\n        # ! JCI Assmpution 2: No context variable is confounded with a system variable\n        # ! JCI Assmpution 3: The context distribution contains no (conditional) independences\n\n        knowledge = {self.vars.index(f): dict() for f in self.vars}\n\n        # ! JCI Assmpution 1\n        for k in self.contexts:\n            for x in self.systems:\n                for tau_i in range(0, self.max_lag + 1):\n                    knowledge[self.vars.index(k)][(self.vars.index(x), -tau_i)] = ''\n\n        # ! JCI Assmpution 2\n        for k in self.contexts:\n            for x in self.systems:\n                if x not in self.sys_context or (x in self.sys_context and k != self.sys_context[x]):\n                    for tau_i in range(0, self.max_lag + 1): knowledge[self.vars.index(x)][(self.vars.index(k), -tau_i)] = ''\n                elif x in self.sys_context and k == self.sys_context[x]:\n                    knowledge[self.vars.index(x)][(self.vars.index(k), 0)] = '--&gt;'\n                    knowledge[self.vars.index(k)][(self.vars.index(x), 0)] = '&lt;--'\n                    for tau_i in range(1, self.max_lag + 1): knowledge[self.vars.index(x)][(self.vars.index(k), -tau_i)] = ''\n                    for tau_i in range(1, self.max_lag + 1): knowledge[self.vars.index(k)][(self.vars.index(x), -tau_i)] = ''\n\n        # ! JCI Assmpution 3\n        for k1 in self.contexts:\n            for k2 in remove_from_list(self.contexts, k1):\n                knowledge[self.vars.index(k1)][(self.vars.index(k2), 0)] = '&lt;-&gt;'\n                # for tau_i in range(0, self.max_lag + 1): knowledge[self.vars.index(k1)][(self.vars.index(k2), -tau_i)] = '&lt;-&gt;'\n\n        # ! This models the context variables as chain across different time steps\n        for k in self.contexts:\n            for tau_i in range(1, self.max_lag + 1):\n                knowledge[self.vars.index(k)][(self.vars.index(k), -tau_i)] = '--&gt;' if tau_i == 1 else ''\n\n\n\n\n        out = {}\n        for j in range(len(self.vars)):\n            inner_dict = {} \n\n            for i in range(len(self.vars)):\n                for tau_i in range(0, self.max_lag + 1):\n                    if tau_i &gt; 0 or i != j:\n                        value = \"o?&gt;\" if tau_i &gt; 0 else \"o?o\"\n                        inner_dict[(i, -tau_i)] = value\n\n            out[j] = inner_dict\n\n        for j, links_j in knowledge.items():\n            for (i, lag_i), link_ij in links_j.items():\n                if link_ij == \"\":\n                    del out[j][(i, lag_i)]\n                else: \n                    out[j][(i, lag_i)] = link_ij\n        return out\n\n\n    def run_filter(self):\n\"\"\"Run filter method.\"\"\"\n        CP.info(\"Selecting relevant features among: \" + str(self.filter_data.features))\n\n        self.sel_method.initialise(self.obs_data, self.f_alpha, self.min_lag, self.max_lag, self.CM)\n        self.CM = self.sel_method.compute_dependencies()\n\n\n    def run_validator(self, link_assumptions = None) -&gt; DAG:\n\"\"\"\n        Run Validator (LPCMCI).\n\n        Args:\n            link_assumptions (dict, optional): link assumption with context. Defaults to None.\n\n        Returns:\n            DAG: causal model with context.\n        \"\"\"\n        self.validator = myLPCMCI(self.validator_data,\n                                self.min_lag, self.max_lag,\n                                self.sys_context,\n                                self.val_condtest,\n                                CP.verbosity,\n                                self.alpha)\n        causal_model = self.validator.run(link_assumptions)\n        causal_model.sys_context = self.CM.sys_context      \n\n        return causal_model\n\n\n    def run(self, remove_unneeded = True, nofilter = True) -&gt; DAG:\n\"\"\"\n        Run CAnDOIT.\n\n        Returns:\n            DAG: causal model.\n        \"\"\"\n        link_assumptions = None\n\n        if not nofilter:\n            #FIXME: to include also the filter. for now this is wrong\n            ## 1. FILTER\n            self.run_filter()\n\n            # list of selected features based on filter dependencies\n            self.CM.remove_unneeded_features()\n            if not self.CM.features: return None, None\n\n            self.obs_data.shrink(self.CM.features)\n            f_dag = copy.deepcopy(self.CM)\n\n            ## 2. VALIDATOR\n            # Add dependencies corresponding to the context variables \n            # ONLY if the the related system variable is still present\n            self.CM.add_context() \n\n            # shrink dataframe d by using the filter result\n            self.validator_data.shrink(self.CM.features)\n\n            # selected links to check by the validator\n            link_assumptions = self.CM.get_link_assumptions()\n\n        else:\n            # fullg = DAG(self.validator_data.features, self.min_lag, self.max_lag, False)\n            # fullg.sys_context = self.CM.sys_context\n            link_assumptions = self.JCI_assumptions()\n\n        # calculate dependencies on selected links\n        self.CM = self.run_validator(link_assumptions)\n\n        # list of selected features based on validator dependencies\n        if remove_unneeded: self.CM.remove_unneeded_features()\n        if self.exclude_context: self.CM.remove_context()\n\n        self.save()\n\n        return self.CM\n\n\n    def load(self, res_path):\n\"\"\"\n        Load previously estimated result.\n\n        Args:\n            res_path (str): pickle file path.\n        \"\"\"\n        with open(res_path, 'rb') as f:\n            r = pickle.load(f)\n            self.CM = r['causal_model']\n            self.f_alpha = r['filter_alpha']\n            self.alpha = r['alpha']\n            self.dag_path = r['dag_path']\n            self.ts_dag_path = r['ts_dag_path']\n\n\n    def save(self):\n\"\"\"Save causal discovery result as pickle file if resfolder is set.\"\"\"\n        if self.respath is not None:\n            if self.CM:\n                res = dict()\n                res['causal_model'] = copy.deepcopy(self.CM)\n                res['features'] = copy.deepcopy(self.CM.features)\n                res['filter_alpha'] = self.f_alpha\n                res['alpha'] = self.alpha\n                res['dag_path'] = self.dag_path\n                res['ts_dag_path'] = self.ts_dag_path\n                with open(self.respath, 'wb') as resfile:\n                    pickle.dump(res, resfile)\n            else:\n                CP.warning(\"Causal model impossible to save\")\n\n\n    def _prepare_data(self, obser_data, inter_data, plot_data):\n\"\"\"\n        Prepare data for filter and validator phases.\n\n        Args:\n            obser_data (Data): observational data.\n            inter_data (Data): interventional data.\n            plot_data (bool): boolean bit to plot the generated data.\n\n        Returns:\n            Data, Data: filter data obj and validator data obj.\n        \"\"\"\n        # Filter phase data preparation\n        filter_data = copy.deepcopy(obser_data.d)\n        for int_data in inter_data.values(): filter_data = pd.concat([filter_data, int_data.d], axis = 0, ignore_index = True)\n        filter_data = Data(filter_data, vars = obser_data.features)\n\n        # Validator phase data preparation\n        validator_data = copy.deepcopy(obser_data.d)\n        context_vars = dict()\n        for int_var, int_data in inter_data.items():\n\n            # Create context variable name\n            context_varname = 'C' + int_var\n\n            # Store a dict of context variable and system variable corresponding to an intervention\n            self.CM.sys_context[int_var] = context_varname\n\n            # Create context variable data\n            # context_data = np.ones(shape=int_data.d[int_var].shape)\n            context_data = int_data.d[int_var]\n            context_start = len(validator_data)\n            context_end = context_start + len(context_data)\n            context_vars[context_varname] = {'data': context_data, 'start': context_start, 'end': context_end}\n\n            validator_data = pd.concat([validator_data, int_data.d], axis = 0, ignore_index = True)\n\n        for var in context_vars:\n            new_column = np.zeros(shape = (len(validator_data),))\n            new_column[context_vars[var]['start']: context_vars[var]['end']] = context_vars[var]['data']\n            validator_data[var] = new_column\n\n        validator_data = Data(validator_data, vars = list(validator_data.columns))\n\n        if plot_data: validator_data.plot_timeseries()\n        return filter_data, validator_data\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.CAnDOIT.CAnDOIT.isThereInterv","title":"<code>isThereInterv: bool</code>  <code>property</code>","text":"<p>Check whether an intervention is present or not.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>flag to identify if an intervention is present or not.</p>"},{"location":"FPCMCI/#causalflow.causal_discovery.CAnDOIT.CAnDOIT.JCI_assumptions","title":"<code>JCI_assumptions()</code>","text":"<p>Initialise the algorithm initial causal structure with the JCI assumptions.</p> Source code in <code>causalflow/causal_discovery/CAnDOIT.py</code> <pre><code>def JCI_assumptions(self):\n\"\"\"Initialise the algorithm initial causal structure with the JCI assumptions.\"\"\"\n    # ! JCI Assmpution 1: No system variable causes any context variable\n    # ! JCI Assmpution 2: No context variable is confounded with a system variable\n    # ! JCI Assmpution 3: The context distribution contains no (conditional) independences\n\n    knowledge = {self.vars.index(f): dict() for f in self.vars}\n\n    # ! JCI Assmpution 1\n    for k in self.contexts:\n        for x in self.systems:\n            for tau_i in range(0, self.max_lag + 1):\n                knowledge[self.vars.index(k)][(self.vars.index(x), -tau_i)] = ''\n\n    # ! JCI Assmpution 2\n    for k in self.contexts:\n        for x in self.systems:\n            if x not in self.sys_context or (x in self.sys_context and k != self.sys_context[x]):\n                for tau_i in range(0, self.max_lag + 1): knowledge[self.vars.index(x)][(self.vars.index(k), -tau_i)] = ''\n            elif x in self.sys_context and k == self.sys_context[x]:\n                knowledge[self.vars.index(x)][(self.vars.index(k), 0)] = '--&gt;'\n                knowledge[self.vars.index(k)][(self.vars.index(x), 0)] = '&lt;--'\n                for tau_i in range(1, self.max_lag + 1): knowledge[self.vars.index(x)][(self.vars.index(k), -tau_i)] = ''\n                for tau_i in range(1, self.max_lag + 1): knowledge[self.vars.index(k)][(self.vars.index(x), -tau_i)] = ''\n\n    # ! JCI Assmpution 3\n    for k1 in self.contexts:\n        for k2 in remove_from_list(self.contexts, k1):\n            knowledge[self.vars.index(k1)][(self.vars.index(k2), 0)] = '&lt;-&gt;'\n            # for tau_i in range(0, self.max_lag + 1): knowledge[self.vars.index(k1)][(self.vars.index(k2), -tau_i)] = '&lt;-&gt;'\n\n    # ! This models the context variables as chain across different time steps\n    for k in self.contexts:\n        for tau_i in range(1, self.max_lag + 1):\n            knowledge[self.vars.index(k)][(self.vars.index(k), -tau_i)] = '--&gt;' if tau_i == 1 else ''\n\n\n\n\n    out = {}\n    for j in range(len(self.vars)):\n        inner_dict = {} \n\n        for i in range(len(self.vars)):\n            for tau_i in range(0, self.max_lag + 1):\n                if tau_i &gt; 0 or i != j:\n                    value = \"o?&gt;\" if tau_i &gt; 0 else \"o?o\"\n                    inner_dict[(i, -tau_i)] = value\n\n        out[j] = inner_dict\n\n    for j, links_j in knowledge.items():\n        for (i, lag_i), link_ij in links_j.items():\n            if link_ij == \"\":\n                del out[j][(i, lag_i)]\n            else: \n                out[j][(i, lag_i)] = link_ij\n    return out\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.CAnDOIT.CAnDOIT.__init__","title":"<code>__init__(observation_data, intervention_data, min_lag, max_lag, sel_method, val_condtest, verbosity, f_alpha=0.05, alpha=0.05, resfolder=None, neglect_only_autodep=False, exclude_context=True, plot_data=False, clean_cls=True)</code>","text":"<p>Class contructor.</p> <p>Parameters:</p> Name Type Description Default <code>observation_data</code> <code>Data</code> <p>observational data to analyse.</p> required <code>intervention_data</code> <code>dict</code> <p>interventional data to analyse in the form {INTERVENTION_VARIABLE : Data (same variables of observation_data)}.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>sel_method</code> <code>SelectionMethod</code> <p>selection method.</p> required <code>val_condtest</code> <code>CondIndTest</code> <p>validation method.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>f_alpha</code> <code>float</code> <p>filter significance level. Defaults to 0.05.</p> <code>0.05</code> <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>exclude_context</code> <code>bool</code> <p>Bit for neglecting context variables. Defaults to False.</p> <code>True</code> <code>plot_data</code> <code>bool</code> <p>Bit for plotting your data. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/CAnDOIT.py</code> <pre><code>def __init__(self, \n             observation_data: Data, \n             intervention_data: dict, \n             min_lag, max_lag,\n             sel_method: SelectionMethod, val_condtest: CondIndTest, \n             verbosity: CPLevel, \n             f_alpha = 0.05, \n             alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False,\n             exclude_context = True,\n             plot_data = False,\n             clean_cls = True):\n\"\"\"\n    Class contructor.\n\n    Args:\n        observation_data (Data): observational data to analyse.\n        intervention_data (dict): interventional data to analyse in the form {INTERVENTION_VARIABLE : Data (same variables of observation_data)}.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        sel_method (SelectionMethod): selection method.\n        val_condtest (CondIndTest): validation method.\n        verbosity (CPLevel): verbosity level.\n        f_alpha (float, optional): filter significance level. Defaults to 0.05.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        exclude_context (bool, optional): Bit for neglecting context variables. Defaults to False.\n        plot_data (bool, optional): Bit for plotting your data. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    self.obs_data = observation_data\n    self.systems = observation_data.features\n    self.contexts = []\n    self.sys_context = {}\n    for k in intervention_data.keys():\n        self.contexts.append(\"C\" + k)\n        self.sys_context[k] = \"C\" + k\n    self.vars = self.systems + self.contexts\n\n    self.f_alpha = f_alpha\n    self.sel_method = sel_method\n    self.val_condtest = val_condtest\n    self.exclude_context = exclude_context\n    super().__init__(self.obs_data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n    # Create filter and validator data\n    self.filter_data, self.validator_data = self._prepare_data(self.obs_data, intervention_data, plot_data)\n\n\n    CP.info(\"\\n\")\n    CP.info(DASH)\n    CP.info(\"Observational data length: \" + str(observation_data.T))\n    CP.info(\"Interventional data length: \" + str(sum([d.T for d in intervention_data.values()])))\n    CP.info(\"Min lag time: \" + str(min_lag))\n    CP.info(\"Max lag time: \" + str(max_lag))\n    CP.info(\"Filter significance level: \" + str(f_alpha))\n    CP.info(\"PCMCI significance level: \" + str(alpha))\n    CP.info(\"Selection method: \" + sel_method.name)\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.CAnDOIT.CAnDOIT.load","title":"<code>load(res_path)</code>","text":"<p>Load previously estimated result.</p> <p>Parameters:</p> Name Type Description Default <code>res_path</code> <code>str</code> <p>pickle file path.</p> required Source code in <code>causalflow/causal_discovery/CAnDOIT.py</code> <pre><code>def load(self, res_path):\n\"\"\"\n    Load previously estimated result.\n\n    Args:\n        res_path (str): pickle file path.\n    \"\"\"\n    with open(res_path, 'rb') as f:\n        r = pickle.load(f)\n        self.CM = r['causal_model']\n        self.f_alpha = r['filter_alpha']\n        self.alpha = r['alpha']\n        self.dag_path = r['dag_path']\n        self.ts_dag_path = r['ts_dag_path']\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.CAnDOIT.CAnDOIT.run","title":"<code>run(remove_unneeded=True, nofilter=True)</code>","text":"<p>Run CAnDOIT.</p> <p>Returns:</p> Name Type Description <code>DAG</code> <code>DAG</code> <p>causal model.</p> Source code in <code>causalflow/causal_discovery/CAnDOIT.py</code> <pre><code>def run(self, remove_unneeded = True, nofilter = True) -&gt; DAG:\n\"\"\"\n    Run CAnDOIT.\n\n    Returns:\n        DAG: causal model.\n    \"\"\"\n    link_assumptions = None\n\n    if not nofilter:\n        #FIXME: to include also the filter. for now this is wrong\n        ## 1. FILTER\n        self.run_filter()\n\n        # list of selected features based on filter dependencies\n        self.CM.remove_unneeded_features()\n        if not self.CM.features: return None, None\n\n        self.obs_data.shrink(self.CM.features)\n        f_dag = copy.deepcopy(self.CM)\n\n        ## 2. VALIDATOR\n        # Add dependencies corresponding to the context variables \n        # ONLY if the the related system variable is still present\n        self.CM.add_context() \n\n        # shrink dataframe d by using the filter result\n        self.validator_data.shrink(self.CM.features)\n\n        # selected links to check by the validator\n        link_assumptions = self.CM.get_link_assumptions()\n\n    else:\n        # fullg = DAG(self.validator_data.features, self.min_lag, self.max_lag, False)\n        # fullg.sys_context = self.CM.sys_context\n        link_assumptions = self.JCI_assumptions()\n\n    # calculate dependencies on selected links\n    self.CM = self.run_validator(link_assumptions)\n\n    # list of selected features based on validator dependencies\n    if remove_unneeded: self.CM.remove_unneeded_features()\n    if self.exclude_context: self.CM.remove_context()\n\n    self.save()\n\n    return self.CM\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.CAnDOIT.CAnDOIT.run_filter","title":"<code>run_filter()</code>","text":"<p>Run filter method.</p> Source code in <code>causalflow/causal_discovery/CAnDOIT.py</code> <pre><code>def run_filter(self):\n\"\"\"Run filter method.\"\"\"\n    CP.info(\"Selecting relevant features among: \" + str(self.filter_data.features))\n\n    self.sel_method.initialise(self.obs_data, self.f_alpha, self.min_lag, self.max_lag, self.CM)\n    self.CM = self.sel_method.compute_dependencies()\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.CAnDOIT.CAnDOIT.run_validator","title":"<code>run_validator(link_assumptions=None)</code>","text":"<p>Run Validator (LPCMCI).</p> <p>Parameters:</p> Name Type Description Default <code>link_assumptions</code> <code>dict</code> <p>link assumption with context. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DAG</code> <code>DAG</code> <p>causal model with context.</p> Source code in <code>causalflow/causal_discovery/CAnDOIT.py</code> <pre><code>def run_validator(self, link_assumptions = None) -&gt; DAG:\n\"\"\"\n    Run Validator (LPCMCI).\n\n    Args:\n        link_assumptions (dict, optional): link assumption with context. Defaults to None.\n\n    Returns:\n        DAG: causal model with context.\n    \"\"\"\n    self.validator = myLPCMCI(self.validator_data,\n                            self.min_lag, self.max_lag,\n                            self.sys_context,\n                            self.val_condtest,\n                            CP.verbosity,\n                            self.alpha)\n    causal_model = self.validator.run(link_assumptions)\n    causal_model.sys_context = self.CM.sys_context      \n\n    return causal_model\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.CAnDOIT.CAnDOIT.save","title":"<code>save()</code>","text":"<p>Save causal discovery result as pickle file if resfolder is set.</p> Source code in <code>causalflow/causal_discovery/CAnDOIT.py</code> <pre><code>def save(self):\n\"\"\"Save causal discovery result as pickle file if resfolder is set.\"\"\"\n    if self.respath is not None:\n        if self.CM:\n            res = dict()\n            res['causal_model'] = copy.deepcopy(self.CM)\n            res['features'] = copy.deepcopy(self.CM.features)\n            res['filter_alpha'] = self.f_alpha\n            res['alpha'] = self.alpha\n            res['dag_path'] = self.dag_path\n            res['ts_dag_path'] = self.ts_dag_path\n            with open(self.respath, 'wb') as resfile:\n                pickle.dump(res, resfile)\n        else:\n            CP.warning(\"Causal model impossible to save\")\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.DYNOTEARS.DYNOTEARS","title":"<code>DYNOTEARS</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>DYNOTEARS causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/DYNOTEARS.py</code> <pre><code>class DYNOTEARS(CausalDiscoveryMethod):\n\"\"\"DYNOTEARS causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data, \n                 min_lag,\n                 max_lag, \n                 verbosity, \n                 alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n    def run(self) -&gt; DAG:\n\"\"\"\n        Run DYNOTEARS algorithm.\n\n        Returns:\n            DAG: causal discovery result.\n        \"\"\"\n        graph_dict = dict()\n        for name in self.data.features:\n            graph_dict[name] = []\n        sm = from_pandas_dynamic(self.data.d, p=self.max_lag)\n\n        tname_to_name_dict = dict()\n        count_lag = 0\n        idx_name = 0\n        for tname in sm.nodes:\n            tname_to_name_dict[tname] = self.data.features[idx_name]\n            if count_lag == self.max_lag:\n                idx_name = idx_name +1\n                count_lag = -1\n            count_lag = count_lag +1\n\n        for ce in sm.edges:\n            c = ce[0]\n            e = ce[1]\n            w = sm.adj[c][e][\"weight\"]\n            tc = int(c.partition(\"lag\")[2])\n            te = int(e.partition(\"lag\")[2])\n            t = tc - te\n            if (tname_to_name_dict[c], -t) not in graph_dict[tname_to_name_dict[e]]:\n                graph_dict[tname_to_name_dict[e]].append((tname_to_name_dict[c], w, -t))\n\n        self.CM = self._to_DAG(graph_dict)\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n\n\n    def _to_DAG(self, graph):\n\"\"\"\n        Re-elaborate the result in a DAG.\n\n        Returns:\n            (DAG): result re-elaborated.\n        \"\"\"\n        tmp_dag = DAG(self.data.features, self.min_lag, self.max_lag, self.neglect_only_autodep)\n        tmp_dag.sys_context = dict()\n        for t in graph.keys():\n            for s in graph[t]:\n                lag = abs(s[2])\n                if lag &gt;= self.min_lag and lag &lt;= self.max_lag:\n                    tmp_dag.add_source(t, s[0], abs(s[1]), 0, s[2])\n        return tmp_dag\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.DYNOTEARS.DYNOTEARS.__init__","title":"<code>__init__(data, min_lag, max_lag, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/DYNOTEARS.py</code> <pre><code>def __init__(self, \n             data, \n             min_lag,\n             max_lag, \n             verbosity, \n             alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.DYNOTEARS.DYNOTEARS.run","title":"<code>run()</code>","text":"<p>Run DYNOTEARS algorithm.</p> <p>Returns:</p> Name Type Description <code>DAG</code> <code>DAG</code> <p>causal discovery result.</p> Source code in <code>causalflow/causal_discovery/baseline/DYNOTEARS.py</code> <pre><code>def run(self) -&gt; DAG:\n\"\"\"\n    Run DYNOTEARS algorithm.\n\n    Returns:\n        DAG: causal discovery result.\n    \"\"\"\n    graph_dict = dict()\n    for name in self.data.features:\n        graph_dict[name] = []\n    sm = from_pandas_dynamic(self.data.d, p=self.max_lag)\n\n    tname_to_name_dict = dict()\n    count_lag = 0\n    idx_name = 0\n    for tname in sm.nodes:\n        tname_to_name_dict[tname] = self.data.features[idx_name]\n        if count_lag == self.max_lag:\n            idx_name = idx_name +1\n            count_lag = -1\n        count_lag = count_lag +1\n\n    for ce in sm.edges:\n        c = ce[0]\n        e = ce[1]\n        w = sm.adj[c][e][\"weight\"]\n        tc = int(c.partition(\"lag\")[2])\n        te = int(e.partition(\"lag\")[2])\n        t = tc - te\n        if (tname_to_name_dict[c], -t) not in graph_dict[tname_to_name_dict[e]]:\n            graph_dict[tname_to_name_dict[e]].append((tname_to_name_dict[c], w, -t))\n\n    self.CM = self._to_DAG(graph_dict)\n\n    if self.resfolder is not None: self.logger.close()\n    return self.CM\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.LPCMCI.LPCMCI","title":"<code>LPCMCI</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>LPCMCI causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/LPCMCI.py</code> <pre><code>class LPCMCI(CausalDiscoveryMethod):\n\"\"\"LPCMCI causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data: Data,\n                 min_lag, max_lag, \n                 val_condtest: CondIndTest, \n                 verbosity: CPLevel,\n                 alpha = 0.05, \n                 resfolder = None, \n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            val_condtest (CondIndTest): validation method.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n        # build tigramite dataset\n        vector = np.vectorize(float)\n        d = vector(data.d)\n\n        # init pcmci\n        self.lpcmci = lpcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                             cond_ind_test = val_condtest,\n                             verbosity = verbosity.value)\n\n\n    def run(self, link_assumptions = None) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        CP.info('\\n')\n        CP.info(DASH)\n        CP.info(\"Running Causal Discovery Algorithm\")\n        self.result = self.lpcmci.run_lpcmci(link_assumptions = link_assumptions,\n                                             tau_max = self.max_lag,\n                                             tau_min = self.min_lag,\n                                             pc_alpha = self.alpha)\n\n        self.CM = self._to_DAG()\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n\n\n    def _to_DAG(self):\n\"\"\"\n        Re-elaborate the PCMCI result in a new dictionary.\n\n        Returns:\n            (DAG): lpcmci result re-elaborated.\n        \"\"\"\n        vars = self.data.features\n        tmp_dag = DAG(vars, self.min_lag, self.max_lag)\n        tmp_dag.sys_context = dict()\n        N, lags = self.result['graph'][0].shape\n        for s in range(len(self.result['graph'])):\n            for t in range(N):\n                for lag in range(lags):\n                    if self.result['graph'][s][t,lag] != '':\n                        arrowtype = self.result['graph'][s][t,lag]\n\n                        if arrowtype == LinkType.Bidirected.value:\n                            if ((vars[s], abs(lag)) in tmp_dag.g[vars[t]].sources and \n                                tmp_dag.g[t].sources[(vars[s], abs(lag))][TYPE] == LinkType.Bidirected.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif arrowtype == LinkType.Uncertain.value:\n                            if ((vars[t], abs(lag)) in tmp_dag.g[vars[s]].sources and \n                                tmp_dag.g[vars[s]].sources[(vars[t], abs(lag))][TYPE] == LinkType.Uncertain.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif (arrowtype == LinkType.Directed.value or\n                              arrowtype == LinkType.HalfUncertain.value):\n                            tmp_dag.add_source(vars[t], \n                                            vars[s],\n                                            self.result['val_matrix'][s][t,lag],\n                                            self.result['p_matrix'][s][t,lag],\n                                            lag,\n                                            arrowtype)\n        return tmp_dag\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.LPCMCI.LPCMCI.__init__","title":"<code>__init__(data, min_lag, max_lag, val_condtest, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>val_condtest</code> <code>CondIndTest</code> <p>validation method.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/LPCMCI.py</code> <pre><code>def __init__(self, \n             data: Data,\n             min_lag, max_lag, \n             val_condtest: CondIndTest, \n             verbosity: CPLevel,\n             alpha = 0.05, \n             resfolder = None, \n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        val_condtest (CondIndTest): validation method.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n    # build tigramite dataset\n    vector = np.vectorize(float)\n    d = vector(data.d)\n\n    # init pcmci\n    self.lpcmci = lpcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                         cond_ind_test = val_condtest,\n                         verbosity = verbosity.value)\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.LPCMCI.LPCMCI.run","title":"<code>run(link_assumptions=None)</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/LPCMCI.py</code> <pre><code>def run(self, link_assumptions = None) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    CP.info('\\n')\n    CP.info(DASH)\n    CP.info(\"Running Causal Discovery Algorithm\")\n    self.result = self.lpcmci.run_lpcmci(link_assumptions = link_assumptions,\n                                         tau_max = self.max_lag,\n                                         tau_min = self.min_lag,\n                                         pc_alpha = self.alpha)\n\n    self.CM = self._to_DAG()\n\n    if self.resfolder is not None: self.logger.close()\n    return self.CM\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.PCMCI.PCMCI","title":"<code>PCMCI</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>PCMCI causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/PCMCI.py</code> <pre><code>class PCMCI(CausalDiscoveryMethod):\n\"\"\"PCMCI causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data: Data, \n                 min_lag, max_lag, \n                 val_condtest: CondIndTest, \n                 verbosity: CPLevel,\n                 pc_alpha = 0.05, \n                 alpha = 0.05, \n                 resfolder = None, \n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            val_condtest (CondIndTest): validation method.\n            verbosity (CPLevel): verbosity level.\n            pc_alpha (float, optional): PC significance level. Defaults to 0.05.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n        self.pc_alpha = pc_alpha\n\n        # build tigramite dataset\n        vector = np.vectorize(float)\n        d = vector(data.d)\n\n        # init pcmci\n        self.pcmci = pcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                           cond_ind_test = val_condtest,\n                           verbosity = verbosity.value)\n\n\n    def run(self) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        CP.info('\\n')\n        CP.info(DASH)\n        CP.info(\"Running Causal Discovery Algorithm\")\n\n        self.result = self.pcmci.run_pcmci(tau_max = self.max_lag,\n                                           tau_min = self.min_lag,\n                                           alpha_level = self.alpha,\n                                           pc_alpha = self.pc_alpha)\n\n        self.CM = self._to_DAG()\n\n        if self.resfolder is not None: self.logger.close()        \n        return self.CM\n\n\n    def _to_DAG(self):\n\"\"\"\n        Re-elaborates the PCMCI result in a new dictionary.\n\n        Returns:\n            (DAG): pcmci result re-elaborated.\n        \"\"\"\n        vars = self.data.features\n        tmp_dag = DAG(vars, self.min_lag, self.max_lag)\n        tmp_dag.sys_context = dict()\n        N, lags = self.result['graph'][0].shape\n        for s in range(len(self.result['graph'])):\n            for t in range(N):\n                for lag in range(lags):\n                    if self.result['graph'][s][t,lag] != '':\n                        arrowtype = self.result['graph'][s][t,lag]\n\n                        if arrowtype == LinkType.Bidirected.value:\n                            if ((vars[s], abs(lag)) in tmp_dag.g[vars[t]].sources and \n                                tmp_dag.g[t].sources[(vars[s], abs(lag))][TYPE] == LinkType.Bidirected.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif arrowtype == LinkType.Uncertain.value:\n                            if ((vars[t], abs(lag)) in tmp_dag.g[vars[s]].sources and \n                                tmp_dag.g[vars[s]].sources[(vars[t], abs(lag))][TYPE] == LinkType.Uncertain.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif (arrowtype == LinkType.Directed.value or\n                              arrowtype == LinkType.HalfUncertain.value):\n                            tmp_dag.add_source(vars[t], \n                                            vars[s],\n                                            self.result['val_matrix'][s][t,lag],\n                                            self.result['p_matrix'][s][t,lag],\n                                            lag,\n                                            arrowtype)\n        return tmp_dag\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.PCMCI.PCMCI.__init__","title":"<code>__init__(data, min_lag, max_lag, val_condtest, verbosity, pc_alpha=0.05, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>val_condtest</code> <code>CondIndTest</code> <p>validation method.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>pc_alpha</code> <code>float</code> <p>PC significance level. Defaults to 0.05.</p> <code>0.05</code> <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/PCMCI.py</code> <pre><code>def __init__(self, \n             data: Data, \n             min_lag, max_lag, \n             val_condtest: CondIndTest, \n             verbosity: CPLevel,\n             pc_alpha = 0.05, \n             alpha = 0.05, \n             resfolder = None, \n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        val_condtest (CondIndTest): validation method.\n        verbosity (CPLevel): verbosity level.\n        pc_alpha (float, optional): PC significance level. Defaults to 0.05.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n    self.pc_alpha = pc_alpha\n\n    # build tigramite dataset\n    vector = np.vectorize(float)\n    d = vector(data.d)\n\n    # init pcmci\n    self.pcmci = pcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                       cond_ind_test = val_condtest,\n                       verbosity = verbosity.value)\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.PCMCI.PCMCI.run","title":"<code>run()</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/PCMCI.py</code> <pre><code>def run(self) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    CP.info('\\n')\n    CP.info(DASH)\n    CP.info(\"Running Causal Discovery Algorithm\")\n\n    self.result = self.pcmci.run_pcmci(tau_max = self.max_lag,\n                                       tau_min = self.min_lag,\n                                       alpha_level = self.alpha,\n                                       pc_alpha = self.pc_alpha)\n\n    self.CM = self._to_DAG()\n\n    if self.resfolder is not None: self.logger.close()        \n    return self.CM\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.PCMCIplus.PCMCIplus","title":"<code>PCMCIplus</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>PCMCI+ causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/PCMCIplus.py</code> <pre><code>class PCMCIplus(CausalDiscoveryMethod):\n\"\"\"PCMCI+ causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data: Data, \n                 min_lag, max_lag, \n                 val_condtest: CondIndTest, \n                 verbosity: CPLevel,\n                 alpha = 0.05, \n                 resfolder = None, \n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            val_condtest (CondIndTest): validation method.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n        # build tigramite dataset\n        vector = np.vectorize(float)\n        d = vector(data.d)\n\n        # init pcmci\n        self.pcmci = pcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                           cond_ind_test = val_condtest,\n                           verbosity = verbosity.value)\n\n\n    def run(self, link_assumptions=None) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        CP.info('\\n')\n        CP.info(DASH)\n        CP.info(\"Running Causal Discovery Algorithm\")\n\n        self.result = self.pcmci.run_pcmciplus(link_assumptions=link_assumptions,\n                                               tau_max = self.max_lag,\n                                               tau_min = 0,\n                                               pc_alpha = self.alpha)\n\n        self.CM = self._to_DAG()\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n\n\n    def _to_DAG(self):\n\"\"\"\n        Re-elaborates the PCMCI result in a new dictionary.\n\n        Returns:\n            (DAG): pcmci result re-elaborated.\n        \"\"\"\n        vars = self.data.features\n        tmp_dag = DAG(vars, self.min_lag, self.max_lag)\n        tmp_dag.sys_context = dict()\n        N, lags = self.result['graph'][0].shape\n        for s in range(len(self.result['graph'])):\n            for t in range(N):\n                for lag in range(lags):\n                    if self.result['graph'][s][t,lag] != '':\n                        arrowtype = self.result['graph'][s][t,lag]\n\n                        if arrowtype == LinkType.Bidirected.value:\n                            if ((vars[s], abs(lag)) in tmp_dag.g[vars[t]].sources and \n                                tmp_dag.g[t].sources[(vars[s], abs(lag))][TYPE] == LinkType.Bidirected.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif arrowtype == LinkType.Uncertain.value:\n                            if ((vars[t], abs(lag)) in tmp_dag.g[vars[s]].sources and \n                                tmp_dag.g[vars[s]].sources[(vars[t], abs(lag))][TYPE] == LinkType.Uncertain.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif (arrowtype == LinkType.Directed.value or\n                              arrowtype == LinkType.HalfUncertain.value):\n                            tmp_dag.add_source(vars[t], \n                                            vars[s],\n                                            self.result['val_matrix'][s][t,lag],\n                                            self.result['p_matrix'][s][t,lag],\n                                            lag,\n                                            arrowtype)\n        return tmp_dag\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.PCMCIplus.PCMCIplus.__init__","title":"<code>__init__(data, min_lag, max_lag, val_condtest, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>val_condtest</code> <code>CondIndTest</code> <p>validation method.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/PCMCIplus.py</code> <pre><code>def __init__(self, \n             data: Data, \n             min_lag, max_lag, \n             val_condtest: CondIndTest, \n             verbosity: CPLevel,\n             alpha = 0.05, \n             resfolder = None, \n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        val_condtest (CondIndTest): validation method.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n    # build tigramite dataset\n    vector = np.vectorize(float)\n    d = vector(data.d)\n\n    # init pcmci\n    self.pcmci = pcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                       cond_ind_test = val_condtest,\n                       verbosity = verbosity.value)\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.PCMCIplus.PCMCIplus.run","title":"<code>run(link_assumptions=None)</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/PCMCIplus.py</code> <pre><code>def run(self, link_assumptions=None) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    CP.info('\\n')\n    CP.info(DASH)\n    CP.info(\"Running Causal Discovery Algorithm\")\n\n    self.result = self.pcmci.run_pcmciplus(link_assumptions=link_assumptions,\n                                           tau_max = self.max_lag,\n                                           tau_min = 0,\n                                           pc_alpha = self.alpha)\n\n    self.CM = self._to_DAG()\n\n    if self.resfolder is not None: self.logger.close()\n    return self.CM\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.TCDF.TCDF","title":"<code>TCDF</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>TCDF causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/TCDF.py</code> <pre><code>class TCDF(CausalDiscoveryMethod):\n\"\"\"TCDF causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data, \n                 min_lag,\n                 max_lag, \n                 verbosity, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            verbosity (CPLevel): verbosity level.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, resfolder=resfolder, neglect_only_autodep=neglect_only_autodep, clean_cls=clean_cls)\n\n\n    def run(self, \n            epochs=1000,  \n            kernel_size=4, \n            dilation_coefficient=4, \n            hidden_layers=0, \n            learning_rate=0.01,\n            cuda=False) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        # Remove all arguments from directory\n        dir_path = os.path.dirname(os.path.realpath(__file__))\n        Path(dir_path+\"/args\").mkdir(exist_ok=True)\n        Path(dir_path+\"/results\").mkdir(exist_ok=True)\n        script = dir_path + \"/pkgs/TCDF-master/runTCDF\" + \".py\"\n        r_arg_list = []\n        r_arg_list.append(\"--epochs\")\n        r_arg_list.append(str(epochs))\n        r_arg_list.append(\"--kernel_size\")\n        r_arg_list.append(str(kernel_size))\n        r_arg_list.append(\"--dilation_coefficient\")\n        r_arg_list.append(str(dilation_coefficient))\n        r_arg_list.append(\"--hidden_layers\")\n        r_arg_list.append(str(hidden_layers))\n        r_arg_list.append(\"--learning_rate\")\n        r_arg_list.append(str(learning_rate))\n        r_arg_list.append(\"--significance\")\n        r_arg_list.append(str(0.8))\n        self.data.d.to_csv(dir_path + \"/args/data.csv\", index=False)\n        r_arg_list.append(\"--data\")\n        r_arg_list.append(dir_path + \"/args/data.csv\")            \n        if cuda: r_arg_list.append(\"--cuda\")\n        r_arg_list.append(\"--path\")\n        r_arg_list.append(dir_path)\n\n        cmd = [\"python\", script] + r_arg_list\n        p = Popen(cmd, cwd=\"./\", stdin=PIPE, stdout=PIPE, stderr=PIPE)\n\n        # Return R output or error\n        output, error = p.communicate()\n        CP.info(output.decode('utf-8'))\n        if p.returncode == 0:\n            g_dict = json.load(open(dir_path + \"/results/tcdf_result.txt\"))\n            for key in g_dict.keys():\n                key_list = []\n                for elem in g_dict[key]:\n                    key_list.append(tuple(elem))\n                g_dict[key] = key_list\n            utils.clean(dir_path)\n            self.CM = self._to_DAG(g_dict)\n\n            if self.resfolder is not None: self.logger.close()\n            return self.CM\n        else:\n            utils.clean(dir_path)\n            CP.warning('Python Error:\\n {0}'.format(error))\n            exit(0)\n\n\n    def _to_DAG(self, graph):\n\"\"\"\n        Re-elaborate the result in a DAG.\n\n        Returns:\n            (DAG): result re-elaborated.\n        \"\"\"\n        tmp_dag = DAG(self.data.features, self.min_lag, self.max_lag, self.neglect_only_autodep)\n        tmp_dag.sys_context = dict()\n        for t in graph.keys():\n            for s in graph[t]:\n                lag = abs(s[1])\n                if lag &gt;= self.min_lag and lag &lt;= self.max_lag:\n                    tmp_dag.add_source(t, s[0], utils.DSCORE, 0, s[1])\n        return tmp_dag\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.TCDF.TCDF.__init__","title":"<code>__init__(data, min_lag, max_lag, verbosity, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/TCDF.py</code> <pre><code>def __init__(self, \n             data, \n             min_lag,\n             max_lag, \n             verbosity, \n             resfolder = None,\n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        verbosity (CPLevel): verbosity level.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, resfolder=resfolder, neglect_only_autodep=neglect_only_autodep, clean_cls=clean_cls)\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.TCDF.TCDF.run","title":"<code>run(epochs=1000, kernel_size=4, dilation_coefficient=4, hidden_layers=0, learning_rate=0.01, cuda=False)</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/TCDF.py</code> <pre><code>def run(self, \n        epochs=1000,  \n        kernel_size=4, \n        dilation_coefficient=4, \n        hidden_layers=0, \n        learning_rate=0.01,\n        cuda=False) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    # Remove all arguments from directory\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    Path(dir_path+\"/args\").mkdir(exist_ok=True)\n    Path(dir_path+\"/results\").mkdir(exist_ok=True)\n    script = dir_path + \"/pkgs/TCDF-master/runTCDF\" + \".py\"\n    r_arg_list = []\n    r_arg_list.append(\"--epochs\")\n    r_arg_list.append(str(epochs))\n    r_arg_list.append(\"--kernel_size\")\n    r_arg_list.append(str(kernel_size))\n    r_arg_list.append(\"--dilation_coefficient\")\n    r_arg_list.append(str(dilation_coefficient))\n    r_arg_list.append(\"--hidden_layers\")\n    r_arg_list.append(str(hidden_layers))\n    r_arg_list.append(\"--learning_rate\")\n    r_arg_list.append(str(learning_rate))\n    r_arg_list.append(\"--significance\")\n    r_arg_list.append(str(0.8))\n    self.data.d.to_csv(dir_path + \"/args/data.csv\", index=False)\n    r_arg_list.append(\"--data\")\n    r_arg_list.append(dir_path + \"/args/data.csv\")            \n    if cuda: r_arg_list.append(\"--cuda\")\n    r_arg_list.append(\"--path\")\n    r_arg_list.append(dir_path)\n\n    cmd = [\"python\", script] + r_arg_list\n    p = Popen(cmd, cwd=\"./\", stdin=PIPE, stdout=PIPE, stderr=PIPE)\n\n    # Return R output or error\n    output, error = p.communicate()\n    CP.info(output.decode('utf-8'))\n    if p.returncode == 0:\n        g_dict = json.load(open(dir_path + \"/results/tcdf_result.txt\"))\n        for key in g_dict.keys():\n            key_list = []\n            for elem in g_dict[key]:\n                key_list.append(tuple(elem))\n            g_dict[key] = key_list\n        utils.clean(dir_path)\n        self.CM = self._to_DAG(g_dict)\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n    else:\n        utils.clean(dir_path)\n        CP.warning('Python Error:\\n {0}'.format(error))\n        exit(0)\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.tsFCI.tsFCI","title":"<code>tsFCI</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>tsFCI causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/tsFCI.py</code> <pre><code>class tsFCI(CausalDiscoveryMethod):\n\"\"\"tsFCI causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data, \n                 min_lag,\n                 max_lag, \n                 verbosity, \n                 alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n\n    def run(self) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        # Remove all arguments from directory\n        dir_path = os.path.dirname(os.path.realpath(__file__))\n        Path(dir_path + \"/args\").mkdir(exist_ok=True)\n        Path(dir_path + \"/results\").mkdir(exist_ok=True)\n\n        script = dir_path + \"/pkgs/tsfci.R\"\n        r_arg_list = []\n\n        # COMMAND WITH ARGUMENTS\n        self.data.d.to_csv(dir_path + \"/args/data.csv\", index=False)\n        r_arg_list.append(dir_path + \"/args/data.csv\")\n        r_arg_list.append(str(self.alpha))\n        r_arg_list.append(str(self.max_lag))\n\n        r_arg_list.append(dir_path)\n        cmd = [\"Rscript\", script] + r_arg_list\n\n        p = Popen(cmd, cwd=\"./\", stdin=PIPE, stdout=PIPE, stderr=PIPE)\n\n        # Return R output or error\n        output, error = p.communicate()\n        print(output.decode('utf-8'))\n        if p.returncode == 0:\n            g_df = pd.read_csv(dir_path + \"/results/result.csv\", header=0, index_col=0)\n            g_dict = self.ts_fci_dataframe_to_dict(g_df, self.data.features, self.max_lag)\n            self.CM = self._to_DAG(g_dict)\n            utils.clean(dir_path)\n\n            if self.resfolder is not None: self.logger.close()\n            return self.CM\n\n        else:\n            utils.clean(dir_path)\n            print('R Error:\\n {0}'.format(error.decode('utf-8')))\n            exit(0)\n\n\n    def _to_DAG(self, graph):\n\"\"\"\n        Re-elaborate the result in a DAG.\n\n        Returns:\n            (DAG): result re-elaborated.\n        \"\"\"\n        tmp_dag = DAG(self.data.features, self.min_lag, self.max_lag, self.neglect_only_autodep)\n        tmp_dag.sys_context = dict()\n        for t in graph.keys():\n            for s in graph[t]:\n                lag = abs(s[1])\n                if lag &gt;= self.min_lag and lag &lt;= self.max_lag:\n                    tmp_dag.add_source(t, s[0], utils.DSCORE, 0, s[1])\n        return tmp_dag\n\n\n    def ts_fci_dataframe_to_dict(self, df, names, nlags) -&gt; dict:\n\"\"\"\n        Convert tsFCI result into a dict for _to_DAG.\n\n        Args:\n            df (DataFrame): graph.\n            names (list[str]): variables' name.\n            nlags (int): max time lag.\n\n        Returns:\n            dict: dict graph.\n        \"\"\"\n        # todo: check if its correct\n        for i in range(df.shape[1]):\n            for j in range(i+1, df.shape[1]):\n                if df[df.columns[i]].loc[df.columns[j]] == 2:\n                    if df[df.columns[j]].loc[df.columns[i]] == 2:\n                        print(df.columns[i] + \" &lt;-&gt; \" + df.columns[j])\n\n        g_dict = dict()\n        for name_y in names:\n            g_dict[name_y] = []\n        for ty in range(nlags):\n            for name_y in names:\n                t_name_y = df.columns[ty*len(names)+names.index(name_y)]\n                for tx in range(nlags):\n                    for name_x in names:\n                        t_name_x = df.columns[tx * len(names) + names.index(name_x)]\n                        if df[t_name_y].loc[t_name_x] == 2:\n                            if (name_x, tx-ty) not in g_dict[name_y]:\n                                g_dict[name_y].append((name_x, tx - ty))\n        print(g_dict)\n        return g_dict\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.tsFCI.tsFCI.__init__","title":"<code>__init__(data, min_lag, max_lag, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/tsFCI.py</code> <pre><code>def __init__(self, \n             data, \n             min_lag,\n             max_lag, \n             verbosity, \n             alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.tsFCI.tsFCI.run","title":"<code>run()</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/tsFCI.py</code> <pre><code>def run(self) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    # Remove all arguments from directory\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    Path(dir_path + \"/args\").mkdir(exist_ok=True)\n    Path(dir_path + \"/results\").mkdir(exist_ok=True)\n\n    script = dir_path + \"/pkgs/tsfci.R\"\n    r_arg_list = []\n\n    # COMMAND WITH ARGUMENTS\n    self.data.d.to_csv(dir_path + \"/args/data.csv\", index=False)\n    r_arg_list.append(dir_path + \"/args/data.csv\")\n    r_arg_list.append(str(self.alpha))\n    r_arg_list.append(str(self.max_lag))\n\n    r_arg_list.append(dir_path)\n    cmd = [\"Rscript\", script] + r_arg_list\n\n    p = Popen(cmd, cwd=\"./\", stdin=PIPE, stdout=PIPE, stderr=PIPE)\n\n    # Return R output or error\n    output, error = p.communicate()\n    print(output.decode('utf-8'))\n    if p.returncode == 0:\n        g_df = pd.read_csv(dir_path + \"/results/result.csv\", header=0, index_col=0)\n        g_dict = self.ts_fci_dataframe_to_dict(g_df, self.data.features, self.max_lag)\n        self.CM = self._to_DAG(g_dict)\n        utils.clean(dir_path)\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n\n    else:\n        utils.clean(dir_path)\n        print('R Error:\\n {0}'.format(error.decode('utf-8')))\n        exit(0)\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.tsFCI.tsFCI.ts_fci_dataframe_to_dict","title":"<code>ts_fci_dataframe_to_dict(df, names, nlags)</code>","text":"<p>Convert tsFCI result into a dict for _to_DAG.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>graph.</p> required <code>names</code> <code>list[str]</code> <p>variables' name.</p> required <code>nlags</code> <code>int</code> <p>max time lag.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dict graph.</p> Source code in <code>causalflow/causal_discovery/baseline/tsFCI.py</code> <pre><code>def ts_fci_dataframe_to_dict(self, df, names, nlags) -&gt; dict:\n\"\"\"\n    Convert tsFCI result into a dict for _to_DAG.\n\n    Args:\n        df (DataFrame): graph.\n        names (list[str]): variables' name.\n        nlags (int): max time lag.\n\n    Returns:\n        dict: dict graph.\n    \"\"\"\n    # todo: check if its correct\n    for i in range(df.shape[1]):\n        for j in range(i+1, df.shape[1]):\n            if df[df.columns[i]].loc[df.columns[j]] == 2:\n                if df[df.columns[j]].loc[df.columns[i]] == 2:\n                    print(df.columns[i] + \" &lt;-&gt; \" + df.columns[j])\n\n    g_dict = dict()\n    for name_y in names:\n        g_dict[name_y] = []\n    for ty in range(nlags):\n        for name_y in names:\n            t_name_y = df.columns[ty*len(names)+names.index(name_y)]\n            for tx in range(nlags):\n                for name_x in names:\n                    t_name_x = df.columns[tx * len(names) + names.index(name_x)]\n                    if df[t_name_y].loc[t_name_x] == 2:\n                        if (name_x, tx-ty) not in g_dict[name_y]:\n                            g_dict[name_y].append((name_x, tx - ty))\n    print(g_dict)\n    return g_dict\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.VarLiNGAM.VarLiNGAM","title":"<code>VarLiNGAM</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>VarLiNGAM causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/VarLiNGAM.py</code> <pre><code>class VarLiNGAM(CausalDiscoveryMethod):\n\"\"\"VarLiNGAM causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data, \n                 min_lag,\n                 max_lag, \n                 verbosity, \n                 alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n\n    def run(self) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        split_by_causal_effect_sign = True\n\n        model = VARLiNGAM(lags = self.max_lag, criterion='bic', prune=True)\n        model.fit(self.data.d)\n\n        m = model._adjacency_matrices\n        am = np.concatenate([*m], axis=1)\n\n        dag = np.abs(am) &gt; self.alpha\n\n        if split_by_causal_effect_sign:\n            direction = np.array(np.where(dag))\n            signs = np.zeros_like(dag).astype('int64')\n            for i, j in direction.T:\n                signs[i][j] = np.sign(am[i][j]).astype('int64')\n            dag = signs\n\n        dag = np.abs(dag)\n        names = self.data.features\n        res_dict = dict()\n        for e in range(dag.shape[0]):\n            res_dict[names[e]] = []\n        for c in range(dag.shape[0]):\n            for te in range(dag.shape[1]):\n                if dag[c][te] == 1:\n                    e = te%dag.shape[0]\n                    t = te//dag.shape[0]\n                    res_dict[names[e]].append((names[c], -t))\n        self.CM = self._to_DAG(res_dict)\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n\n    def _to_DAG(self, graph):\n\"\"\"\n        Re-elaborates the result in a DAG.\n\n        Returns:\n            (DAG): result re-elaborated.\n        \"\"\"\n        tmp_dag = DAG(self.data.features, self.min_lag, self.max_lag, self.neglect_only_autodep)\n        tmp_dag.sys_context = dict()\n        for t in graph.keys():\n            for s in graph[t]:\n                lag = abs(s[1])\n                if lag &gt;= self.min_lag and lag &lt;= self.max_lag:\n                    tmp_dag.add_source(t, s[0], utils.DSCORE, 0, s[1])\n        return tmp_dag\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.VarLiNGAM.VarLiNGAM.__init__","title":"<code>__init__(data, min_lag, max_lag, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/VarLiNGAM.py</code> <pre><code>def __init__(self, \n             data, \n             min_lag,\n             max_lag, \n             verbosity, \n             alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n</code></pre>"},{"location":"FPCMCI/#causalflow.causal_discovery.baseline.VarLiNGAM.VarLiNGAM.run","title":"<code>run()</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/VarLiNGAM.py</code> <pre><code>def run(self) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    split_by_causal_effect_sign = True\n\n    model = VARLiNGAM(lags = self.max_lag, criterion='bic', prune=True)\n    model.fit(self.data.d)\n\n    m = model._adjacency_matrices\n    am = np.concatenate([*m], axis=1)\n\n    dag = np.abs(am) &gt; self.alpha\n\n    if split_by_causal_effect_sign:\n        direction = np.array(np.where(dag))\n        signs = np.zeros_like(dag).astype('int64')\n        for i, j in direction.T:\n            signs[i][j] = np.sign(am[i][j]).astype('int64')\n        dag = signs\n\n    dag = np.abs(dag)\n    names = self.data.features\n    res_dict = dict()\n    for e in range(dag.shape[0]):\n        res_dict[names[e]] = []\n    for c in range(dag.shape[0]):\n        for te in range(dag.shape[1]):\n            if dag[c][te] == 1:\n                e = te%dag.shape[0]\n                t = te//dag.shape[0]\n                res_dict[names[e]].append((names[c], -t))\n    self.CM = self._to_DAG(res_dict)\n\n    if self.resfolder is not None: self.logger.close()\n    return self.CM\n</code></pre>"},{"location":"basics/","title":"Utilities","text":"<p>This module provides the support classes and various constants.</p> Classes <p>LabelType: label type. LinkType: link type. ImageExt: image extention.</p> <p>This module provides the Logger class.</p> Classes <p>Logger: class responsible for the log.</p> <p>This module provides methods for computing various metrics.</p> <p>This module provides utilities methods.</p>"},{"location":"basics/#causalflow.basics.constants.ImageExt","title":"<code>ImageExt</code>","text":"<p>             Bases: <code>Enum</code></p> <p>ImageExt Enumerator.</p> Source code in <code>causalflow/basics/constants.py</code> <pre><code>class ImageExt(Enum):\n\"\"\"ImageExt Enumerator.\"\"\"\n\n    PNG = \".png\"\n    PDF = \".pdf\"\n    JPG = \".jpg\"\n</code></pre>"},{"location":"basics/#causalflow.basics.constants.LabelType","title":"<code>LabelType</code>","text":"<p>             Bases: <code>Enum</code></p> <p>LabelType Enumerator.</p> Source code in <code>causalflow/basics/constants.py</code> <pre><code>class LabelType(Enum):\n\"\"\"LabelType Enumerator.\"\"\"\n\n    Lag = \"Lag\"\n    Score = \"Score\"\n    NoLabels = \"NoLabels\"\n</code></pre>"},{"location":"basics/#causalflow.basics.constants.LinkType","title":"<code>LinkType</code>","text":"<p>             Bases: <code>Enum</code></p> <p>LinkType Enumerator.</p> Source code in <code>causalflow/basics/constants.py</code> <pre><code>class LinkType(Enum):\n\"\"\"LinkType Enumerator.\"\"\"\n\n    Directed = \"--&gt;\"\n    Uncertain = \"o-o\"\n    Bidirected = \"&lt;-&gt;\"\n    HalfUncertain = \"o-&gt;\"\n</code></pre>"},{"location":"basics/#causalflow.basics.logger.Logger","title":"<code>Logger</code>","text":"<p>             Bases: <code>object</code></p> <p>Logger class.</p> Source code in <code>causalflow/basics/logger.py</code> <pre><code>class Logger(object):\n\"\"\"Logger class.\"\"\"\n\n    def __init__(self, path, clean_console = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            path (str): log file path.\n            clean_console (bool, optional): clean console flag. Defaults to True.\n        \"\"\"\n        self.terminal = sys.stdout\n        self.log = open(path, \"w\")\n        if clean_console: cls()\n\n    def write(self, message):\n\"\"\"Write message.\"\"\"\n        self.terminal.write(message)\n        self.log.write(message)\n\n    def flush(self):\n\"\"\"python3 compatibility.\"\"\"\n        # this flush method is needed for python 3 compatibility.\n        # this handles the flush command by doing nothing.\n        # you might want to specify some extra behavior here.\n        pass\n\n    def close(self):\n\"\"\"Close logger.\"\"\"\n        sys.stdout = sys.__stdout__\n        self.log.close()\n</code></pre>"},{"location":"basics/#causalflow.basics.logger.Logger.__init__","title":"<code>__init__(path, clean_console=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>log file path.</p> required <code>clean_console</code> <code>bool</code> <p>clean console flag. Defaults to True.</p> <code>True</code> Source code in <code>causalflow/basics/logger.py</code> <pre><code>def __init__(self, path, clean_console = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        path (str): log file path.\n        clean_console (bool, optional): clean console flag. Defaults to True.\n    \"\"\"\n    self.terminal = sys.stdout\n    self.log = open(path, \"w\")\n    if clean_console: cls()\n</code></pre>"},{"location":"basics/#causalflow.basics.logger.Logger.close","title":"<code>close()</code>","text":"<p>Close logger.</p> Source code in <code>causalflow/basics/logger.py</code> <pre><code>def close(self):\n\"\"\"Close logger.\"\"\"\n    sys.stdout = sys.__stdout__\n    self.log.close()\n</code></pre>"},{"location":"basics/#causalflow.basics.logger.Logger.flush","title":"<code>flush()</code>","text":"<p>python3 compatibility.</p> Source code in <code>causalflow/basics/logger.py</code> <pre><code>def flush(self):\n\"\"\"python3 compatibility.\"\"\"\n    # this flush method is needed for python 3 compatibility.\n    # this handles the flush command by doing nothing.\n    # you might want to specify some extra behavior here.\n    pass\n</code></pre>"},{"location":"basics/#causalflow.basics.logger.Logger.write","title":"<code>write(message)</code>","text":"<p>Write message.</p> Source code in <code>causalflow/basics/logger.py</code> <pre><code>def write(self, message):\n\"\"\"Write message.\"\"\"\n    self.terminal.write(message)\n    self.log.write(message)\n</code></pre>"},{"location":"basics/#causalflow.basics.metrics.FNR","title":"<code>FNR(gt, cm, alsoOrient=False)</code>","text":"<p>Compute False Negative Rate between ground-truth causal graph and the estimated one.</p> <p>Parameters:</p> Name Type Description Default <code>cm</code> <code>dict</code> <p>estimated SCM.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>false negative rate.</p> Source code in <code>causalflow/basics/metrics.py</code> <pre><code>def FNR(gt, cm, alsoOrient = False) -&gt; float:\n\"\"\"\n    Compute False Negative Rate between ground-truth causal graph and the estimated one.\n\n    Args:\n        cm (dict): estimated SCM.\n\n    Returns:\n        float: false negative rate.\n    \"\"\"\n    fn = get_FN(gt, cm, alsoOrient)\n    tp = get_TP(gt, cm, alsoOrient)\n    if tp + fn == 0: return 0\n    return fn / (tp + fn)\n</code></pre>"},{"location":"basics/#causalflow.basics.metrics.FPR","title":"<code>FPR(gt, min_lag, max_lag, cm, alsoOrient=False)</code>","text":"<p>Compute False Positve Rate between ground-truth causal graph and the estimated one.</p> <p>Parameters:</p> Name Type Description Default <code>cm</code> <code>dict</code> <p>estimated SCM.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>false positive rate.</p> Source code in <code>causalflow/basics/metrics.py</code> <pre><code>def FPR(gt, min_lag, max_lag, cm, alsoOrient = False) -&gt; float:\n\"\"\"\n    Compute False Positve Rate between ground-truth causal graph and the estimated one.\n\n    Args:\n        cm (dict): estimated SCM.\n\n    Returns:\n        float: false positive rate.\n    \"\"\"\n    fp = get_FP(gt, cm, alsoOrient)\n    tn = get_TN(gt, min_lag, max_lag, cm, alsoOrient)\n    if tn + fp == 0: return 0\n    return fp / (tn + fp)\n</code></pre>"},{"location":"basics/#causalflow.basics.metrics.TNR","title":"<code>TNR(gt, min_lag, max_lag, cm, alsoOrient=False)</code>","text":"<p>Compute True Negative Rate between ground-truth causal graph and the estimated one.</p> <p>Parameters:</p> Name Type Description Default <code>cm</code> <code>dict</code> <p>estimated SCM.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>true negative rate.</p> Source code in <code>causalflow/basics/metrics.py</code> <pre><code>def TNR(gt, min_lag, max_lag, cm, alsoOrient = False) -&gt; float:\n\"\"\"\n    Compute True Negative Rate between ground-truth causal graph and the estimated one.\n\n    Args:\n        cm (dict): estimated SCM.\n\n    Returns:\n        float: true negative rate.\n    \"\"\"\n    tn = get_TN(gt, min_lag, max_lag, cm, alsoOrient)\n    fp = get_FP(gt, cm, alsoOrient)\n    if tn + fp == 0: return 0\n    return tn / (tn + fp)\n</code></pre>"},{"location":"basics/#causalflow.basics.metrics.TPR","title":"<code>TPR(gt, cm, alsoOrient=False)</code>","text":"<p>Compute True Positive Rate between ground-truth causal graph and the estimated one.</p> <p>Parameters:</p> Name Type Description Default <code>cm</code> <code>dict</code> <p>estimated SCM.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>true positive rate.</p> Source code in <code>causalflow/basics/metrics.py</code> <pre><code>def TPR(gt, cm, alsoOrient = False) -&gt; float:\n\"\"\"\n    Compute True Positive Rate between ground-truth causal graph and the estimated one.\n\n    Args:\n        cm (dict): estimated SCM.\n\n    Returns:\n        float: true positive rate.\n    \"\"\"\n    tp = get_TP(gt, cm, alsoOrient)\n    fn = get_FN(gt, cm, alsoOrient)\n    if tp + fn == 0: return 0\n    return tp / (tp + fn)\n</code></pre>"},{"location":"basics/#causalflow.basics.metrics.f1_score","title":"<code>f1_score(gt, cm, alsoOrient=False)</code>","text":"<p>Compute F1-score between ground-truth causal graph and the estimated one.</p> <p>Parameters:</p> Name Type Description Default <code>cm</code> <code>dict</code> <p>estimated SCM.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>F1-score.</p> Source code in <code>causalflow/basics/metrics.py</code> <pre><code>def f1_score(gt, cm, alsoOrient = False) -&gt; float:\n\"\"\"\n    Compute F1-score between ground-truth causal graph and the estimated one.\n\n    Args:\n        cm (dict): estimated SCM.\n\n    Returns:\n        float: F1-score.\n    \"\"\"\n    p = precision(gt, cm, alsoOrient)\n    r = recall(gt, cm, alsoOrient)\n    if p + r == 0: return 0\n    return (2 * p * r) / (p + r)\n</code></pre>"},{"location":"basics/#causalflow.basics.metrics.fully_connected_dag","title":"<code>fully_connected_dag(features, min_lag, max_lag, alsoOrient=False)</code>","text":"<p>Build a fully connected DAG.</p> Source code in <code>causalflow/basics/metrics.py</code> <pre><code>def fully_connected_dag(features, min_lag, max_lag, alsoOrient = False):\n\"\"\"Build a fully connected DAG.\"\"\"\n    if not alsoOrient:\n        g = {f: list() for f in features}\n        for t in g:\n            for s in g:\n                for l in range(min_lag, max_lag + 1):\n                    if s == t and l == 0: continue \n                    g[t].append((s, -abs(l)))\n    else:\n        g = {f: {} for f in features}\n        for t in g:\n            for s in g:\n                for l in range(min_lag, max_lag + 1):\n                    if s == t and l == 0: continue \n                    g[t][(s, -abs(l))] = ['--&gt;', 'o-&gt;', '&lt;-&gt;', 'o-o']\n    return g\n</code></pre>"},{"location":"basics/#causalflow.basics.metrics.get_FN","title":"<code>get_FN(gt, cm, alsoOrient=False)</code>","text":"<p>Compute false negative number: edge present in the groundtruth but absent in the causal model.</p> <p>Parameters:</p> Name Type Description Default <code>cm</code> <code>dict</code> <p>estimated SCM.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>false negative.</p> Source code in <code>causalflow/basics/metrics.py</code> <pre><code>def get_FN(gt, cm, alsoOrient = False) -&gt; int:\n\"\"\"\n    Compute false negative number: edge present in the groundtruth but absent in the causal model.\n\n    Args:\n        cm (dict): estimated SCM.\n\n    Returns:\n        int: false negative.\n    \"\"\"\n    counter = 0\n    if not alsoOrient:\n        for t in gt.keys():\n            for s in gt[t]:\n                if s not in cm[t]: counter += 1\n    else:\n        for t in gt.keys():\n            for s in gt[t]:\n                if s not in cm[t]: counter += 3\n                else:\n                    if gt[t][s][0] != cm[t][s][0]: counter += 1\n                    if gt[t][s][1] != cm[t][s][1]: counter += 1\n                    if gt[t][s][2] != cm[t][s][2]: counter += 1\n    return counter\n</code></pre>"},{"location":"basics/#causalflow.basics.metrics.get_FP","title":"<code>get_FP(gt, cm, alsoOrient=False)</code>","text":"<p>Compute false positive number: edge present in the causal model but absent in the groundtruth.</p> <p>Parameters:</p> Name Type Description Default <code>cm</code> <code>dict</code> <p>estimated SCM.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>false positive.</p> Source code in <code>causalflow/basics/metrics.py</code> <pre><code>def get_FP(gt, cm, alsoOrient = False) -&gt; int:\n\"\"\"\n    Compute false positive number: edge present in the causal model but absent in the groundtruth.\n\n    Args:\n        cm (dict): estimated SCM.\n\n    Returns:\n        int: false positive.\n    \"\"\"\n    counter = 0\n    if not alsoOrient:\n        for t in cm.keys():\n            for s in cm[t]:\n                if s not in gt[t]: counter += 1\n    else:\n        for t in cm.keys():\n            for s in cm[t]:\n                if s not in gt[t]: counter += 3\n                else:\n                    if gt[t][s][0] != cm[t][s][0]: counter += 1\n                    if gt[t][s][1] != cm[t][s][1]: counter += 1\n                    if gt[t][s][2] != cm[t][s][2]: counter += 1\n    return counter\n</code></pre>"},{"location":"basics/#causalflow.basics.metrics.get_TN","title":"<code>get_TN(gt, min_lag, max_lag, cm, alsoOrient=False)</code>","text":"<p>Compute true negative number: edge absent in the groundtruth and absent in the causal model.</p> <p>Parameters:</p> Name Type Description Default <code>cm</code> <code>dict</code> <p>estimated SCM.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>true negative.</p> Source code in <code>causalflow/basics/metrics.py</code> <pre><code>def get_TN(gt, min_lag, max_lag, cm, alsoOrient = False) -&gt; int:\n\"\"\"\n    Compute true negative number: edge absent in the groundtruth and absent in the causal model.\n\n    Args:\n        cm (dict): estimated SCM.\n\n    Returns:\n        int: true negative.\n    \"\"\"\n    fullg = fully_connected_dag(list(gt.keys()), min_lag, max_lag, alsoOrient)\n    counter = 0\n    if not alsoOrient:\n        gt_TN = deepcopy(fullg)\n\n        # Build the True Negative graph [complementary graph of the ground-truth]\n        for t in fullg:\n            for s in fullg[t]:\n                if s in gt[t]:\n                    gt_TN[t].remove(s)\n\n        for t in gt_TN.keys():\n            for s in gt_TN[t]:\n                if s not in cm[t]: counter += 1\n    else:\n        gt_TN = deepcopy(fullg)\n\n        # Build the True Negative graph [complementary graph of the ground-truth]\n        for t in fullg:\n            for s in fullg[t]:\n                for edge in fullg[t][s]:\n                    if s not in gt[t]: continue\n                    if edge == gt[t][s]:\n                        gt_TN[t][s].remove(edge)\n                if len(fullg[t][s]) == 0: del gt_TN[t][s]\n\n        for t in gt_TN.keys():\n            for s in gt_TN[t]:\n                if s not in cm[t]: \n                    counter += len(gt_TN[t][s])\n                else:\n                    for edge in gt_TN[t][s]:\n                        if edge != cm[t][s]: counter += 1\n\n    return counter\n</code></pre>"},{"location":"basics/#causalflow.basics.metrics.get_TP","title":"<code>get_TP(gt, cm, alsoOrient=False)</code>","text":"<p>Compute true positive number: edge present in the causal model and present in the groundtruth.</p> <p>Parameters:</p> Name Type Description Default <code>cm</code> <code>dict</code> <p>estimated SCM.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>true positive.</p> Source code in <code>causalflow/basics/metrics.py</code> <pre><code>def get_TP(gt, cm, alsoOrient = False) -&gt; int:\n\"\"\"\n    Compute true positive number: edge present in the causal model and present in the groundtruth.\n\n    Args:\n        cm (dict): estimated SCM.\n\n    Returns:\n        int: true positive.\n    \"\"\"\n    counter = 0\n    if not alsoOrient:\n        for t in cm.keys():\n            for s in cm[t]:\n                if s in gt[t]: counter += 1\n    else:\n        for t in cm.keys():\n            for s in cm[t]:\n                if s in gt[t]:\n                    if gt[t][s][0] == cm[t][s][0]: counter += 1\n                    if gt[t][s][1] == cm[t][s][1]: counter += 1\n                    if gt[t][s][2] == cm[t][s][2]: counter += 1\n    return counter\n</code></pre>"},{"location":"basics/#causalflow.basics.metrics.precision","title":"<code>precision(gt, cm, alsoOrient=False)</code>","text":"<p>Compute Precision between ground-truth causal graph and the estimated one.</p> <p>Parameters:</p> Name Type Description Default <code>cm</code> <code>dict</code> <p>estimated SCM.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>precision.</p> Source code in <code>causalflow/basics/metrics.py</code> <pre><code>def precision(gt, cm, alsoOrient = False) -&gt; float:\n\"\"\"\n    Compute Precision between ground-truth causal graph and the estimated one.\n\n    Args:\n        cm (dict): estimated SCM.\n\n    Returns:\n        float: precision.\n    \"\"\"\n    tp = get_TP(gt, cm, alsoOrient)\n    fp = get_FP(gt, cm, alsoOrient)\n    if tp + fp == 0: return 0\n    return tp/(tp + fp)\n</code></pre>"},{"location":"basics/#causalflow.basics.metrics.recall","title":"<code>recall(gt, cm, alsoOrient=False)</code>","text":"<p>Compute Recall between ground-truth causal graph and the estimated one.</p> <p>Parameters:</p> Name Type Description Default <code>cm</code> <code>dict</code> <p>estimated SCM.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>recall.</p> Source code in <code>causalflow/basics/metrics.py</code> <pre><code>def recall(gt, cm, alsoOrient = False) -&gt; float:\n\"\"\"\n    Compute Recall between ground-truth causal graph and the estimated one.\n\n    Args:\n        cm (dict): estimated SCM.\n\n    Returns:\n        float: recall.\n    \"\"\"\n    tp = get_TP(gt, cm, alsoOrient)\n    fn = get_FN(gt, cm, alsoOrient)\n    if tp + fn == 0: return 0\n    return tp/(tp + fn)\n</code></pre>"},{"location":"basics/#causalflow.basics.metrics.shd","title":"<code>shd(gt, cm, alsoOrient=False)</code>","text":"<p>Compute Structural Hamming Distance between ground-truth causal graph and the estimated one.</p> <p>Parameters:</p> Name Type Description Default <code>cm</code> <code>dict</code> <p>estimated SCM.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>shd.</p> Source code in <code>causalflow/basics/metrics.py</code> <pre><code>def shd(gt, cm, alsoOrient = False) -&gt; int:\n\"\"\"\n    Compute Structural Hamming Distance between ground-truth causal graph and the estimated one.\n\n    Args:\n        cm (dict): estimated SCM.\n\n    Returns:\n        int: shd.\n    \"\"\"\n    fn = get_FN(gt, cm, alsoOrient)\n    fp = get_FP(gt, cm, alsoOrient)\n    return fn + fp\n</code></pre>"},{"location":"basics/#causalflow.basics.utils.cls","title":"<code>cls()</code>","text":"<p>Clear terminal.</p> Source code in <code>causalflow/basics/utils.py</code> <pre><code>def cls():\n\"\"\"Clear terminal.\"\"\"\n    os.system('cls' if os.name == 'nt' else 'clear')\n</code></pre>"},{"location":"basics/#causalflow.basics.utils.create_results_folder","title":"<code>create_results_folder(resfolder)</code>","text":"<p>Create results folder if doesn't exist.</p> Source code in <code>causalflow/basics/utils.py</code> <pre><code>def create_results_folder(resfolder):\n\"\"\"Create results folder if doesn't exist.\"\"\"\n    Path(resfolder).mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"basics/#causalflow.basics.utils.get_selectorpath","title":"<code>get_selectorpath(resfolder)</code>","text":"<p>Return log file path.</p> <p>Parameters:</p> Name Type Description Default <code>resfolder</code> <code>str</code> <p>result folder.</p> required <p>Returns:</p> Type Description <code>str</code> <p>log file path.</p> Source code in <code>causalflow/basics/utils.py</code> <pre><code>def get_selectorpath(resfolder):\n\"\"\"\n    Return log file path.\n\n    Args:\n        resfolder (str): result folder.\n\n    Returns:\n        (str): log file path.\n    \"\"\"\n    Path(resfolder).mkdir(parents=True, exist_ok=True)\n    return SEP.join([resfolder, LOG_FILENAME]), SEP.join([resfolder, RES_FILENAME]), SEP.join([resfolder, DAG_FILENAME]), SEP.join([resfolder, TSDAG_FILENAME])\n</code></pre>"},{"location":"basics/#causalflow.basics.utils.remove_from_list","title":"<code>remove_from_list(list, item)</code>","text":"<p>Create a copy of a list and remove from it an item.</p> Source code in <code>causalflow/basics/utils.py</code> <pre><code>def remove_from_list(list: list(), item) -&gt; list():\n\"\"\"Create a copy of a list and remove from it an item.\"\"\"\n    tmp = copy.deepcopy(list)\n    tmp.remove(item)\n    return tmp\n</code></pre>"},{"location":"causal_discovery/","title":"Causal Discovery","text":"<p>This module provides the CausalDiscoveryMethod class.</p> Classes <p>CausalDiscoveryMethod: abstract class used by all the causal discovery algorithms.</p> <p>This module provides the DYNOTEARS class.</p> Classes <p>DYNOTEARS: class containing the DYNOTEARS causal discovery algorithm.</p> <p>This module provides the LPCMCI class.</p> Classes <p>LPCMCI: class containing the LPCMCI causal discovery algorithm.</p> <p>This module provides the PCMCI class.</p> Classes <p>PCMCI: class containing the PCMCI causal discovery algorithm.</p> <p>This module provides the PCMCI+ class.</p> Classes <p>PCMCIplus: class containing the PCMCI+ causal discovery algorithm.</p> <p>This module provides the TCDF class.</p> Classes <p>TCDF: class containing the TCDF causal discovery algorithm.</p> <p>This module provides the tsFCI class.</p> Classes <p>tsFCI: class containing the tsFCI causal discovery algorithm.</p> <p>This module provides the VarLiNGAM class.</p> Classes <p>VarLiNGAM: class containing the VarLiNGAM causal discovery algorithm.</p>"},{"location":"causal_discovery/#causalflow.causal_discovery.CausalDiscoveryMethod.CausalDiscoveryMethod","title":"<code>CausalDiscoveryMethod</code>","text":"<p>             Bases: <code>ABC</code></p> <p>CausalDiscoveryMethod class.</p> <p>CausalDiscoveryMethod is an abstract causal discovery method for  large-scale time series datasets.</p> Source code in <code>causalflow/causal_discovery/CausalDiscoveryMethod.py</code> <pre><code>class CausalDiscoveryMethod(ABC):\n\"\"\"\n    CausalDiscoveryMethod class.\n\n    CausalDiscoveryMethod is an abstract causal discovery method for \n    large-scale time series datasets.\n    \"\"\"\n\n    def __init__(self, \n                 data: Data, \n                 min_lag, max_lag, \n                 verbosity: CPLevel, \n                 alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class contructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n\n        \"\"\"\n        self.data = data\n        self.alpha = alpha\n        self.min_lag = min_lag\n        self.max_lag = max_lag\n        self.CM = DAG(self.data.features, min_lag, max_lag, neglect_only_autodep)\n        self.neglect_only_autodep = neglect_only_autodep\n\n        self.resfolder = resfolder\n        self.respath, self.dag_path, self.ts_dag_path = None, None, None\n        if resfolder is not None:\n            logpath, self.respath, self.dag_path, self.ts_dag_path = utils.get_selectorpath(resfolder)  \n            self.logger = Logger(logpath, clean_cls)\n            sys.stdout = self.logger\n\n        CP.set_verbosity(verbosity)\n\n\n    @abstractmethod\n    def run(self) -&gt; DAG:\n\"\"\"\n        Run causal discovery method.\n\n        Returns:\n            DAG: causal model.\n        \"\"\"\n        pass\n\n\n    def load(self, res_path):\n\"\"\"\n        Load previously estimated result .\n\n        Args:\n            res_path (str): pickle file path.\n        \"\"\"\n        with open(res_path, 'rb') as f:\n            r = pickle.load(f)\n            self.CM = r['causal_model']\n            self.alpha = r['alpha']\n            self.dag_path = r['dag_path']\n            self.ts_dag_path = r['ts_dag_path']\n\n\n    def save(self):\n\"\"\"Save causal discovery result as pickle file if resfolder is set.\"\"\"\n        if self.respath is not None:\n            if self.CM:\n                res = dict()\n                res['causal_model'] = copy.deepcopy(self.CM)\n                res['alpha'] = self.alpha\n                res['dag_path'] = self.dag_path\n                res['ts_dag_path'] = self.ts_dag_path\n                with open(self.respath, 'wb') as resfile:\n                    pickle.dump(res, resfile)\n            else:\n                CP.warning(\"Causal model impossible to save\")\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.CausalDiscoveryMethod.CausalDiscoveryMethod.__init__","title":"<code>__init__(data, min_lag, max_lag, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class contructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/CausalDiscoveryMethod.py</code> <pre><code>def __init__(self, \n             data: Data, \n             min_lag, max_lag, \n             verbosity: CPLevel, \n             alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class contructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n\n    \"\"\"\n    self.data = data\n    self.alpha = alpha\n    self.min_lag = min_lag\n    self.max_lag = max_lag\n    self.CM = DAG(self.data.features, min_lag, max_lag, neglect_only_autodep)\n    self.neglect_only_autodep = neglect_only_autodep\n\n    self.resfolder = resfolder\n    self.respath, self.dag_path, self.ts_dag_path = None, None, None\n    if resfolder is not None:\n        logpath, self.respath, self.dag_path, self.ts_dag_path = utils.get_selectorpath(resfolder)  \n        self.logger = Logger(logpath, clean_cls)\n        sys.stdout = self.logger\n\n    CP.set_verbosity(verbosity)\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.CausalDiscoveryMethod.CausalDiscoveryMethod.load","title":"<code>load(res_path)</code>","text":"<p>Load previously estimated result .</p> <p>Parameters:</p> Name Type Description Default <code>res_path</code> <code>str</code> <p>pickle file path.</p> required Source code in <code>causalflow/causal_discovery/CausalDiscoveryMethod.py</code> <pre><code>def load(self, res_path):\n\"\"\"\n    Load previously estimated result .\n\n    Args:\n        res_path (str): pickle file path.\n    \"\"\"\n    with open(res_path, 'rb') as f:\n        r = pickle.load(f)\n        self.CM = r['causal_model']\n        self.alpha = r['alpha']\n        self.dag_path = r['dag_path']\n        self.ts_dag_path = r['ts_dag_path']\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.CausalDiscoveryMethod.CausalDiscoveryMethod.run","title":"<code>run()</code>  <code>abstractmethod</code>","text":"<p>Run causal discovery method.</p> <p>Returns:</p> Name Type Description <code>DAG</code> <code>DAG</code> <p>causal model.</p> Source code in <code>causalflow/causal_discovery/CausalDiscoveryMethod.py</code> <pre><code>@abstractmethod\ndef run(self) -&gt; DAG:\n\"\"\"\n    Run causal discovery method.\n\n    Returns:\n        DAG: causal model.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.CausalDiscoveryMethod.CausalDiscoveryMethod.save","title":"<code>save()</code>","text":"<p>Save causal discovery result as pickle file if resfolder is set.</p> Source code in <code>causalflow/causal_discovery/CausalDiscoveryMethod.py</code> <pre><code>def save(self):\n\"\"\"Save causal discovery result as pickle file if resfolder is set.\"\"\"\n    if self.respath is not None:\n        if self.CM:\n            res = dict()\n            res['causal_model'] = copy.deepcopy(self.CM)\n            res['alpha'] = self.alpha\n            res['dag_path'] = self.dag_path\n            res['ts_dag_path'] = self.ts_dag_path\n            with open(self.respath, 'wb') as resfile:\n                pickle.dump(res, resfile)\n        else:\n            CP.warning(\"Causal model impossible to save\")\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.DYNOTEARS.DYNOTEARS","title":"<code>DYNOTEARS</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>DYNOTEARS causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/DYNOTEARS.py</code> <pre><code>class DYNOTEARS(CausalDiscoveryMethod):\n\"\"\"DYNOTEARS causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data, \n                 min_lag,\n                 max_lag, \n                 verbosity, \n                 alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n    def run(self) -&gt; DAG:\n\"\"\"\n        Run DYNOTEARS algorithm.\n\n        Returns:\n            DAG: causal discovery result.\n        \"\"\"\n        graph_dict = dict()\n        for name in self.data.features:\n            graph_dict[name] = []\n        sm = from_pandas_dynamic(self.data.d, p=self.max_lag)\n\n        tname_to_name_dict = dict()\n        count_lag = 0\n        idx_name = 0\n        for tname in sm.nodes:\n            tname_to_name_dict[tname] = self.data.features[idx_name]\n            if count_lag == self.max_lag:\n                idx_name = idx_name +1\n                count_lag = -1\n            count_lag = count_lag +1\n\n        for ce in sm.edges:\n            c = ce[0]\n            e = ce[1]\n            w = sm.adj[c][e][\"weight\"]\n            tc = int(c.partition(\"lag\")[2])\n            te = int(e.partition(\"lag\")[2])\n            t = tc - te\n            if (tname_to_name_dict[c], -t) not in graph_dict[tname_to_name_dict[e]]:\n                graph_dict[tname_to_name_dict[e]].append((tname_to_name_dict[c], w, -t))\n\n        self.CM = self._to_DAG(graph_dict)\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n\n\n    def _to_DAG(self, graph):\n\"\"\"\n        Re-elaborate the result in a DAG.\n\n        Returns:\n            (DAG): result re-elaborated.\n        \"\"\"\n        tmp_dag = DAG(self.data.features, self.min_lag, self.max_lag, self.neglect_only_autodep)\n        tmp_dag.sys_context = dict()\n        for t in graph.keys():\n            for s in graph[t]:\n                lag = abs(s[2])\n                if lag &gt;= self.min_lag and lag &lt;= self.max_lag:\n                    tmp_dag.add_source(t, s[0], abs(s[1]), 0, s[2])\n        return tmp_dag\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.DYNOTEARS.DYNOTEARS.__init__","title":"<code>__init__(data, min_lag, max_lag, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/DYNOTEARS.py</code> <pre><code>def __init__(self, \n             data, \n             min_lag,\n             max_lag, \n             verbosity, \n             alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.DYNOTEARS.DYNOTEARS.run","title":"<code>run()</code>","text":"<p>Run DYNOTEARS algorithm.</p> <p>Returns:</p> Name Type Description <code>DAG</code> <code>DAG</code> <p>causal discovery result.</p> Source code in <code>causalflow/causal_discovery/baseline/DYNOTEARS.py</code> <pre><code>def run(self) -&gt; DAG:\n\"\"\"\n    Run DYNOTEARS algorithm.\n\n    Returns:\n        DAG: causal discovery result.\n    \"\"\"\n    graph_dict = dict()\n    for name in self.data.features:\n        graph_dict[name] = []\n    sm = from_pandas_dynamic(self.data.d, p=self.max_lag)\n\n    tname_to_name_dict = dict()\n    count_lag = 0\n    idx_name = 0\n    for tname in sm.nodes:\n        tname_to_name_dict[tname] = self.data.features[idx_name]\n        if count_lag == self.max_lag:\n            idx_name = idx_name +1\n            count_lag = -1\n        count_lag = count_lag +1\n\n    for ce in sm.edges:\n        c = ce[0]\n        e = ce[1]\n        w = sm.adj[c][e][\"weight\"]\n        tc = int(c.partition(\"lag\")[2])\n        te = int(e.partition(\"lag\")[2])\n        t = tc - te\n        if (tname_to_name_dict[c], -t) not in graph_dict[tname_to_name_dict[e]]:\n            graph_dict[tname_to_name_dict[e]].append((tname_to_name_dict[c], w, -t))\n\n    self.CM = self._to_DAG(graph_dict)\n\n    if self.resfolder is not None: self.logger.close()\n    return self.CM\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.LPCMCI.LPCMCI","title":"<code>LPCMCI</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>LPCMCI causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/LPCMCI.py</code> <pre><code>class LPCMCI(CausalDiscoveryMethod):\n\"\"\"LPCMCI causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data: Data,\n                 min_lag, max_lag, \n                 val_condtest: CondIndTest, \n                 verbosity: CPLevel,\n                 alpha = 0.05, \n                 resfolder = None, \n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            val_condtest (CondIndTest): validation method.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n        # build tigramite dataset\n        vector = np.vectorize(float)\n        d = vector(data.d)\n\n        # init pcmci\n        self.lpcmci = lpcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                             cond_ind_test = val_condtest,\n                             verbosity = verbosity.value)\n\n\n    def run(self, link_assumptions = None) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        CP.info('\\n')\n        CP.info(DASH)\n        CP.info(\"Running Causal Discovery Algorithm\")\n        self.result = self.lpcmci.run_lpcmci(link_assumptions = link_assumptions,\n                                             tau_max = self.max_lag,\n                                             tau_min = self.min_lag,\n                                             pc_alpha = self.alpha)\n\n        self.CM = self._to_DAG()\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n\n\n    def _to_DAG(self):\n\"\"\"\n        Re-elaborate the PCMCI result in a new dictionary.\n\n        Returns:\n            (DAG): lpcmci result re-elaborated.\n        \"\"\"\n        vars = self.data.features\n        tmp_dag = DAG(vars, self.min_lag, self.max_lag)\n        tmp_dag.sys_context = dict()\n        N, lags = self.result['graph'][0].shape\n        for s in range(len(self.result['graph'])):\n            for t in range(N):\n                for lag in range(lags):\n                    if self.result['graph'][s][t,lag] != '':\n                        arrowtype = self.result['graph'][s][t,lag]\n\n                        if arrowtype == LinkType.Bidirected.value:\n                            if ((vars[s], abs(lag)) in tmp_dag.g[vars[t]].sources and \n                                tmp_dag.g[t].sources[(vars[s], abs(lag))][TYPE] == LinkType.Bidirected.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif arrowtype == LinkType.Uncertain.value:\n                            if ((vars[t], abs(lag)) in tmp_dag.g[vars[s]].sources and \n                                tmp_dag.g[vars[s]].sources[(vars[t], abs(lag))][TYPE] == LinkType.Uncertain.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif (arrowtype == LinkType.Directed.value or\n                              arrowtype == LinkType.HalfUncertain.value):\n                            tmp_dag.add_source(vars[t], \n                                            vars[s],\n                                            self.result['val_matrix'][s][t,lag],\n                                            self.result['p_matrix'][s][t,lag],\n                                            lag,\n                                            arrowtype)\n        return tmp_dag\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.LPCMCI.LPCMCI.__init__","title":"<code>__init__(data, min_lag, max_lag, val_condtest, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>val_condtest</code> <code>CondIndTest</code> <p>validation method.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/LPCMCI.py</code> <pre><code>def __init__(self, \n             data: Data,\n             min_lag, max_lag, \n             val_condtest: CondIndTest, \n             verbosity: CPLevel,\n             alpha = 0.05, \n             resfolder = None, \n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        val_condtest (CondIndTest): validation method.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n    # build tigramite dataset\n    vector = np.vectorize(float)\n    d = vector(data.d)\n\n    # init pcmci\n    self.lpcmci = lpcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                         cond_ind_test = val_condtest,\n                         verbosity = verbosity.value)\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.LPCMCI.LPCMCI.run","title":"<code>run(link_assumptions=None)</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/LPCMCI.py</code> <pre><code>def run(self, link_assumptions = None) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    CP.info('\\n')\n    CP.info(DASH)\n    CP.info(\"Running Causal Discovery Algorithm\")\n    self.result = self.lpcmci.run_lpcmci(link_assumptions = link_assumptions,\n                                         tau_max = self.max_lag,\n                                         tau_min = self.min_lag,\n                                         pc_alpha = self.alpha)\n\n    self.CM = self._to_DAG()\n\n    if self.resfolder is not None: self.logger.close()\n    return self.CM\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.PCMCI.PCMCI","title":"<code>PCMCI</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>PCMCI causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/PCMCI.py</code> <pre><code>class PCMCI(CausalDiscoveryMethod):\n\"\"\"PCMCI causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data: Data, \n                 min_lag, max_lag, \n                 val_condtest: CondIndTest, \n                 verbosity: CPLevel,\n                 pc_alpha = 0.05, \n                 alpha = 0.05, \n                 resfolder = None, \n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            val_condtest (CondIndTest): validation method.\n            verbosity (CPLevel): verbosity level.\n            pc_alpha (float, optional): PC significance level. Defaults to 0.05.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n        self.pc_alpha = pc_alpha\n\n        # build tigramite dataset\n        vector = np.vectorize(float)\n        d = vector(data.d)\n\n        # init pcmci\n        self.pcmci = pcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                           cond_ind_test = val_condtest,\n                           verbosity = verbosity.value)\n\n\n    def run(self) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        CP.info('\\n')\n        CP.info(DASH)\n        CP.info(\"Running Causal Discovery Algorithm\")\n\n        self.result = self.pcmci.run_pcmci(tau_max = self.max_lag,\n                                           tau_min = self.min_lag,\n                                           alpha_level = self.alpha,\n                                           pc_alpha = self.pc_alpha)\n\n        self.CM = self._to_DAG()\n\n        if self.resfolder is not None: self.logger.close()        \n        return self.CM\n\n\n    def _to_DAG(self):\n\"\"\"\n        Re-elaborates the PCMCI result in a new dictionary.\n\n        Returns:\n            (DAG): pcmci result re-elaborated.\n        \"\"\"\n        vars = self.data.features\n        tmp_dag = DAG(vars, self.min_lag, self.max_lag)\n        tmp_dag.sys_context = dict()\n        N, lags = self.result['graph'][0].shape\n        for s in range(len(self.result['graph'])):\n            for t in range(N):\n                for lag in range(lags):\n                    if self.result['graph'][s][t,lag] != '':\n                        arrowtype = self.result['graph'][s][t,lag]\n\n                        if arrowtype == LinkType.Bidirected.value:\n                            if ((vars[s], abs(lag)) in tmp_dag.g[vars[t]].sources and \n                                tmp_dag.g[t].sources[(vars[s], abs(lag))][TYPE] == LinkType.Bidirected.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif arrowtype == LinkType.Uncertain.value:\n                            if ((vars[t], abs(lag)) in tmp_dag.g[vars[s]].sources and \n                                tmp_dag.g[vars[s]].sources[(vars[t], abs(lag))][TYPE] == LinkType.Uncertain.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif (arrowtype == LinkType.Directed.value or\n                              arrowtype == LinkType.HalfUncertain.value):\n                            tmp_dag.add_source(vars[t], \n                                            vars[s],\n                                            self.result['val_matrix'][s][t,lag],\n                                            self.result['p_matrix'][s][t,lag],\n                                            lag,\n                                            arrowtype)\n        return tmp_dag\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.PCMCI.PCMCI.__init__","title":"<code>__init__(data, min_lag, max_lag, val_condtest, verbosity, pc_alpha=0.05, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>val_condtest</code> <code>CondIndTest</code> <p>validation method.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>pc_alpha</code> <code>float</code> <p>PC significance level. Defaults to 0.05.</p> <code>0.05</code> <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/PCMCI.py</code> <pre><code>def __init__(self, \n             data: Data, \n             min_lag, max_lag, \n             val_condtest: CondIndTest, \n             verbosity: CPLevel,\n             pc_alpha = 0.05, \n             alpha = 0.05, \n             resfolder = None, \n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        val_condtest (CondIndTest): validation method.\n        verbosity (CPLevel): verbosity level.\n        pc_alpha (float, optional): PC significance level. Defaults to 0.05.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n    self.pc_alpha = pc_alpha\n\n    # build tigramite dataset\n    vector = np.vectorize(float)\n    d = vector(data.d)\n\n    # init pcmci\n    self.pcmci = pcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                       cond_ind_test = val_condtest,\n                       verbosity = verbosity.value)\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.PCMCI.PCMCI.run","title":"<code>run()</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/PCMCI.py</code> <pre><code>def run(self) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    CP.info('\\n')\n    CP.info(DASH)\n    CP.info(\"Running Causal Discovery Algorithm\")\n\n    self.result = self.pcmci.run_pcmci(tau_max = self.max_lag,\n                                       tau_min = self.min_lag,\n                                       alpha_level = self.alpha,\n                                       pc_alpha = self.pc_alpha)\n\n    self.CM = self._to_DAG()\n\n    if self.resfolder is not None: self.logger.close()        \n    return self.CM\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.PCMCIplus.PCMCIplus","title":"<code>PCMCIplus</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>PCMCI+ causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/PCMCIplus.py</code> <pre><code>class PCMCIplus(CausalDiscoveryMethod):\n\"\"\"PCMCI+ causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data: Data, \n                 min_lag, max_lag, \n                 val_condtest: CondIndTest, \n                 verbosity: CPLevel,\n                 alpha = 0.05, \n                 resfolder = None, \n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            val_condtest (CondIndTest): validation method.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n        # build tigramite dataset\n        vector = np.vectorize(float)\n        d = vector(data.d)\n\n        # init pcmci\n        self.pcmci = pcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                           cond_ind_test = val_condtest,\n                           verbosity = verbosity.value)\n\n\n    def run(self, link_assumptions=None) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        CP.info('\\n')\n        CP.info(DASH)\n        CP.info(\"Running Causal Discovery Algorithm\")\n\n        self.result = self.pcmci.run_pcmciplus(link_assumptions=link_assumptions,\n                                               tau_max = self.max_lag,\n                                               tau_min = 0,\n                                               pc_alpha = self.alpha)\n\n        self.CM = self._to_DAG()\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n\n\n    def _to_DAG(self):\n\"\"\"\n        Re-elaborates the PCMCI result in a new dictionary.\n\n        Returns:\n            (DAG): pcmci result re-elaborated.\n        \"\"\"\n        vars = self.data.features\n        tmp_dag = DAG(vars, self.min_lag, self.max_lag)\n        tmp_dag.sys_context = dict()\n        N, lags = self.result['graph'][0].shape\n        for s in range(len(self.result['graph'])):\n            for t in range(N):\n                for lag in range(lags):\n                    if self.result['graph'][s][t,lag] != '':\n                        arrowtype = self.result['graph'][s][t,lag]\n\n                        if arrowtype == LinkType.Bidirected.value:\n                            if ((vars[s], abs(lag)) in tmp_dag.g[vars[t]].sources and \n                                tmp_dag.g[t].sources[(vars[s], abs(lag))][TYPE] == LinkType.Bidirected.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif arrowtype == LinkType.Uncertain.value:\n                            if ((vars[t], abs(lag)) in tmp_dag.g[vars[s]].sources and \n                                tmp_dag.g[vars[s]].sources[(vars[t], abs(lag))][TYPE] == LinkType.Uncertain.value):\n                                continue\n                            else:\n                                tmp_dag.add_source(vars[t], \n                                                vars[s],\n                                                self.result['val_matrix'][s][t,lag],\n                                                self.result['p_matrix'][s][t,lag],\n                                                lag,\n                                                arrowtype)\n\n\n                        elif (arrowtype == LinkType.Directed.value or\n                              arrowtype == LinkType.HalfUncertain.value):\n                            tmp_dag.add_source(vars[t], \n                                            vars[s],\n                                            self.result['val_matrix'][s][t,lag],\n                                            self.result['p_matrix'][s][t,lag],\n                                            lag,\n                                            arrowtype)\n        return tmp_dag\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.PCMCIplus.PCMCIplus.__init__","title":"<code>__init__(data, min_lag, max_lag, val_condtest, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>val_condtest</code> <code>CondIndTest</code> <p>validation method.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/PCMCIplus.py</code> <pre><code>def __init__(self, \n             data: Data, \n             min_lag, max_lag, \n             val_condtest: CondIndTest, \n             verbosity: CPLevel,\n             alpha = 0.05, \n             resfolder = None, \n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        val_condtest (CondIndTest): validation method.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n    # build tigramite dataset\n    vector = np.vectorize(float)\n    d = vector(data.d)\n\n    # init pcmci\n    self.pcmci = pcmci(dataframe = pp.DataFrame(data = d, var_names = data.features),\n                       cond_ind_test = val_condtest,\n                       verbosity = verbosity.value)\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.PCMCIplus.PCMCIplus.run","title":"<code>run(link_assumptions=None)</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/PCMCIplus.py</code> <pre><code>def run(self, link_assumptions=None) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    CP.info('\\n')\n    CP.info(DASH)\n    CP.info(\"Running Causal Discovery Algorithm\")\n\n    self.result = self.pcmci.run_pcmciplus(link_assumptions=link_assumptions,\n                                           tau_max = self.max_lag,\n                                           tau_min = 0,\n                                           pc_alpha = self.alpha)\n\n    self.CM = self._to_DAG()\n\n    if self.resfolder is not None: self.logger.close()\n    return self.CM\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.TCDF.TCDF","title":"<code>TCDF</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>TCDF causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/TCDF.py</code> <pre><code>class TCDF(CausalDiscoveryMethod):\n\"\"\"TCDF causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data, \n                 min_lag,\n                 max_lag, \n                 verbosity, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            verbosity (CPLevel): verbosity level.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, resfolder=resfolder, neglect_only_autodep=neglect_only_autodep, clean_cls=clean_cls)\n\n\n    def run(self, \n            epochs=1000,  \n            kernel_size=4, \n            dilation_coefficient=4, \n            hidden_layers=0, \n            learning_rate=0.01,\n            cuda=False) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        # Remove all arguments from directory\n        dir_path = os.path.dirname(os.path.realpath(__file__))\n        Path(dir_path+\"/args\").mkdir(exist_ok=True)\n        Path(dir_path+\"/results\").mkdir(exist_ok=True)\n        script = dir_path + \"/pkgs/TCDF-master/runTCDF\" + \".py\"\n        r_arg_list = []\n        r_arg_list.append(\"--epochs\")\n        r_arg_list.append(str(epochs))\n        r_arg_list.append(\"--kernel_size\")\n        r_arg_list.append(str(kernel_size))\n        r_arg_list.append(\"--dilation_coefficient\")\n        r_arg_list.append(str(dilation_coefficient))\n        r_arg_list.append(\"--hidden_layers\")\n        r_arg_list.append(str(hidden_layers))\n        r_arg_list.append(\"--learning_rate\")\n        r_arg_list.append(str(learning_rate))\n        r_arg_list.append(\"--significance\")\n        r_arg_list.append(str(0.8))\n        self.data.d.to_csv(dir_path + \"/args/data.csv\", index=False)\n        r_arg_list.append(\"--data\")\n        r_arg_list.append(dir_path + \"/args/data.csv\")            \n        if cuda: r_arg_list.append(\"--cuda\")\n        r_arg_list.append(\"--path\")\n        r_arg_list.append(dir_path)\n\n        cmd = [\"python\", script] + r_arg_list\n        p = Popen(cmd, cwd=\"./\", stdin=PIPE, stdout=PIPE, stderr=PIPE)\n\n        # Return R output or error\n        output, error = p.communicate()\n        CP.info(output.decode('utf-8'))\n        if p.returncode == 0:\n            g_dict = json.load(open(dir_path + \"/results/tcdf_result.txt\"))\n            for key in g_dict.keys():\n                key_list = []\n                for elem in g_dict[key]:\n                    key_list.append(tuple(elem))\n                g_dict[key] = key_list\n            utils.clean(dir_path)\n            self.CM = self._to_DAG(g_dict)\n\n            if self.resfolder is not None: self.logger.close()\n            return self.CM\n        else:\n            utils.clean(dir_path)\n            CP.warning('Python Error:\\n {0}'.format(error))\n            exit(0)\n\n\n    def _to_DAG(self, graph):\n\"\"\"\n        Re-elaborate the result in a DAG.\n\n        Returns:\n            (DAG): result re-elaborated.\n        \"\"\"\n        tmp_dag = DAG(self.data.features, self.min_lag, self.max_lag, self.neglect_only_autodep)\n        tmp_dag.sys_context = dict()\n        for t in graph.keys():\n            for s in graph[t]:\n                lag = abs(s[1])\n                if lag &gt;= self.min_lag and lag &lt;= self.max_lag:\n                    tmp_dag.add_source(t, s[0], utils.DSCORE, 0, s[1])\n        return tmp_dag\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.TCDF.TCDF.__init__","title":"<code>__init__(data, min_lag, max_lag, verbosity, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/TCDF.py</code> <pre><code>def __init__(self, \n             data, \n             min_lag,\n             max_lag, \n             verbosity, \n             resfolder = None,\n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        verbosity (CPLevel): verbosity level.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, resfolder=resfolder, neglect_only_autodep=neglect_only_autodep, clean_cls=clean_cls)\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.TCDF.TCDF.run","title":"<code>run(epochs=1000, kernel_size=4, dilation_coefficient=4, hidden_layers=0, learning_rate=0.01, cuda=False)</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/TCDF.py</code> <pre><code>def run(self, \n        epochs=1000,  \n        kernel_size=4, \n        dilation_coefficient=4, \n        hidden_layers=0, \n        learning_rate=0.01,\n        cuda=False) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    # Remove all arguments from directory\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    Path(dir_path+\"/args\").mkdir(exist_ok=True)\n    Path(dir_path+\"/results\").mkdir(exist_ok=True)\n    script = dir_path + \"/pkgs/TCDF-master/runTCDF\" + \".py\"\n    r_arg_list = []\n    r_arg_list.append(\"--epochs\")\n    r_arg_list.append(str(epochs))\n    r_arg_list.append(\"--kernel_size\")\n    r_arg_list.append(str(kernel_size))\n    r_arg_list.append(\"--dilation_coefficient\")\n    r_arg_list.append(str(dilation_coefficient))\n    r_arg_list.append(\"--hidden_layers\")\n    r_arg_list.append(str(hidden_layers))\n    r_arg_list.append(\"--learning_rate\")\n    r_arg_list.append(str(learning_rate))\n    r_arg_list.append(\"--significance\")\n    r_arg_list.append(str(0.8))\n    self.data.d.to_csv(dir_path + \"/args/data.csv\", index=False)\n    r_arg_list.append(\"--data\")\n    r_arg_list.append(dir_path + \"/args/data.csv\")            \n    if cuda: r_arg_list.append(\"--cuda\")\n    r_arg_list.append(\"--path\")\n    r_arg_list.append(dir_path)\n\n    cmd = [\"python\", script] + r_arg_list\n    p = Popen(cmd, cwd=\"./\", stdin=PIPE, stdout=PIPE, stderr=PIPE)\n\n    # Return R output or error\n    output, error = p.communicate()\n    CP.info(output.decode('utf-8'))\n    if p.returncode == 0:\n        g_dict = json.load(open(dir_path + \"/results/tcdf_result.txt\"))\n        for key in g_dict.keys():\n            key_list = []\n            for elem in g_dict[key]:\n                key_list.append(tuple(elem))\n            g_dict[key] = key_list\n        utils.clean(dir_path)\n        self.CM = self._to_DAG(g_dict)\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n    else:\n        utils.clean(dir_path)\n        CP.warning('Python Error:\\n {0}'.format(error))\n        exit(0)\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.tsFCI.tsFCI","title":"<code>tsFCI</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>tsFCI causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/tsFCI.py</code> <pre><code>class tsFCI(CausalDiscoveryMethod):\n\"\"\"tsFCI causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data, \n                 min_lag,\n                 max_lag, \n                 verbosity, \n                 alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n\n    def run(self) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        # Remove all arguments from directory\n        dir_path = os.path.dirname(os.path.realpath(__file__))\n        Path(dir_path + \"/args\").mkdir(exist_ok=True)\n        Path(dir_path + \"/results\").mkdir(exist_ok=True)\n\n        script = dir_path + \"/pkgs/tsfci.R\"\n        r_arg_list = []\n\n        # COMMAND WITH ARGUMENTS\n        self.data.d.to_csv(dir_path + \"/args/data.csv\", index=False)\n        r_arg_list.append(dir_path + \"/args/data.csv\")\n        r_arg_list.append(str(self.alpha))\n        r_arg_list.append(str(self.max_lag))\n\n        r_arg_list.append(dir_path)\n        cmd = [\"Rscript\", script] + r_arg_list\n\n        p = Popen(cmd, cwd=\"./\", stdin=PIPE, stdout=PIPE, stderr=PIPE)\n\n        # Return R output or error\n        output, error = p.communicate()\n        print(output.decode('utf-8'))\n        if p.returncode == 0:\n            g_df = pd.read_csv(dir_path + \"/results/result.csv\", header=0, index_col=0)\n            g_dict = self.ts_fci_dataframe_to_dict(g_df, self.data.features, self.max_lag)\n            self.CM = self._to_DAG(g_dict)\n            utils.clean(dir_path)\n\n            if self.resfolder is not None: self.logger.close()\n            return self.CM\n\n        else:\n            utils.clean(dir_path)\n            print('R Error:\\n {0}'.format(error.decode('utf-8')))\n            exit(0)\n\n\n    def _to_DAG(self, graph):\n\"\"\"\n        Re-elaborate the result in a DAG.\n\n        Returns:\n            (DAG): result re-elaborated.\n        \"\"\"\n        tmp_dag = DAG(self.data.features, self.min_lag, self.max_lag, self.neglect_only_autodep)\n        tmp_dag.sys_context = dict()\n        for t in graph.keys():\n            for s in graph[t]:\n                lag = abs(s[1])\n                if lag &gt;= self.min_lag and lag &lt;= self.max_lag:\n                    tmp_dag.add_source(t, s[0], utils.DSCORE, 0, s[1])\n        return tmp_dag\n\n\n    def ts_fci_dataframe_to_dict(self, df, names, nlags) -&gt; dict:\n\"\"\"\n        Convert tsFCI result into a dict for _to_DAG.\n\n        Args:\n            df (DataFrame): graph.\n            names (list[str]): variables' name.\n            nlags (int): max time lag.\n\n        Returns:\n            dict: dict graph.\n        \"\"\"\n        # todo: check if its correct\n        for i in range(df.shape[1]):\n            for j in range(i+1, df.shape[1]):\n                if df[df.columns[i]].loc[df.columns[j]] == 2:\n                    if df[df.columns[j]].loc[df.columns[i]] == 2:\n                        print(df.columns[i] + \" &lt;-&gt; \" + df.columns[j])\n\n        g_dict = dict()\n        for name_y in names:\n            g_dict[name_y] = []\n        for ty in range(nlags):\n            for name_y in names:\n                t_name_y = df.columns[ty*len(names)+names.index(name_y)]\n                for tx in range(nlags):\n                    for name_x in names:\n                        t_name_x = df.columns[tx * len(names) + names.index(name_x)]\n                        if df[t_name_y].loc[t_name_x] == 2:\n                            if (name_x, tx-ty) not in g_dict[name_y]:\n                                g_dict[name_y].append((name_x, tx - ty))\n        print(g_dict)\n        return g_dict\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.tsFCI.tsFCI.__init__","title":"<code>__init__(data, min_lag, max_lag, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/tsFCI.py</code> <pre><code>def __init__(self, \n             data, \n             min_lag,\n             max_lag, \n             verbosity, \n             alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.tsFCI.tsFCI.run","title":"<code>run()</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/tsFCI.py</code> <pre><code>def run(self) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    # Remove all arguments from directory\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    Path(dir_path + \"/args\").mkdir(exist_ok=True)\n    Path(dir_path + \"/results\").mkdir(exist_ok=True)\n\n    script = dir_path + \"/pkgs/tsfci.R\"\n    r_arg_list = []\n\n    # COMMAND WITH ARGUMENTS\n    self.data.d.to_csv(dir_path + \"/args/data.csv\", index=False)\n    r_arg_list.append(dir_path + \"/args/data.csv\")\n    r_arg_list.append(str(self.alpha))\n    r_arg_list.append(str(self.max_lag))\n\n    r_arg_list.append(dir_path)\n    cmd = [\"Rscript\", script] + r_arg_list\n\n    p = Popen(cmd, cwd=\"./\", stdin=PIPE, stdout=PIPE, stderr=PIPE)\n\n    # Return R output or error\n    output, error = p.communicate()\n    print(output.decode('utf-8'))\n    if p.returncode == 0:\n        g_df = pd.read_csv(dir_path + \"/results/result.csv\", header=0, index_col=0)\n        g_dict = self.ts_fci_dataframe_to_dict(g_df, self.data.features, self.max_lag)\n        self.CM = self._to_DAG(g_dict)\n        utils.clean(dir_path)\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n\n    else:\n        utils.clean(dir_path)\n        print('R Error:\\n {0}'.format(error.decode('utf-8')))\n        exit(0)\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.tsFCI.tsFCI.ts_fci_dataframe_to_dict","title":"<code>ts_fci_dataframe_to_dict(df, names, nlags)</code>","text":"<p>Convert tsFCI result into a dict for _to_DAG.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>graph.</p> required <code>names</code> <code>list[str]</code> <p>variables' name.</p> required <code>nlags</code> <code>int</code> <p>max time lag.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>dict graph.</p> Source code in <code>causalflow/causal_discovery/baseline/tsFCI.py</code> <pre><code>def ts_fci_dataframe_to_dict(self, df, names, nlags) -&gt; dict:\n\"\"\"\n    Convert tsFCI result into a dict for _to_DAG.\n\n    Args:\n        df (DataFrame): graph.\n        names (list[str]): variables' name.\n        nlags (int): max time lag.\n\n    Returns:\n        dict: dict graph.\n    \"\"\"\n    # todo: check if its correct\n    for i in range(df.shape[1]):\n        for j in range(i+1, df.shape[1]):\n            if df[df.columns[i]].loc[df.columns[j]] == 2:\n                if df[df.columns[j]].loc[df.columns[i]] == 2:\n                    print(df.columns[i] + \" &lt;-&gt; \" + df.columns[j])\n\n    g_dict = dict()\n    for name_y in names:\n        g_dict[name_y] = []\n    for ty in range(nlags):\n        for name_y in names:\n            t_name_y = df.columns[ty*len(names)+names.index(name_y)]\n            for tx in range(nlags):\n                for name_x in names:\n                    t_name_x = df.columns[tx * len(names) + names.index(name_x)]\n                    if df[t_name_y].loc[t_name_x] == 2:\n                        if (name_x, tx-ty) not in g_dict[name_y]:\n                            g_dict[name_y].append((name_x, tx - ty))\n    print(g_dict)\n    return g_dict\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.VarLiNGAM.VarLiNGAM","title":"<code>VarLiNGAM</code>","text":"<p>             Bases: <code>CausalDiscoveryMethod</code></p> <p>VarLiNGAM causal discovery method.</p> Source code in <code>causalflow/causal_discovery/baseline/VarLiNGAM.py</code> <pre><code>class VarLiNGAM(CausalDiscoveryMethod):\n\"\"\"VarLiNGAM causal discovery method.\"\"\"\n\n    def __init__(self, \n                 data, \n                 min_lag,\n                 max_lag, \n                 verbosity, \n                 alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False,\n                 clean_cls = True):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (Data): data to analyse.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            verbosity (CPLevel): verbosity level.\n            alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n            clean_cls (bool): Clean console bit. Default to True.\n        \"\"\"\n        super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n\n\n    def run(self) -&gt; DAG:\n\"\"\"\n        Run causal discovery algorithm.\n\n        Returns:\n            (DAG): estimated causal model.\n        \"\"\"\n        split_by_causal_effect_sign = True\n\n        model = VARLiNGAM(lags = self.max_lag, criterion='bic', prune=True)\n        model.fit(self.data.d)\n\n        m = model._adjacency_matrices\n        am = np.concatenate([*m], axis=1)\n\n        dag = np.abs(am) &gt; self.alpha\n\n        if split_by_causal_effect_sign:\n            direction = np.array(np.where(dag))\n            signs = np.zeros_like(dag).astype('int64')\n            for i, j in direction.T:\n                signs[i][j] = np.sign(am[i][j]).astype('int64')\n            dag = signs\n\n        dag = np.abs(dag)\n        names = self.data.features\n        res_dict = dict()\n        for e in range(dag.shape[0]):\n            res_dict[names[e]] = []\n        for c in range(dag.shape[0]):\n            for te in range(dag.shape[1]):\n                if dag[c][te] == 1:\n                    e = te%dag.shape[0]\n                    t = te//dag.shape[0]\n                    res_dict[names[e]].append((names[c], -t))\n        self.CM = self._to_DAG(res_dict)\n\n        if self.resfolder is not None: self.logger.close()\n        return self.CM\n\n    def _to_DAG(self, graph):\n\"\"\"\n        Re-elaborates the result in a DAG.\n\n        Returns:\n            (DAG): result re-elaborated.\n        \"\"\"\n        tmp_dag = DAG(self.data.features, self.min_lag, self.max_lag, self.neglect_only_autodep)\n        tmp_dag.sys_context = dict()\n        for t in graph.keys():\n            for s in graph[t]:\n                lag = abs(s[1])\n                if lag &gt;= self.min_lag and lag &lt;= self.max_lag:\n                    tmp_dag.add_source(t, s[0], utils.DSCORE, 0, s[1])\n        return tmp_dag\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.VarLiNGAM.VarLiNGAM.__init__","title":"<code>__init__(data, min_lag, max_lag, verbosity, alpha=0.05, resfolder=None, neglect_only_autodep=False, clean_cls=True)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level.</p> required <code>alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> Source code in <code>causalflow/causal_discovery/baseline/VarLiNGAM.py</code> <pre><code>def __init__(self, \n             data, \n             min_lag,\n             max_lag, \n             verbosity, \n             alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False,\n             clean_cls = True):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (Data): data to analyse.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        verbosity (CPLevel): verbosity level.\n        alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        clean_cls (bool): Clean console bit. Default to True.\n    \"\"\"\n    super().__init__(data, min_lag, max_lag, verbosity, alpha, resfolder, neglect_only_autodep, clean_cls)\n</code></pre>"},{"location":"causal_discovery/#causalflow.causal_discovery.baseline.VarLiNGAM.VarLiNGAM.run","title":"<code>run()</code>","text":"<p>Run causal discovery algorithm.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model.</p> Source code in <code>causalflow/causal_discovery/baseline/VarLiNGAM.py</code> <pre><code>def run(self) -&gt; DAG:\n\"\"\"\n    Run causal discovery algorithm.\n\n    Returns:\n        (DAG): estimated causal model.\n    \"\"\"\n    split_by_causal_effect_sign = True\n\n    model = VARLiNGAM(lags = self.max_lag, criterion='bic', prune=True)\n    model.fit(self.data.d)\n\n    m = model._adjacency_matrices\n    am = np.concatenate([*m], axis=1)\n\n    dag = np.abs(am) &gt; self.alpha\n\n    if split_by_causal_effect_sign:\n        direction = np.array(np.where(dag))\n        signs = np.zeros_like(dag).astype('int64')\n        for i, j in direction.T:\n            signs[i][j] = np.sign(am[i][j]).astype('int64')\n        dag = signs\n\n    dag = np.abs(dag)\n    names = self.data.features\n    res_dict = dict()\n    for e in range(dag.shape[0]):\n        res_dict[names[e]] = []\n    for c in range(dag.shape[0]):\n        for te in range(dag.shape[1]):\n            if dag[c][te] == 1:\n                e = te%dag.shape[0]\n                t = te//dag.shape[0]\n                res_dict[names[e]].append((names[c], -t))\n    self.CM = self._to_DAG(res_dict)\n\n    if self.resfolder is not None: self.logger.close()\n    return self.CM\n</code></pre>"},{"location":"feature_selection_method/","title":"Feature Selection Methods","text":"<p>This module provides various classes for feature selection analysis.</p> Classes <p>CTest: support class for handling different feature selection methods. SelectionMethod: Abstract class.</p> <p>This module provides various classes for Correlation-based feature selection analysis.</p> Classes <p>Corr: Correlation class.</p> <p>This module provides various classes for Partial Correlation-based feature selection analysis.</p> Classes <p>ParCorr: Partial Correlation class.</p> <p>This module provides various classes for Mutual Information-based feature selection analysis.</p> Classes <p>MIestimator: support class for handling different Mutual Information estimators. MI: Mutual Information class.</p> <p>This module provides various classes for Transfer Entropy-based feature selection analysis.</p> Classes <p>TEestimator: support class for handling different Transfer Entropy estimators. TE: Transfer Entropy class.</p>"},{"location":"feature_selection_method/#causalflow.selection_methods.SelectionMethod.CTest","title":"<code>CTest</code>","text":"<p>             Bases: <code>Enum</code></p> <p>CTest Enumerator.</p> Source code in <code>causalflow/selection_methods/SelectionMethod.py</code> <pre><code>class CTest(Enum):\n\"\"\"CTest Enumerator.\"\"\"\n\n    Corr = \"Correlation\"\n    MI = \"Mutual Information\"\n    TE = \"Transfer Entropy\"\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.SelectionMethod.SelectionMethod","title":"<code>SelectionMethod</code>","text":"<p>             Bases: <code>ABC</code></p> <p>SelectionMethod abstract class.</p> Source code in <code>causalflow/selection_methods/SelectionMethod.py</code> <pre><code>class SelectionMethod(ABC):\n\"\"\"SelectionMethod abstract class.\"\"\"\n\n    def __init__(self, ctest):\n\"\"\"\n        Class constructor.\n\n        Args:\n            ctest (CTest): Feature Selection method's name.\n        \"\"\"\n        self.ctest = ctest\n        self.data = None\n        self.alpha = None\n        self.min_lag = None\n        self.max_lag = None\n        self.result = None\n\n\n    @property\n    def name(self):\n\"\"\"\n        Return Selection Method name.\n\n        Returns:\n            (str): Selection Method name.\n        \"\"\"\n        return self.ctest.value\n\n\n    def initialise(self, data: Data, alpha, min_lag, max_lag, graph):\n\"\"\"\n        Initialise the selection method.\n\n        Args:\n            data (Data): Data.\n            alpha (float): significance threshold.\n            min_lag (int): min lag time.\n            max_lag (int): max lag time.\n            graph (DAG): initial DAG (empty).\n        \"\"\"\n        self.data = data\n        self.alpha = alpha\n        self.min_lag = min_lag\n        self.max_lag = max_lag\n        self.result = graph\n\n\n    @abstractmethod\n    def compute_dependencies(self) -&gt; DAG:\n\"\"\"Abstract method.\"\"\"\n        pass\n\n\n    def _prepare_ts(self, target, lag, apply_lag = True, consider_autodep = True):\n\"\"\"\n        Prepare the dataframe to the analysis.\n\n        Args:\n            target (str): name target var\n            lag (int): lag time to apply\n            apply_lag (bool, optional): True if you want to apply the lag, False otherwise. Defaults to True.\n            consider_autodep (bool, optional): True if you want to consider autodependecy check. Defaults to True.\n\n        Returns:\n            tuple(DataFrame, DataFrame): source and target dataframe.\n        \"\"\"\n        if not consider_autodep:\n            if apply_lag:\n                Y = self.data.d[target][lag:]\n                X = self.data.d.loc[:, self.data.d.columns != target][:-lag]\n            else:\n                Y = self.data.d[target]\n                X = self.data.d.loc[:, self.data.d.columns != target]\n        else:\n            if apply_lag:\n                Y = self.data.d[target][lag:]\n                X = self.data.d[:-lag]\n            else:\n                Y = self.data.d[target]\n                X = self.data.d\n        return X, Y\n\n\n    def _add_dependency(self, t, s, score, pval, lag):\n\"\"\"\n        Add dependency from source (s) to target (t) specifying the score, pval and the lag.\n\n        Args:\n            t (str): target feature name.\n            s (str): source feature name.\n            score (float): selection method score.\n            pval (float): pval associated to the dependency.\n            lag (int): lag time of the dependency.\n        \"\"\"\n        self.result.add_source(t, s, score, pval, lag)\n\n        str_s = \"(\" + s + \" -\" + str(lag) + \")\"\n        str_t = \"(\" + t + \")\"\n\n        CP.info(\"\\tlink: \" + str_s + \" -?&gt; \" + str_t)\n        CP.info(\"\\t|val = \" + str(round(score,3)) + \" |pval = \" + str(str(round(pval,3))))\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.SelectionMethod.SelectionMethod.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return Selection Method name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Selection Method name.</p>"},{"location":"feature_selection_method/#causalflow.selection_methods.SelectionMethod.SelectionMethod.__init__","title":"<code>__init__(ctest)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>ctest</code> <code>CTest</code> <p>Feature Selection method's name.</p> required Source code in <code>causalflow/selection_methods/SelectionMethod.py</code> <pre><code>def __init__(self, ctest):\n\"\"\"\n    Class constructor.\n\n    Args:\n        ctest (CTest): Feature Selection method's name.\n    \"\"\"\n    self.ctest = ctest\n    self.data = None\n    self.alpha = None\n    self.min_lag = None\n    self.max_lag = None\n    self.result = None\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.SelectionMethod.SelectionMethod.compute_dependencies","title":"<code>compute_dependencies()</code>  <code>abstractmethod</code>","text":"<p>Abstract method.</p> Source code in <code>causalflow/selection_methods/SelectionMethod.py</code> <pre><code>@abstractmethod\ndef compute_dependencies(self) -&gt; DAG:\n\"\"\"Abstract method.\"\"\"\n    pass\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.SelectionMethod.SelectionMethod.initialise","title":"<code>initialise(data, alpha, min_lag, max_lag, graph)</code>","text":"<p>Initialise the selection method.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>Data.</p> required <code>alpha</code> <code>float</code> <p>significance threshold.</p> required <code>min_lag</code> <code>int</code> <p>min lag time.</p> required <code>max_lag</code> <code>int</code> <p>max lag time.</p> required <code>graph</code> <code>DAG</code> <p>initial DAG (empty).</p> required Source code in <code>causalflow/selection_methods/SelectionMethod.py</code> <pre><code>def initialise(self, data: Data, alpha, min_lag, max_lag, graph):\n\"\"\"\n    Initialise the selection method.\n\n    Args:\n        data (Data): Data.\n        alpha (float): significance threshold.\n        min_lag (int): min lag time.\n        max_lag (int): max lag time.\n        graph (DAG): initial DAG (empty).\n    \"\"\"\n    self.data = data\n    self.alpha = alpha\n    self.min_lag = min_lag\n    self.max_lag = max_lag\n    self.result = graph\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.Corr.Corr","title":"<code>Corr</code>","text":"<p>             Bases: <code>SelectionMethod</code></p> <p>Feature selection method based on Correlation analysis.</p> Source code in <code>causalflow/selection_methods/Corr.py</code> <pre><code>class Corr(SelectionMethod):\n\"\"\"Feature selection method based on Correlation analysis.\"\"\"\n\n    def __init__(self):\n\"\"\"Contructor class.\"\"\"\n        super().__init__(CTest.Corr)\n\n\n    def compute_dependencies(self):\n\"\"\"\n        Compute list of dependencies for each target by correlation analysis.\n\n        Returns:\n            (dict): dictonary(TARGET: list SOURCES)\n        \"\"\"\n        CP.info(\"\\n##\")\n        CP.info(\"## \" + self.name + \" analysis\")\n        CP.info(\"##\")\n\n        for lag in range(self.min_lag, self.max_lag + 1):\n            for target in self.data.features:\n                CP.info(\"\\n## Target variable: \" + target)\n\n                X, Y = self._prepare_ts(target, lag)\n                scores, pval = f_regression(X, Y)\n\n                # Filter on pvalue\n                f = pval &lt; self.alpha\n\n                # Result of the selection\n                sel_sources, sel_sources_score, sel_sources_pval = X.columns[f].tolist(), scores[f].tolist(), pval[f].tolist()\n\n                for s, score, pval in zip(sel_sources, sel_sources_score, sel_sources_pval):\n                    self._add_dependency(target, s, score, pval, lag)\n\n        return self.result\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.Corr.Corr.__init__","title":"<code>__init__()</code>","text":"<p>Contructor class.</p> Source code in <code>causalflow/selection_methods/Corr.py</code> <pre><code>def __init__(self):\n\"\"\"Contructor class.\"\"\"\n    super().__init__(CTest.Corr)\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.Corr.Corr.compute_dependencies","title":"<code>compute_dependencies()</code>","text":"<p>Compute list of dependencies for each target by correlation analysis.</p> <p>Returns:</p> Type Description <code>dict</code> <p>dictonary(TARGET: list SOURCES)</p> Source code in <code>causalflow/selection_methods/Corr.py</code> <pre><code>def compute_dependencies(self):\n\"\"\"\n    Compute list of dependencies for each target by correlation analysis.\n\n    Returns:\n        (dict): dictonary(TARGET: list SOURCES)\n    \"\"\"\n    CP.info(\"\\n##\")\n    CP.info(\"## \" + self.name + \" analysis\")\n    CP.info(\"##\")\n\n    for lag in range(self.min_lag, self.max_lag + 1):\n        for target in self.data.features:\n            CP.info(\"\\n## Target variable: \" + target)\n\n            X, Y = self._prepare_ts(target, lag)\n            scores, pval = f_regression(X, Y)\n\n            # Filter on pvalue\n            f = pval &lt; self.alpha\n\n            # Result of the selection\n            sel_sources, sel_sources_score, sel_sources_pval = X.columns[f].tolist(), scores[f].tolist(), pval[f].tolist()\n\n            for s, score, pval in zip(sel_sources, sel_sources_score, sel_sources_pval):\n                self._add_dependency(target, s, score, pval, lag)\n\n    return self.result\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.ParCorr.ParCorr","title":"<code>ParCorr</code>","text":"<p>             Bases: <code>SelectionMethod</code></p> <p>Feature selection method based on Partial Correlation analysis.</p> Source code in <code>causalflow/selection_methods/ParCorr.py</code> <pre><code>class ParCorr(SelectionMethod):\n\"\"\"Feature selection method based on Partial Correlation analysis.\"\"\"\n\n    def __init__(self):\n\"\"\"Class contructor.\"\"\"\n        super().__init__(CTest.Corr)\n\n\n    def get_residual(self, covar, target):\n\"\"\"\n        Calculate residual of the target variable obtaining conditioning on the covar variables.\n\n        Args:\n            covar (np.array): conditioning variables.\n            target (np.array): target variable.\n\n        Returns:\n            (np.array): residual.\n        \"\"\"\n        beta = np.linalg.lstsq(covar, target, rcond=None)[0]\n        return target - np.dot(covar, beta)\n\n\n    def partial_corr(self, X, Y, Z):\n\"\"\"\n        Calculate Partial correlation between X and Y conditioning on Z.\n\n        Args:\n            X (np.array): source candidate variable.\n            Y (np.array): target variable.\n            Z (np.array): conditioning variable.\n\n        Returns:\n            (float, float): partial correlation, p-value.\n        \"\"\"\n        pcorr, pval = stats.pearsonr(self.get_residual(Z, X), self.get_residual(Z, Y))\n\n        return pcorr, pval\n\n    def compute_dependencies(self):\n\"\"\"\n        Compute list of dependencies for each target by partial correlation analysis.\n\n        Returns:\n            (dict): dictonary(TARGET: list SOURCES).\n        \"\"\"\n        CP.info(\"\\n##\")\n        CP.info(\"## \" + self.name + \" analysis\")\n        CP.info(\"##\")\n\n        for lag in range(self.min_lag, self.max_lag + 1):\n            for target in self.data.features:\n                CP.info(\"\\n## Target variable: \" + target)\n                candidates = self.data.features\n\n                Y = np.array(self.data.d[target][lag:])\n\n                while candidates:\n                    tmp_res = None\n                    covars = self._get_sources(target)\n                    Z = np.array(self.data.d[covars][:-lag])\n\n                    for candidate in candidates:\n                        X = np.array(self.data.d[candidate][:-lag])\n                        score, pval = self.partial_corr(X, Y, Z)\n                        if pval &lt; self.alpha and (tmp_res is None or abs(tmp_res[1]) &lt; abs(score)):\n                            tmp_res = (candidate, score, pval)\n\n                    if tmp_res is not None: \n                        self._add_dependency(target, tmp_res[0], tmp_res[1], tmp_res[2], lag)\n                        candidates.remove(tmp_res[0])\n                    else:\n                        break\n        return self.result\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.ParCorr.ParCorr.__init__","title":"<code>__init__()</code>","text":"<p>Class contructor.</p> Source code in <code>causalflow/selection_methods/ParCorr.py</code> <pre><code>def __init__(self):\n\"\"\"Class contructor.\"\"\"\n    super().__init__(CTest.Corr)\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.ParCorr.ParCorr.compute_dependencies","title":"<code>compute_dependencies()</code>","text":"<p>Compute list of dependencies for each target by partial correlation analysis.</p> <p>Returns:</p> Type Description <code>dict</code> <p>dictonary(TARGET: list SOURCES).</p> Source code in <code>causalflow/selection_methods/ParCorr.py</code> <pre><code>def compute_dependencies(self):\n\"\"\"\n    Compute list of dependencies for each target by partial correlation analysis.\n\n    Returns:\n        (dict): dictonary(TARGET: list SOURCES).\n    \"\"\"\n    CP.info(\"\\n##\")\n    CP.info(\"## \" + self.name + \" analysis\")\n    CP.info(\"##\")\n\n    for lag in range(self.min_lag, self.max_lag + 1):\n        for target in self.data.features:\n            CP.info(\"\\n## Target variable: \" + target)\n            candidates = self.data.features\n\n            Y = np.array(self.data.d[target][lag:])\n\n            while candidates:\n                tmp_res = None\n                covars = self._get_sources(target)\n                Z = np.array(self.data.d[covars][:-lag])\n\n                for candidate in candidates:\n                    X = np.array(self.data.d[candidate][:-lag])\n                    score, pval = self.partial_corr(X, Y, Z)\n                    if pval &lt; self.alpha and (tmp_res is None or abs(tmp_res[1]) &lt; abs(score)):\n                        tmp_res = (candidate, score, pval)\n\n                if tmp_res is not None: \n                    self._add_dependency(target, tmp_res[0], tmp_res[1], tmp_res[2], lag)\n                    candidates.remove(tmp_res[0])\n                else:\n                    break\n    return self.result\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.ParCorr.ParCorr.get_residual","title":"<code>get_residual(covar, target)</code>","text":"<p>Calculate residual of the target variable obtaining conditioning on the covar variables.</p> <p>Parameters:</p> Name Type Description Default <code>covar</code> <code>np.array</code> <p>conditioning variables.</p> required <code>target</code> <code>np.array</code> <p>target variable.</p> required <p>Returns:</p> Type Description <code>np.array</code> <p>residual.</p> Source code in <code>causalflow/selection_methods/ParCorr.py</code> <pre><code>def get_residual(self, covar, target):\n\"\"\"\n    Calculate residual of the target variable obtaining conditioning on the covar variables.\n\n    Args:\n        covar (np.array): conditioning variables.\n        target (np.array): target variable.\n\n    Returns:\n        (np.array): residual.\n    \"\"\"\n    beta = np.linalg.lstsq(covar, target, rcond=None)[0]\n    return target - np.dot(covar, beta)\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.ParCorr.ParCorr.partial_corr","title":"<code>partial_corr(X, Y, Z)</code>","text":"<p>Calculate Partial correlation between X and Y conditioning on Z.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.array</code> <p>source candidate variable.</p> required <code>Y</code> <code>np.array</code> <p>target variable.</p> required <code>Z</code> <code>np.array</code> <p>conditioning variable.</p> required <p>Returns:</p> Type Description <code>(float, float)</code> <p>partial correlation, p-value.</p> Source code in <code>causalflow/selection_methods/ParCorr.py</code> <pre><code>def partial_corr(self, X, Y, Z):\n\"\"\"\n    Calculate Partial correlation between X and Y conditioning on Z.\n\n    Args:\n        X (np.array): source candidate variable.\n        Y (np.array): target variable.\n        Z (np.array): conditioning variable.\n\n    Returns:\n        (float, float): partial correlation, p-value.\n    \"\"\"\n    pcorr, pval = stats.pearsonr(self.get_residual(Z, X), self.get_residual(Z, Y))\n\n    return pcorr, pval\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.MI.MI","title":"<code>MI</code>","text":"<p>             Bases: <code>SelectionMethod</code></p> <p>Feature selection method based on Mutual Information analysis.</p> Source code in <code>causalflow/selection_methods/MI.py</code> <pre><code>class MI(SelectionMethod):\n\"\"\"Feature selection method based on Mutual Information analysis.\"\"\"\n\n    def __init__(self, estimator: MIestimator):\n\"\"\"\n        Class contructor.\n\n        Args:\n            estimator (MIestimator): Gaussian/Kraskov\n        \"\"\"\n        super().__init__(CTest.MI)\n        self.estimator = estimator\n\n    @property\n    def isOpenCLinstalled(self) -&gt; bool:\n\"\"\"\n        Check whether the pyopencl pkg is installed.\n\n        Returns:\n            bool: True if pyopencl is installed.\n        \"\"\"\n        try:\n            importlib.import_module('pyopencl')\n            return True\n        except ImportError:\n            return False\n\n    def _select_estimator(self):\n\"\"\"Select the MI estimator.\"\"\"\n        CP.info(\"\\n##\")\n        CP.info(\"## MI Estimator selection\")\n        CP.info(\"##\")\n\n        isGaussian = True\n\n        for f in self.data.features:\n            # Perform Shapiro-Wilk test\n            shapiro_stat, shapiro_p_value = shapiro(self.data.d[f])\n            # Perform Kolmogorov-Smirnov test\n            ks_stat, ks_p_value = kstest(self.data.d[f], 'norm')\n\n            # Print results\n            CP.debug(\"\\n\")\n            CP.debug(f\"Feature '{f}':\")\n            CP.debug(f\"\\t- Shapiro-Wilk test: val={round(shapiro_stat, 2)}, p-val={round(shapiro_p_value, 2)}\")\n            CP.debug(f\"\\t- Kolmogorov-Smirnov test: val={round(ks_stat, 2)}, p-val={round(ks_p_value, 2)}\")\n\n            # Check if p-values are less than significance level (e.g., 0.05) for normality\n            if shapiro_p_value &lt; 0.05 or ks_p_value &lt; 0.05:\n                CP.debug(\"\\tNot normally distributed\")\n                isGaussian = False\n                # break\n            else:\n                CP.debug(\"\\tNormally distributed\")\n\n        if isGaussian:\n            self.estimator = MIestimator.Gaussian\n        else:\n            self.estimator = MIestimator.OpenCLKraskov if self.isOpenCLinstalled else MIestimator.Kraskov\n        CP.info(\"\\n## MI Estimator: \" + self.estimator.value)\n\n    def compute_dependencies(self):\n\"\"\"\n        Compute list of dependencies for each target by mutual information analysis.\n\n        Returns:\n            (DAG): dependency dag\n        \"\"\"\n        if self.estimator is MIestimator.Auto: self._select_estimator()\n\n        with _suppress_stdout():\n            data = Data(self.d.values, dim_order='sp') # sp = samples(row) x processes(col)\n\n            network_analysis = MultivariateMI()\n            settings = {'cmi_estimator': self.estimator.value,\n                        'max_lag_sources': self.max_lag,\n                        'min_lag_sources': self.min_lag,\n                        'alpha_max_stats': self.alpha,\n                        'alpha_min_stats': self.alpha,\n                        'alpha_omnibus': self.alpha,\n                        'alpha_max_seq': self.alpha,\n                        'verbose': False}\n            results = network_analysis.analyse_network(settings=settings, data=data)\n\n        for t in results._single_target.keys():\n            sel_sources = [s[0] for s in results._single_target[t]['selected_vars_sources']]\n            if sel_sources:\n                sel_sources_lag = [s[1] for s in results._single_target[t]['selected_vars_sources']]\n                sel_sources_score = results._single_target[t]['selected_sources_mi']\n                sel_sources_pval = results._single_target[t]['selected_sources_pval']\n                for s, score, pval, lag in zip(sel_sources, sel_sources_score, sel_sources_pval, sel_sources_lag):\n                    self._add_dependency(self.features[t], self.features[s], score, pval, lag)\n\n        return self.result\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.MI.MI.isOpenCLinstalled","title":"<code>isOpenCLinstalled: bool</code>  <code>property</code>","text":"<p>Check whether the pyopencl pkg is installed.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if pyopencl is installed.</p>"},{"location":"feature_selection_method/#causalflow.selection_methods.MI.MI.__init__","title":"<code>__init__(estimator)</code>","text":"<p>Class contructor.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>MIestimator</code> <p>Gaussian/Kraskov</p> required Source code in <code>causalflow/selection_methods/MI.py</code> <pre><code>def __init__(self, estimator: MIestimator):\n\"\"\"\n    Class contructor.\n\n    Args:\n        estimator (MIestimator): Gaussian/Kraskov\n    \"\"\"\n    super().__init__(CTest.MI)\n    self.estimator = estimator\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.MI.MI.compute_dependencies","title":"<code>compute_dependencies()</code>","text":"<p>Compute list of dependencies for each target by mutual information analysis.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>dependency dag</p> Source code in <code>causalflow/selection_methods/MI.py</code> <pre><code>def compute_dependencies(self):\n\"\"\"\n    Compute list of dependencies for each target by mutual information analysis.\n\n    Returns:\n        (DAG): dependency dag\n    \"\"\"\n    if self.estimator is MIestimator.Auto: self._select_estimator()\n\n    with _suppress_stdout():\n        data = Data(self.d.values, dim_order='sp') # sp = samples(row) x processes(col)\n\n        network_analysis = MultivariateMI()\n        settings = {'cmi_estimator': self.estimator.value,\n                    'max_lag_sources': self.max_lag,\n                    'min_lag_sources': self.min_lag,\n                    'alpha_max_stats': self.alpha,\n                    'alpha_min_stats': self.alpha,\n                    'alpha_omnibus': self.alpha,\n                    'alpha_max_seq': self.alpha,\n                    'verbose': False}\n        results = network_analysis.analyse_network(settings=settings, data=data)\n\n    for t in results._single_target.keys():\n        sel_sources = [s[0] for s in results._single_target[t]['selected_vars_sources']]\n        if sel_sources:\n            sel_sources_lag = [s[1] for s in results._single_target[t]['selected_vars_sources']]\n            sel_sources_score = results._single_target[t]['selected_sources_mi']\n            sel_sources_pval = results._single_target[t]['selected_sources_pval']\n            for s, score, pval, lag in zip(sel_sources, sel_sources_score, sel_sources_pval, sel_sources_lag):\n                self._add_dependency(self.features[t], self.features[s], score, pval, lag)\n\n    return self.result\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.MI.MIestimator","title":"<code>MIestimator</code>","text":"<p>             Bases: <code>Enum</code></p> <p>MIestimator Enumerator.</p> Source code in <code>causalflow/selection_methods/MI.py</code> <pre><code>class MIestimator(Enum):\n\"\"\"MIestimator Enumerator.\"\"\"\n\n    Auto = 'Auto'\n    Gaussian = 'JidtGaussianCMI'\n    Kraskov = 'JidtKraskovCMI'\n    OpenCLKraskov = 'OpenCLKraskovCMI'\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.TE.TE","title":"<code>TE</code>","text":"<p>             Bases: <code>SelectionMethod</code></p> <p>Feature selection method based on Trasfer Entropy analysis.</p> Source code in <code>causalflow/selection_methods/TE.py</code> <pre><code>class TE(SelectionMethod):\n\"\"\"Feature selection method based on Trasfer Entropy analysis.\"\"\"\n\n    def __init__(self, estimator: TEestimator):\n\"\"\"\n        Class contructor.\n\n        Args:\n            estimator (TEestimator): Gaussian/Kraskov.\n        \"\"\"\n        super().__init__(CTest.TE)\n        self.estimator = estimator\n\n    @property\n    def isOpenCLinstalled(self) -&gt; bool:\n\"\"\"\n        Check whether the pyopencl pkg is installed.\n\n        Returns:\n            bool: True if pyopencl is installed.\n        \"\"\"\n        try:\n            importlib.import_module('pyopencl')\n            return True\n        except ImportError:\n            return False\n\n    def _select_estimator(self):\n\"\"\"Select the TE estimator.\"\"\"\n        CP.info(\"\\n##\")\n        CP.info(\"## TE Estimator selection\")\n        CP.info(\"##\")\n\n        isGaussian = True\n\n        for f in self.data.features:\n            # Perform Shapiro-Wilk test\n            shapiro_stat, shapiro_p_value = shapiro(self.data.d[f])\n            # Perform Kolmogorov-Smirnov test\n            ks_stat, ks_p_value = kstest(self.data.d[f], 'norm')\n\n            # Print results\n            CP.debug(\"\\n\")\n            CP.debug(f\"Feature '{f}':\")\n            CP.debug(f\"\\t- Shapiro-Wilk test: val={round(shapiro_stat, 2)}, p-val={round(shapiro_p_value, 2)}\")\n            CP.debug(f\"\\t- Kolmogorov-Smirnov test: val={round(ks_stat, 2)}, p-val={round(ks_p_value, 2)}\")\n\n            # Check if p-values are less than significance level (e.g., 0.05) for normality\n            if shapiro_p_value &lt; 0.05 or ks_p_value &lt; 0.05:\n                CP.debug(\"\\tNot normally distributed\")\n                isGaussian = False\n                # break\n            else:\n                CP.debug(\"\\tNormally distributed\")\n\n        if isGaussian:\n            self.estimator = TEestimator.Gaussian\n        else:\n            self.estimator = TEestimator.OpenCLKraskov if self.isOpenCLinstalled else TEestimator.Kraskov\n        CP.info(\"\\n## TE Estimator: \" + self.estimator.value)\n\n\n    def compute_dependencies(self):\n\"\"\"\n        Compute list of dependencies for each target by transfer entropy analysis.\n\n        Returns:\n            (DAG): dependency dag.\n        \"\"\"\n        if self.estimator is TEestimator.Auto: self._select_estimator()\n\n        multi_network_analysis = MultivariateTE()\n        bi_network_analysis = BivariateMI()\n        cross_settings = {'cmi_estimator': self.estimator.value,\n                    'max_lag_sources': self.max_lag,\n                    'min_lag_sources': self.min_lag,\n                    'max_lag_target': self.max_lag,\n                    'min_lag_target': self.min_lag,\n                    'alpha_max_stats': self.alpha,\n                    'alpha_min_stats': self.alpha,\n                    'alpha_omnibus': self.alpha,\n                    'alpha_max_seq': self.alpha,\n                    'verbose': False}\n        autodep_settings = copy.deepcopy(cross_settings)\n        if self.min_lag == 0:\n            autodep_settings['min_lag_sources'] = 1\n\n        CP.info(\"\\n##\")\n        CP.info(\"## \" + self.name + \" analysis\")\n        CP.info(\"##\")\n        for target in self.data.features:\n            CP.info(\"\\n## Target variable: \" + target)\n            with _suppress_stdout():\n                t = self.data.features.index(target)\n\n                # Check auto-dependency\n                tmp_d = np.c_[self.data.d.values[:, t], self.data.d.values[:, t]]\n                data = Data(tmp_d, dim_order='sp') # sp = samples(row) x processes(col)\n                res_auto = bi_network_analysis.analyse_single_target(settings = autodep_settings, data = data, target = 0, sources = 1)\n\n                # Check cross-dependencies\n                data = Data(self.data.d.values, dim_order='sp') # sp = samples(row) x processes(col)\n                res_cross = multi_network_analysis.analyse_single_target(settings = cross_settings, data = data, target = t)\n\n            # Auto-dependency handling\n            auto_lag = [s[1] for s in res_auto._single_target[0]['selected_vars_sources']]\n            auto_score = res_auto._single_target[0]['selected_sources_mi']\n            auto_pval = res_auto._single_target[0]['selected_sources_pval']\n            if auto_score is not None:\n                for score, pval, lag in zip(auto_score, auto_pval, auto_lag):\n                    self._add_dependency(self.data.features[t], self.data.features[t], score, pval, lag)\n\n            # Cross-dependencies handling    \n            sel_sources = [s[0] for s in res_cross._single_target[t]['selected_vars_sources']]\n            if sel_sources:\n                sel_sources_lag = [s[1] for s in res_cross._single_target[t]['selected_vars_sources']]\n                sel_sources_score = res_cross._single_target[t]['selected_sources_te']\n                sel_sources_pval = res_cross._single_target[t]['selected_sources_pval']\n                for s, score, pval, lag in zip(sel_sources, sel_sources_score, sel_sources_pval, sel_sources_lag):\n                    self._add_dependency(self.data.features[t], self.data.features[s], score, pval, lag)\n\n            if auto_score is None and not sel_sources:\n                CP.info(\"\\tno sources selected\")\n\n        return self.result\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.TE.TE.isOpenCLinstalled","title":"<code>isOpenCLinstalled: bool</code>  <code>property</code>","text":"<p>Check whether the pyopencl pkg is installed.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if pyopencl is installed.</p>"},{"location":"feature_selection_method/#causalflow.selection_methods.TE.TE.__init__","title":"<code>__init__(estimator)</code>","text":"<p>Class contructor.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>TEestimator</code> <p>Gaussian/Kraskov.</p> required Source code in <code>causalflow/selection_methods/TE.py</code> <pre><code>def __init__(self, estimator: TEestimator):\n\"\"\"\n    Class contructor.\n\n    Args:\n        estimator (TEestimator): Gaussian/Kraskov.\n    \"\"\"\n    super().__init__(CTest.TE)\n    self.estimator = estimator\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.TE.TE.compute_dependencies","title":"<code>compute_dependencies()</code>","text":"<p>Compute list of dependencies for each target by transfer entropy analysis.</p> <p>Returns:</p> Type Description <code>DAG</code> <p>dependency dag.</p> Source code in <code>causalflow/selection_methods/TE.py</code> <pre><code>def compute_dependencies(self):\n\"\"\"\n    Compute list of dependencies for each target by transfer entropy analysis.\n\n    Returns:\n        (DAG): dependency dag.\n    \"\"\"\n    if self.estimator is TEestimator.Auto: self._select_estimator()\n\n    multi_network_analysis = MultivariateTE()\n    bi_network_analysis = BivariateMI()\n    cross_settings = {'cmi_estimator': self.estimator.value,\n                'max_lag_sources': self.max_lag,\n                'min_lag_sources': self.min_lag,\n                'max_lag_target': self.max_lag,\n                'min_lag_target': self.min_lag,\n                'alpha_max_stats': self.alpha,\n                'alpha_min_stats': self.alpha,\n                'alpha_omnibus': self.alpha,\n                'alpha_max_seq': self.alpha,\n                'verbose': False}\n    autodep_settings = copy.deepcopy(cross_settings)\n    if self.min_lag == 0:\n        autodep_settings['min_lag_sources'] = 1\n\n    CP.info(\"\\n##\")\n    CP.info(\"## \" + self.name + \" analysis\")\n    CP.info(\"##\")\n    for target in self.data.features:\n        CP.info(\"\\n## Target variable: \" + target)\n        with _suppress_stdout():\n            t = self.data.features.index(target)\n\n            # Check auto-dependency\n            tmp_d = np.c_[self.data.d.values[:, t], self.data.d.values[:, t]]\n            data = Data(tmp_d, dim_order='sp') # sp = samples(row) x processes(col)\n            res_auto = bi_network_analysis.analyse_single_target(settings = autodep_settings, data = data, target = 0, sources = 1)\n\n            # Check cross-dependencies\n            data = Data(self.data.d.values, dim_order='sp') # sp = samples(row) x processes(col)\n            res_cross = multi_network_analysis.analyse_single_target(settings = cross_settings, data = data, target = t)\n\n        # Auto-dependency handling\n        auto_lag = [s[1] for s in res_auto._single_target[0]['selected_vars_sources']]\n        auto_score = res_auto._single_target[0]['selected_sources_mi']\n        auto_pval = res_auto._single_target[0]['selected_sources_pval']\n        if auto_score is not None:\n            for score, pval, lag in zip(auto_score, auto_pval, auto_lag):\n                self._add_dependency(self.data.features[t], self.data.features[t], score, pval, lag)\n\n        # Cross-dependencies handling    \n        sel_sources = [s[0] for s in res_cross._single_target[t]['selected_vars_sources']]\n        if sel_sources:\n            sel_sources_lag = [s[1] for s in res_cross._single_target[t]['selected_vars_sources']]\n            sel_sources_score = res_cross._single_target[t]['selected_sources_te']\n            sel_sources_pval = res_cross._single_target[t]['selected_sources_pval']\n            for s, score, pval, lag in zip(sel_sources, sel_sources_score, sel_sources_pval, sel_sources_lag):\n                self._add_dependency(self.data.features[t], self.data.features[s], score, pval, lag)\n\n        if auto_score is None and not sel_sources:\n            CP.info(\"\\tno sources selected\")\n\n    return self.result\n</code></pre>"},{"location":"feature_selection_method/#causalflow.selection_methods.TE.TEestimator","title":"<code>TEestimator</code>","text":"<p>             Bases: <code>Enum</code></p> <p>TEestimator Enumerator.</p> Source code in <code>causalflow/selection_methods/TE.py</code> <pre><code>class TEestimator(Enum):\n\"\"\"TEestimator Enumerator.\"\"\"\n\n    Auto = 'Auto'\n    Gaussian = 'JidtGaussianCMI'\n    Kraskov = 'JidtKraskovCMI'\n    OpenCLKraskov = 'OpenCLKraskovCMI'\n</code></pre>"},{"location":"graph/","title":"Graph","text":"<p>This module provides the Node class.</p> Classes <p>Node: class for facilitating the handling and the creation of nodes for a DAG.</p> <p>This module provides the DAG class.</p> Classes <p>DAG: class for facilitating the handling and the creation of DAGs.</p> <p>This module provides the PAG class.</p> Classes <p>PAG: class for facilitating the handling and the creation of PAGs.</p>"},{"location":"graph/#causalflow.graph.Node.Node","title":"<code>Node</code>","text":"<p>Node class.</p> Source code in <code>causalflow/graph/Node.py</code> <pre><code>class Node():\n\"\"\"Node class.\"\"\"\n\n    def __init__(self, name, neglect_autodep):\n\"\"\"\n        Class contructor.\n\n        Args:\n            name (str): node name.\n            neglect_autodep (bool): flag to decide whether to to skip the node if it is only auto-dependent.\n        \"\"\"\n        self.name = name\n        self.sources = dict()\n        self.children = list()\n        self.neglect_autodep = neglect_autodep\n        self.intervention_node = False        \n        self.associated_context = None        \n\n\n    @property\n    def is_autodependent(self) -&gt; bool:\n\"\"\"\n        Return True if the node is autodependent.\n\n        Returns:\n            bool: Returns True if the node is autodependent. Otherwise False.\n        \"\"\"\n        return self.name in self.sourcelist\n\n\n    @property\n    def is_isolated(self) -&gt; bool:\n\"\"\"\n        Return True if the node is isolated.\n\n        Returns:\n            bool: Returns True if the node is isolated. Otherwise False.\n        \"\"\"\n        if self.neglect_autodep:\n            return (self.is_exogenous and not self.has_child) or self.is_only_autodep or self.is_only_autodep_context\n        return (self.is_exogenous or self.has_only_context) and not self.has_child\n\n\n    @property\n    def is_only_autodep(self) -&gt; bool:\n\"\"\"\n        Return True if the node is ONLY auto-dependent.\n\n        Returns:\n            bool: Returns True if the node is ONLY auto-dependent. Otherwise False.\n        \"\"\"\n        return len(self.sources) == 1 and self.name in self.sourcelist and len(self.children) == 1 and self.name in self.children\n\n\n    @property\n    def has_only_context(self) -&gt; bool:\n\"\"\"\n        Return True if the node has ONLY the context variable as parent.\n\n        Returns:\n            bool: Returns True if the node has ONLY the context variable as parent. Otherwise False.\n        \"\"\"\n        return len(self.sources) == 1 and self.associated_context in self.sourcelist\n\n\n    @property\n    def is_only_autodep_context(self) -&gt; bool:\n\"\"\"\n        Return True if the node has ONLY the context variable and itself as parent.\n\n        Returns:\n            bool: Returns True if the node has ONLY the context variable and itself as parent. Otherwise False.\n        \"\"\"\n        return len(self.sources) == 2 and self.name in self.sourcelist and self.associated_context in self.sourcelist and len(self.children) == 1 and self.name in self.children\n\n\n    @property\n    def is_exogenous(self) -&gt; bool:\n\"\"\"\n        Return True if the node has no parents.\n\n        Returns:\n            bool: Returns True if the node has no parents. Otherwise False.\n        \"\"\"\n        return len(self.sources) == 0\n\n\n    @property\n    def has_child(self) -&gt; bool:\n\"\"\"\n        Return True if the node has at least one child.\n\n        Returns:\n            bool: Returns True if the node has at least one child. Otherwise False.\n        \"\"\"\n        return len(self.children) &gt; 0\n\n\n    @property\n    def sourcelist(self) -&gt; list:\n\"\"\"\n        Return list of source names.\n\n        Returns:\n            list(str): Returns list of source names.\n        \"\"\"\n        return [s[0] for s in self.sources]\n\n\n    @property\n    def autodependency_links(self) -&gt; list:\n\"\"\"\n        Return list of autodependency links.\n\n        Returns:\n            list: Returns list of autodependency links.\n\n        \"\"\"\n        autodep_links = list()\n        if self.is_autodependent:\n            for s in self.sources: \n                if s[0] == self.name: \n                    autodep_links.append(s)\n        return autodep_links\n\n\n    @property\n    def get_max_autodependent(self) -&gt; float:\n\"\"\"\n        Return max score of autodependent link.\n\n        Returns:\n            float: Returns max score of autodependent link.\n        \"\"\"\n        max_score = 0\n        max_s = None\n        if self.is_autodependent:\n            for s in self.sources: \n                if s[0] == self.name:\n                    if self.sources[s][SCORE] &gt; max_score: max_s = s\n        return max_s\n</code></pre>"},{"location":"graph/#causalflow.graph.Node.Node.autodependency_links","title":"<code>autodependency_links: list</code>  <code>property</code>","text":"<p>Return list of autodependency links.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Returns list of autodependency links.</p>"},{"location":"graph/#causalflow.graph.Node.Node.get_max_autodependent","title":"<code>get_max_autodependent: float</code>  <code>property</code>","text":"<p>Return max score of autodependent link.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Returns max score of autodependent link.</p>"},{"location":"graph/#causalflow.graph.Node.Node.has_child","title":"<code>has_child: bool</code>  <code>property</code>","text":"<p>Return True if the node has at least one child.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns True if the node has at least one child. Otherwise False.</p>"},{"location":"graph/#causalflow.graph.Node.Node.has_only_context","title":"<code>has_only_context: bool</code>  <code>property</code>","text":"<p>Return True if the node has ONLY the context variable as parent.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns True if the node has ONLY the context variable as parent. Otherwise False.</p>"},{"location":"graph/#causalflow.graph.Node.Node.is_autodependent","title":"<code>is_autodependent: bool</code>  <code>property</code>","text":"<p>Return True if the node is autodependent.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns True if the node is autodependent. Otherwise False.</p>"},{"location":"graph/#causalflow.graph.Node.Node.is_exogenous","title":"<code>is_exogenous: bool</code>  <code>property</code>","text":"<p>Return True if the node has no parents.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns True if the node has no parents. Otherwise False.</p>"},{"location":"graph/#causalflow.graph.Node.Node.is_isolated","title":"<code>is_isolated: bool</code>  <code>property</code>","text":"<p>Return True if the node is isolated.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns True if the node is isolated. Otherwise False.</p>"},{"location":"graph/#causalflow.graph.Node.Node.is_only_autodep","title":"<code>is_only_autodep: bool</code>  <code>property</code>","text":"<p>Return True if the node is ONLY auto-dependent.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns True if the node is ONLY auto-dependent. Otherwise False.</p>"},{"location":"graph/#causalflow.graph.Node.Node.is_only_autodep_context","title":"<code>is_only_autodep_context: bool</code>  <code>property</code>","text":"<p>Return True if the node has ONLY the context variable and itself as parent.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns True if the node has ONLY the context variable and itself as parent. Otherwise False.</p>"},{"location":"graph/#causalflow.graph.Node.Node.sourcelist","title":"<code>sourcelist: list</code>  <code>property</code>","text":"<p>Return list of source names.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>str</code> <p>Returns list of source names.</p>"},{"location":"graph/#causalflow.graph.Node.Node.__init__","title":"<code>__init__(name, neglect_autodep)</code>","text":"<p>Class contructor.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>node name.</p> required <code>neglect_autodep</code> <code>bool</code> <p>flag to decide whether to to skip the node if it is only auto-dependent.</p> required Source code in <code>causalflow/graph/Node.py</code> <pre><code>def __init__(self, name, neglect_autodep):\n\"\"\"\n    Class contructor.\n\n    Args:\n        name (str): node name.\n        neglect_autodep (bool): flag to decide whether to to skip the node if it is only auto-dependent.\n    \"\"\"\n    self.name = name\n    self.sources = dict()\n    self.children = list()\n    self.neglect_autodep = neglect_autodep\n    self.intervention_node = False        \n    self.associated_context = None        \n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG","title":"<code>DAG</code>","text":"<p>DAG class.</p> Source code in <code>causalflow/graph/DAG.py</code> <pre><code>class DAG():\n\"\"\"DAG class.\"\"\"\n\n    def __init__(self, var_names, min_lag, max_lag, neglect_autodep = False, scm = None):\n\"\"\"\n        DAG constructor.\n\n        Args:\n            var_names (list): variable list.\n            min_lag (int): minimum time lag.\n            max_lag (int): maximum time lag.\n            neglect_autodep (bool, optional): bit to neglect nodes when they are only autodependent. Defaults to False.\n            scm (dict, optional): Build the DAG for SCM. Defaults to None.\n        \"\"\"\n        self.g = {var: Node(var, neglect_autodep) for var in var_names}\n        self.neglect_autodep = neglect_autodep\n        self.sys_context = dict()\n        self.min_lag = min_lag\n        self.max_lag = max_lag\n\n        if scm is not None:\n            for t in scm:\n                    for s in scm[t]: \n                        if len(s) == 2:\n                            self.add_source(t, s[0], 0.3, 0, s[1])\n                        elif len(s) == 3:\n                            self.add_source(t, s[0], 0.3, 0, s[1], s[2])\n\n\n    @property\n    def features(self) -&gt; list:\n\"\"\"\n        Return features list.\n\n        Returns:\n            list: Features list.\n        \"\"\"\n        return list(self.g.keys())\n\n\n    @property\n    def pretty_features(self):\n\"\"\"\n        Return list of features with LaTeX symbols.\n\n        Returns:\n            list(str): list of feature names.\n        \"\"\"\n        return [r'$' + str(v) + '$' for v in self.g.keys()]\n\n\n    @property\n    def autodep_nodes(self) -&gt; list:\n\"\"\"\n        Return the autodependent nodes list.\n\n        Returns:\n            list: Autodependent nodes list.\n        \"\"\"\n        autodeps = list()\n        for t in self.g:\n            # NOTE: I commented this because I want to check all the auto-dep nodes with obs data\n            # if self.g[t].is_autodependent and self.g[t].intervention_node: autodeps.append(t)\n            if self.g[t].is_autodependent: autodeps.append(t)\n        return autodeps\n\n\n    @property\n    def interventions_links(self) -&gt; list:\n\"\"\"\n        Return the intervention links list.\n\n        Returns:\n            list: Intervention link list.\n        \"\"\"\n        int_links = list()\n        for t in self.g:\n            for s in self.g[t].sources:\n                if self.g[s[0]].intervention_node:\n                    int_links.append((s[0], s[1], t))\n        return int_links\n\n\n    def add_source(self, t, s, score, pval, lag, mode = LinkType.Directed.value):\n\"\"\"\n        Add source node to a target node.\n\n        Args:\n            t (str): target node name.\n            s (str): source node name.\n            score (float): dependency score.\n            pval (float): dependency p-value.\n            lag (int): dependency lag.\n        \"\"\"\n        self.g[t].sources[(s, abs(lag))] = {SCORE: score, PVAL: pval, TYPE: mode}\n        self.g[s].children.append(t)\n\n\n    def del_source(self, t, s, lag):\n\"\"\"\n        Remove source node from a target node.\n\n        Args:\n            t (str): target node name.\n            s (str): source node name.\n            lag (int): dependency lag.\n        \"\"\"\n        del self.g[t].sources[(s, lag)]\n        self.g[s].children.remove(t)\n\n\n    def remove_unneeded_features(self):\n\"\"\"Remove isolated nodes.\"\"\"\n        tmp = copy.deepcopy(self.g)\n        for t in self.g.keys():\n            if self.g[t].is_isolated: \n                if self.g[t].intervention_node: del tmp[self.g[t].associated_context]\n                del tmp[t]\n        self.g = tmp\n\n\n    def add_context(self):\n\"\"\"Add context variables.\"\"\"\n        for sys_var, context_var in self.sys_context.items():\n            if sys_var in self.features:\n\n                # Adding context var to the graph\n                self.g[context_var] = Node(context_var, self.neglect_autodep)\n\n                # Adding context var to sys var\n                self.g[sys_var].intervention_node = True\n                self.g[sys_var].associated_context = context_var\n                self.add_source(sys_var, context_var, 1, 0, 0)\n\n        # NOTE: bi-directed link contemporanous link between context vars\n        for sys_var, context_var in self.sys_context.items():\n            if sys_var in self.features:\n                other_context = [value for value in self.sys_context.values() if value != context_var and value in self.features]\n                for other in other_context: self.add_source(context_var, other, 1, 0, 0)\n\n\n    def remove_context(self):\n\"\"\"Remove context variables.\"\"\"\n        for sys_var, context_var in self.sys_context.items():\n            if sys_var in self.g:\n                # Removing context var from sys var\n                # self.g[sys_var].intervention_node = False\n                self.g[sys_var].associated_context = None\n                self.del_source(sys_var, context_var, 0)\n\n                # Removing context var from dag\n                del self.g[context_var]\n\n\n    def get_link_assumptions(self, autodep_ok = False) -&gt; dict:\n\"\"\"\n        Return link assumption dictionary.\n\n        Args:\n            autodep_ok (bool, optional): If true, autodependecy link assumption = --&gt;. Otherwise -?&gt;. Defaults to False.\n\n        Returns:\n            dict: link assumption dictionary.\n        \"\"\"\n        link_assump = {self.features.index(f): dict() for f in self.features}\n        for t in self.g:\n            for s in self.g[t].sources:\n                if autodep_ok and s[0] == t: # NOTE: new condition added in order to not control twice the autodependency links\n                    link_assump[self.features.index(t)][(self.features.index(s[0]), -abs(s[1]))] = '--&gt;'\n\n                elif s[0] not in list(self.sys_context.values()):\n                    if s[1] == 0 and (t, 0) in self.g[s[0]].sources:\n                        link_assump[self.features.index(t)][(self.features.index(s[0]), 0)] = 'o-o'\n                    elif s[1] == 0 and (t, 0) not in self.g[s[0]].sources:\n                        link_assump[self.features.index(t)][(self.features.index(s[0]),0)] = '-?&gt;'\n                        link_assump[self.features.index(s[0])][(self.features.index(t), 0)] = '&lt;?-'\n                    elif s[1] &gt; 0:\n                        link_assump[self.features.index(t)][(self.features.index(s[0]), -abs(s[1]))] = '-?&gt;'\n\n                elif t in self.sys_context.keys() and s[0] == self.sys_context[t]:\n                    link_assump[self.features.index(t)][(self.features.index(s[0]), -abs(s[1]))] = '--&gt;'\n\n        return link_assump\n\n\n    def make_pretty(self) -&gt; dict:\n\"\"\"\n        Make variables' names pretty, i.e. $ varname $ with '{' after '_' and '}' at the end of the string.\n\n        Returns:\n            dict: pretty DAG.\n        \"\"\"\n        def prettify(name):\n            return '$' + re.sub(r'_(\\w+)', r'_{\\1}', name) + '$'\n\n        pretty = dict()\n        for t in self.g:\n            p_t = prettify(t)\n            pretty[p_t] = copy.deepcopy(self.g[t])\n            pretty[p_t].name = p_t\n            pretty[p_t].children = [prettify(c) for c in self.g[t].children]\n            for s in self.g[t].sources:\n                del pretty[p_t].sources[s]\n                p_s = prettify(s[0])\n                pretty[p_t].sources[(p_s, s[1])] = {\n                    SCORE: self.g[t].sources[s][SCORE],\n                    PVAL: self.g[t].sources[s][PVAL],\n                    TYPE: self.g[t].sources[s][TYPE]\n                }\n        return pretty\n\n\n    def dag(self,\n            node_layout = 'dot',\n            min_width = 1, \n            max_width = 5,\n            min_score = 0, \n            max_score = 1,\n            node_size = 8, \n            node_color = 'orange',\n            edge_color = 'grey',\n            bundle_parallel_edges = True,\n            font_size = 12,\n            label_type = LabelType.Lag,\n            save_name = None,\n            img_extention = ImageExt.PNG):\n\"\"\"\n        Build a dag.\n\n        Args:\n            node_layout (str, optional): Node layout. Defaults to 'dot'.\n            min_width (int, optional): minimum linewidth. Defaults to 1.\n            max_width (int, optional): maximum linewidth. Defaults to 5.\n            min_score (int, optional): minimum score range. Defaults to 0.\n            max_score (int, optional): maximum score range. Defaults to 1.\n            node_size (int, optional): node size. Defaults to 8.\n            node_color (str, optional): node color. Defaults to 'orange'.\n            edge_color (str, optional): edge color. Defaults to 'grey'.\n            bundle_parallel_edges (str, optional): bundle parallel edge bit. Defaults to True.\n            font_size (int, optional): font size. Defaults to 12.\n            label_type (LabelType, optional): enum to set whether to show the lag time (LabelType.Lag) or the strength (LabelType.Score) of the dependencies on each link/node or not showing the labels (LabelType.NoLabels). Default LabelType.Lag.\n            save_name (str, optional): Filename path. If None, plot is shown and not saved. Defaults to None.\n            img_extention (ImageExt, optional): Image Extension. Defaults to PNG.\n        \"\"\"\n        r = copy.deepcopy(self)\n        r.g = r.make_pretty()\n\n        G = nx.DiGraph()\n\n        # NODES DEFINITION\n        G.add_nodes_from(r.g.keys())\n\n        # BORDER LINE\n        border = dict()\n        for t in r.g:\n            border[t] = 0\n            if r.g[t].is_autodependent:\n                autodep = r.g[t].get_max_autodependent\n                border[t] = max(self.__scale(r.g[t].sources[autodep][SCORE], min_width, max_width, min_score, max_score), border[t])\n\n        # BORDER LABEL\n        node_label = None\n        if label_type == LabelType.Lag or label_type == LabelType.Score:\n            node_label = {t: [] for t in r.g.keys()}\n            for t in r.g:\n                if r.g[t].is_autodependent:\n                    autodep = r.g[t].get_max_autodependent\n                    if label_type == LabelType.Lag:\n                        node_label[t].append(autodep[1])\n                    elif label_type == LabelType.Score:\n                        node_label[t].append(round(r.g[t].sources[autodep][SCORE], 3))\n                node_label[t] = \",\".join(str(s) for s in node_label[t])\n\n        # EDGE DEFINITION\n        edges = list()\n        edge_width = dict()\n        arrows = {}\n        for t in r.g:\n            for s in r.g[t].sources:\n                if t != s[0]:\n                    self.__add_edge(min_width, max_width, min_score, max_score, \n                                    edges, edge_width, arrows, r, t, s, \n                                    s[0], t)\n\n        G.add_edges_from(edges)\n\n        # EDGE LABEL\n        edge_label = None\n        if label_type == LabelType.Lag or label_type == LabelType.Score:\n            edge_label = {(s[0], t): [] for t in r.g for s in r.g[t].sources if t != s[0]}\n            for t in r.g:\n                for s in r.g[t].sources:\n                    if t != s[0]:\n                        if label_type == LabelType.Lag:\n                            edge_label[(s[0], t)].append(s[1])\n                        elif label_type == LabelType.Score:\n                            edge_label[(s[0], t)].append(round(r.g[t].sources[s][SCORE], 3))\n            for k in edge_label.keys():\n                edge_label[k] = \",\".join(str(s) for s in edge_label[k])\n\n        fig, ax = plt.subplots(figsize=(8,6))\n\n        if edges:\n            a = Graph(G, \n                    node_layout = node_layout,\n                    node_size = node_size,\n                    node_color = node_color,\n                    node_labels = node_label,\n                    node_edge_width = border,\n                    node_label_fontdict = dict(size=font_size),\n                    node_edge_color = edge_color,\n                    node_label_offset = 0.1,\n                    node_alpha = 1,\n\n                    arrows = arrows,\n                    edge_layout = 'arc',\n                    edge_label = label_type != LabelType.NoLabels,\n                    edge_labels = edge_label,\n                    edge_label_fontdict = dict(size=font_size),\n                    edge_color = edge_color, \n                    tail_color = 'white', \n                    edge_width = edge_width,\n                    edge_alpha = 1,\n                    edge_zorder = 1,\n                    edge_label_position = 0.35,\n                    edge_layout_kwargs = dict(bundle_parallel_edges = bundle_parallel_edges, k = 0.05))\n\n            nx.draw_networkx_labels(G, \n                                    pos = a.node_positions,\n                                    labels = {n: n for n in G},\n                                    font_size = font_size)\n\n        if save_name is not None:\n            plt.savefig(save_name + img_extention.value, dpi = 300)\n        else:\n            plt.show()\n\n\n    def __add_edge(self, min_width, max_width, min_score, max_score, edges, edge_width, arrows, r, t, s, s_node, t_node):\n        edges.append((s_node, t_node))\n        score = r.g[t].sources[s][SCORE] if r.g[t].sources[s][SCORE] != float('inf') else 1\n        edge_width[(s_node, t_node)] = self.__scale(score, min_width, max_width, min_score, max_score)\n\n        if r.g[t].sources[s][TYPE] == LinkType.Directed.value:\n            arrows[(s_node, t_node)] = {'h':'&gt;', 't':''}\n\n        elif r.g[t].sources[s][TYPE] == LinkType.Bidirected.value:\n            edges.append((t_node, s_node))\n            edge_width[(t_node, s_node)] = self.__scale(score, min_width, max_width, min_score, max_score)\n            arrows[(t_node, s_node)] = {'h':'&gt;', 't':''}\n            arrows[(s_node, t_node)] = {'h':'&gt;', 't':''}\n\n        elif r.g[t].sources[s][TYPE] == LinkType.HalfUncertain.value:\n            arrows[(s_node, t_node)] = {'h':'&gt;', 't':'o'}\n\n        elif r.g[t].sources[s][TYPE] == LinkType.Uncertain.value:\n            arrows[(s_node, t_node)] = {'h':'o', 't':'o'}\n\n        else:\n            raise ValueError(f\"{r.g[t].sources[s][TYPE]} not included in LinkType\")\n\n\n    def ts_dag(self,\n               min_width = 1, \n               max_width = 5,\n               min_score = 0, \n               max_score = 1,\n               node_size = 8,\n               x_disp = 2,\n               y_disp = 0.5,\n               node_color = 'orange',\n               edge_color = 'grey',\n               tail_color = 'black',\n               font_size = 8,\n               save_name = None,\n               img_extention = ImageExt.PNG):\n\"\"\"\n        Build a timeseries dag.\n\n        Args:\n            min_width (int, optional): minimum linewidth. Defaults to 1.\n            max_width (int, optional): maximum linewidth. Defaults to 5.\n            min_score (int, optional): minimum score range. Defaults to 0.\n            max_score (int, optional): maximum score range. Defaults to 1.\n            node_size (int, optional): node size. Defaults to 8.\n            x_disp (int, optional): node displacement along x. Defaults to 2.\n            y_disp (int, optional): node displacement along y. Defaults to 0.5.\n            node_color (str/list, optional): node color. \n                                             If a string, all the nodes will have the same colour. \n                                             If a list (same dimension of features), each colour will have the specified colour.\n                                             Defaults to 'orange'.\n            edge_color (str, optional): edge color. Defaults to 'grey'.\n            tail_color (str, optional): tail color. Defaults to 'black'.\n            font_size (int, optional): font size. Defaults to 12.\n            save_name (str, optional): Filename path. If None, plot is shown and not saved. Defaults to None.\n            img_extention (ImageExt, optional): Image Extension. Defaults to PNG.\n        \"\"\"\n        r = copy.deepcopy(self)\n        r.g = r.make_pretty()\n\n        G = nx.DiGraph()\n\n        # Add nodes to the graph\n        if isinstance(node_color, list):\n            node_c = dict()\n        else:\n            node_c = node_color\n        for i in range(len(self.features)):\n            for j in range(self.max_lag + 1):\n                G.add_node((j, i))\n                if isinstance(node_color, list): node_c[(j, i)] = node_color[abs(i - (len(r.g.keys()) - 1))]\n\n        pos = {n : (n[0]*x_disp, n[1]*y_disp) for n in G.nodes()}\n        scale = max(pos.values())\n\n        # edges definition\n        edges = list()\n        edge_width = dict()\n        arrows = dict()\n\n        for t in r.g:\n            for s in r.g[t].sources:\n                s_index = len(r.g.keys())-1 - list(r.g.keys()).index(s[0])\n                t_index = len(r.g.keys())-1 - list(r.g.keys()).index(t)\n\n                # Contemporaneous dependecies\n                if s[1] == 0:\n                    for i in range(self.max_lag + 1):\n                        s_node = (i, s_index)\n                        t_node = (i, t_index)\n                        self.__add_edge(min_width, max_width, min_score, max_score, \n                                        edges, edge_width, arrows, r, t, s, \n                                        s_node, t_node)\n\n                # Lagged dependecies\n                else:\n                    s_lag = self.max_lag - s[1]\n                    t_lag = self.max_lag\n                    while s_lag &gt;= 0:\n                        s_node = (s_lag, s_index)\n                        t_node = (t_lag, t_index)\n                        self.__add_edge(min_width, max_width, min_score, max_score, \n                                        edges, edge_width, arrows, r, t, s, \n                                        s_node, t_node)\n                        s_lag -= 1\n                        t_lag -= 1\n\n        G.add_edges_from(edges)\n\n        fig, ax = plt.subplots(figsize=(8,6))\n\n        # label definition\n        for n in G.nodes():\n            if n[0] == 0:\n                ax.text(pos[n][0]-0.1, pos[n][1], list(r.g.keys())[len(r.g.keys()) - 1 - n[1]], horizontalalignment='center', verticalalignment='center', fontsize=font_size)\n\n        # time line text drawing\n        pos_tau = set([pos[p][0] for p in pos])\n        max_y = max([pos[p][1] for p in pos])\n        for p in pos_tau:\n            if abs(int(p/x_disp) - self.max_lag) == 0:\n                ax.text(p, max_y + 0.1, r\"$t$\", horizontalalignment='center', fontsize=font_size)\n            else:\n                ax.text(p, max_y + 0.1, r\"$t-\" + str(abs(int(p/x_disp) - self.max_lag)) + \"$\", horizontalalignment='center', fontsize=font_size)\n\n        Graph(G,\n            node_layout = {p : np.array(pos[p]) for p in pos},\n            node_size = node_size,\n            node_color = node_c,\n            node_label_offset = 0,\n            node_edge_width = 0,\n            node_label_fontdict = dict(size=font_size),\n            node_alpha = 1,\n\n            arrows = arrows,\n            edge_layout = 'curved',\n            edge_label = False,\n            edge_color = edge_color, \n            tail_color = tail_color, \n            edge_width = edge_width,\n            edge_alpha = 1,\n            edge_zorder = 1,\n            scale = (scale[0] + 2, scale[1] + 2))\n\n        if save_name is not None:\n            plt.savefig(save_name + img_extention.value, dpi = 300)\n        else:\n            plt.show()\n\n\n    def __scale(self, score, min_width, max_width, min_score = 0, max_score = 1):\n\"\"\"\n        Scale the score of the cause-effect relationship strength to a linewitdth.\n\n        Args:\n            score (float): score to scale.\n            min_width (float): minimum linewidth.\n            max_width (float): maximum linewidth.\n            min_score (int, optional): minimum score range. Defaults to 0.\n            max_score (int, optional): maximum score range. Defaults to 1.\n\n        Returns:\n            (float): scaled score.\n        \"\"\"\n        return ((score - min_score) / (max_score - min_score)) * (max_width - min_width) + min_width\n\n\n    def get_skeleton(self) -&gt; np.array:\n\"\"\"\n        Return skeleton matrix.\n\n        Skeleton matrix is composed by 0 and 1.\n        1 &lt;- if there is a link from source to target \n        0 &lt;- if there is not a link from source to target \n\n        Returns:\n            np.array: skeleton matrix\n        \"\"\"\n        r = np.full((len(self.features), len(self.features), self.max_lag + 1), '', dtype=object)\n        for t in self.g.keys():\n            for s in self.g[t].sources:\n                r[self.features.index(t), self.features.index(s[0])][s[1]] = 1\n        return np.array(r)\n\n\n    def get_val_matrix(self) -&gt; np.array:\n\"\"\"\n        Return val matrix.\n\n        Val matrix contains information about the strength of the links componing the causal model.\n\n        Returns:\n            np.array: val matrix.\n        \"\"\"\n        r = np.zeros((len(self.features), len(self.features), self.max_lag + 1))\n        for t in self.g.keys():\n            for s, info in self.g[t].sources.items():\n                    r[self.features.index(t), self.features.index(s[0])][s[1]] = info[SCORE]\n        return np.array(r)\n\n\n    def get_pval_matrix(self) -&gt; np.array:\n\"\"\"\n        Return pval matrix.\n\n        Pval matrix contains information about the pval of the links componing the causal model.\n\n        Returns:\n            np.array: pval matrix\n        \"\"\"\n        r = np.zeros((len(self.features), len(self.features), self.max_lag + 1))\n        for t in self.g.keys():\n            for s, info in self.g[t].sources.items():\n                r[self.features.index(t), self.features.index(s[0])][s[1]] = info[PVAL]\n        return np.array(r)\n\n\n    def get_graph_matrix(self) -&gt; np.array:\n\"\"\"\n        Return graph matrix.\n\n        Graph matrix contains information about the link type. E.g., --&gt;, &lt;-&gt;, ..\n\n        Returns:\n            np.array: graph matrix.\n        \"\"\"\n        r = np.full((len(self.features), len(self.features), self.max_lag + 1), '', dtype=object)\n        for t in self.g.keys():\n            for s, info in self.g[t].sources.items():\n                r[self.features.index(t), self.features.index(s[0])][s[1]] = info[TYPE]\n        return np.array(r)\n\n\n    def get_Adj(self, indexed = False) -&gt; dict:   \n\"\"\"\n        Return Adjacency dictionary.\n\n        Args:\n            indexed (bool, optional): If true, returns the SCM with index instead of variables' names. Otherwise it uses variables' names. Defaults to False.\n\n        Returns:\n            dict: SCM.\n        \"\"\"\n        if not indexed:\n            scm = {v: list() for v in self.features}\n            for t in self.g:\n                for s in self.g[t].sources:\n                    scm[t].append((s[0], -abs(s[1]))) \n        else:\n            scm = {self.features.index(v): list() for v in self.features}\n            for t in self.g:\n                for s in self.g[t].sources:\n                    scm[self.features.index(t)].append((self.features.index(s[0]), -abs(s[1]))) \n        return scm\n\n\n    def get_Graph(self) -&gt; dict:\n\"\"\"\n        Return Graph dictionary. E.g. {X1: {(X2, -2): '--&gt;'}, X2: {(X3, -1): '-?&gt;'}, X3: {(X3, -1): '--&gt;'}}.\n\n        Returns:\n            dict: graph dictionary.\n        \"\"\"\n        scm = {v: dict() for v in self.features}\n        for t in self.g:\n            for s in self.g[t].sources:\n                scm[t][(s[0], -abs(s[1]))] = self.g[t].sources[s][TYPE] \n        return scm\n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG.autodep_nodes","title":"<code>autodep_nodes: list</code>  <code>property</code>","text":"<p>Return the autodependent nodes list.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Autodependent nodes list.</p>"},{"location":"graph/#causalflow.graph.DAG.DAG.features","title":"<code>features: list</code>  <code>property</code>","text":"<p>Return features list.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Features list.</p>"},{"location":"graph/#causalflow.graph.DAG.DAG.interventions_links","title":"<code>interventions_links: list</code>  <code>property</code>","text":"<p>Return the intervention links list.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Intervention link list.</p>"},{"location":"graph/#causalflow.graph.DAG.DAG.pretty_features","title":"<code>pretty_features</code>  <code>property</code>","text":"<p>Return list of features with LaTeX symbols.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>str</code> <p>list of feature names.</p>"},{"location":"graph/#causalflow.graph.DAG.DAG.__init__","title":"<code>__init__(var_names, min_lag, max_lag, neglect_autodep=False, scm=None)</code>","text":"<p>DAG constructor.</p> <p>Parameters:</p> Name Type Description Default <code>var_names</code> <code>list</code> <p>variable list.</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag.</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag.</p> required <code>neglect_autodep</code> <code>bool</code> <p>bit to neglect nodes when they are only autodependent. Defaults to False.</p> <code>False</code> <code>scm</code> <code>dict</code> <p>Build the DAG for SCM. Defaults to None.</p> <code>None</code> Source code in <code>causalflow/graph/DAG.py</code> <pre><code>def __init__(self, var_names, min_lag, max_lag, neglect_autodep = False, scm = None):\n\"\"\"\n    DAG constructor.\n\n    Args:\n        var_names (list): variable list.\n        min_lag (int): minimum time lag.\n        max_lag (int): maximum time lag.\n        neglect_autodep (bool, optional): bit to neglect nodes when they are only autodependent. Defaults to False.\n        scm (dict, optional): Build the DAG for SCM. Defaults to None.\n    \"\"\"\n    self.g = {var: Node(var, neglect_autodep) for var in var_names}\n    self.neglect_autodep = neglect_autodep\n    self.sys_context = dict()\n    self.min_lag = min_lag\n    self.max_lag = max_lag\n\n    if scm is not None:\n        for t in scm:\n                for s in scm[t]: \n                    if len(s) == 2:\n                        self.add_source(t, s[0], 0.3, 0, s[1])\n                    elif len(s) == 3:\n                        self.add_source(t, s[0], 0.3, 0, s[1], s[2])\n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG.__scale","title":"<code>__scale(score, min_width, max_width, min_score=0, max_score=1)</code>","text":"<p>Scale the score of the cause-effect relationship strength to a linewitdth.</p> <p>Parameters:</p> Name Type Description Default <code>score</code> <code>float</code> <p>score to scale.</p> required <code>min_width</code> <code>float</code> <p>minimum linewidth.</p> required <code>max_width</code> <code>float</code> <p>maximum linewidth.</p> required <code>min_score</code> <code>int</code> <p>minimum score range. Defaults to 0.</p> <code>0</code> <code>max_score</code> <code>int</code> <p>maximum score range. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>float</code> <p>scaled score.</p> Source code in <code>causalflow/graph/DAG.py</code> <pre><code>def __scale(self, score, min_width, max_width, min_score = 0, max_score = 1):\n\"\"\"\n    Scale the score of the cause-effect relationship strength to a linewitdth.\n\n    Args:\n        score (float): score to scale.\n        min_width (float): minimum linewidth.\n        max_width (float): maximum linewidth.\n        min_score (int, optional): minimum score range. Defaults to 0.\n        max_score (int, optional): maximum score range. Defaults to 1.\n\n    Returns:\n        (float): scaled score.\n    \"\"\"\n    return ((score - min_score) / (max_score - min_score)) * (max_width - min_width) + min_width\n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG.add_context","title":"<code>add_context()</code>","text":"<p>Add context variables.</p> Source code in <code>causalflow/graph/DAG.py</code> <pre><code>def add_context(self):\n\"\"\"Add context variables.\"\"\"\n    for sys_var, context_var in self.sys_context.items():\n        if sys_var in self.features:\n\n            # Adding context var to the graph\n            self.g[context_var] = Node(context_var, self.neglect_autodep)\n\n            # Adding context var to sys var\n            self.g[sys_var].intervention_node = True\n            self.g[sys_var].associated_context = context_var\n            self.add_source(sys_var, context_var, 1, 0, 0)\n\n    # NOTE: bi-directed link contemporanous link between context vars\n    for sys_var, context_var in self.sys_context.items():\n        if sys_var in self.features:\n            other_context = [value for value in self.sys_context.values() if value != context_var and value in self.features]\n            for other in other_context: self.add_source(context_var, other, 1, 0, 0)\n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG.add_source","title":"<code>add_source(t, s, score, pval, lag, mode=LinkType.Directed.value)</code>","text":"<p>Add source node to a target node.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>str</code> <p>target node name.</p> required <code>s</code> <code>str</code> <p>source node name.</p> required <code>score</code> <code>float</code> <p>dependency score.</p> required <code>pval</code> <code>float</code> <p>dependency p-value.</p> required <code>lag</code> <code>int</code> <p>dependency lag.</p> required Source code in <code>causalflow/graph/DAG.py</code> <pre><code>def add_source(self, t, s, score, pval, lag, mode = LinkType.Directed.value):\n\"\"\"\n    Add source node to a target node.\n\n    Args:\n        t (str): target node name.\n        s (str): source node name.\n        score (float): dependency score.\n        pval (float): dependency p-value.\n        lag (int): dependency lag.\n    \"\"\"\n    self.g[t].sources[(s, abs(lag))] = {SCORE: score, PVAL: pval, TYPE: mode}\n    self.g[s].children.append(t)\n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG.dag","title":"<code>dag(node_layout='dot', min_width=1, max_width=5, min_score=0, max_score=1, node_size=8, node_color='orange', edge_color='grey', bundle_parallel_edges=True, font_size=12, label_type=LabelType.Lag, save_name=None, img_extention=ImageExt.PNG)</code>","text":"<p>Build a dag.</p> <p>Parameters:</p> Name Type Description Default <code>node_layout</code> <code>str</code> <p>Node layout. Defaults to 'dot'.</p> <code>'dot'</code> <code>min_width</code> <code>int</code> <p>minimum linewidth. Defaults to 1.</p> <code>1</code> <code>max_width</code> <code>int</code> <p>maximum linewidth. Defaults to 5.</p> <code>5</code> <code>min_score</code> <code>int</code> <p>minimum score range. Defaults to 0.</p> <code>0</code> <code>max_score</code> <code>int</code> <p>maximum score range. Defaults to 1.</p> <code>1</code> <code>node_size</code> <code>int</code> <p>node size. Defaults to 8.</p> <code>8</code> <code>node_color</code> <code>str</code> <p>node color. Defaults to 'orange'.</p> <code>'orange'</code> <code>edge_color</code> <code>str</code> <p>edge color. Defaults to 'grey'.</p> <code>'grey'</code> <code>bundle_parallel_edges</code> <code>str</code> <p>bundle parallel edge bit. Defaults to True.</p> <code>True</code> <code>font_size</code> <code>int</code> <p>font size. Defaults to 12.</p> <code>12</code> <code>label_type</code> <code>LabelType</code> <p>enum to set whether to show the lag time (LabelType.Lag) or the strength (LabelType.Score) of the dependencies on each link/node or not showing the labels (LabelType.NoLabels). Default LabelType.Lag.</p> <code>LabelType.Lag</code> <code>save_name</code> <code>str</code> <p>Filename path. If None, plot is shown and not saved. Defaults to None.</p> <code>None</code> <code>img_extention</code> <code>ImageExt</code> <p>Image Extension. Defaults to PNG.</p> <code>ImageExt.PNG</code> Source code in <code>causalflow/graph/DAG.py</code> <pre><code>def dag(self,\n        node_layout = 'dot',\n        min_width = 1, \n        max_width = 5,\n        min_score = 0, \n        max_score = 1,\n        node_size = 8, \n        node_color = 'orange',\n        edge_color = 'grey',\n        bundle_parallel_edges = True,\n        font_size = 12,\n        label_type = LabelType.Lag,\n        save_name = None,\n        img_extention = ImageExt.PNG):\n\"\"\"\n    Build a dag.\n\n    Args:\n        node_layout (str, optional): Node layout. Defaults to 'dot'.\n        min_width (int, optional): minimum linewidth. Defaults to 1.\n        max_width (int, optional): maximum linewidth. Defaults to 5.\n        min_score (int, optional): minimum score range. Defaults to 0.\n        max_score (int, optional): maximum score range. Defaults to 1.\n        node_size (int, optional): node size. Defaults to 8.\n        node_color (str, optional): node color. Defaults to 'orange'.\n        edge_color (str, optional): edge color. Defaults to 'grey'.\n        bundle_parallel_edges (str, optional): bundle parallel edge bit. Defaults to True.\n        font_size (int, optional): font size. Defaults to 12.\n        label_type (LabelType, optional): enum to set whether to show the lag time (LabelType.Lag) or the strength (LabelType.Score) of the dependencies on each link/node or not showing the labels (LabelType.NoLabels). Default LabelType.Lag.\n        save_name (str, optional): Filename path. If None, plot is shown and not saved. Defaults to None.\n        img_extention (ImageExt, optional): Image Extension. Defaults to PNG.\n    \"\"\"\n    r = copy.deepcopy(self)\n    r.g = r.make_pretty()\n\n    G = nx.DiGraph()\n\n    # NODES DEFINITION\n    G.add_nodes_from(r.g.keys())\n\n    # BORDER LINE\n    border = dict()\n    for t in r.g:\n        border[t] = 0\n        if r.g[t].is_autodependent:\n            autodep = r.g[t].get_max_autodependent\n            border[t] = max(self.__scale(r.g[t].sources[autodep][SCORE], min_width, max_width, min_score, max_score), border[t])\n\n    # BORDER LABEL\n    node_label = None\n    if label_type == LabelType.Lag or label_type == LabelType.Score:\n        node_label = {t: [] for t in r.g.keys()}\n        for t in r.g:\n            if r.g[t].is_autodependent:\n                autodep = r.g[t].get_max_autodependent\n                if label_type == LabelType.Lag:\n                    node_label[t].append(autodep[1])\n                elif label_type == LabelType.Score:\n                    node_label[t].append(round(r.g[t].sources[autodep][SCORE], 3))\n            node_label[t] = \",\".join(str(s) for s in node_label[t])\n\n    # EDGE DEFINITION\n    edges = list()\n    edge_width = dict()\n    arrows = {}\n    for t in r.g:\n        for s in r.g[t].sources:\n            if t != s[0]:\n                self.__add_edge(min_width, max_width, min_score, max_score, \n                                edges, edge_width, arrows, r, t, s, \n                                s[0], t)\n\n    G.add_edges_from(edges)\n\n    # EDGE LABEL\n    edge_label = None\n    if label_type == LabelType.Lag or label_type == LabelType.Score:\n        edge_label = {(s[0], t): [] for t in r.g for s in r.g[t].sources if t != s[0]}\n        for t in r.g:\n            for s in r.g[t].sources:\n                if t != s[0]:\n                    if label_type == LabelType.Lag:\n                        edge_label[(s[0], t)].append(s[1])\n                    elif label_type == LabelType.Score:\n                        edge_label[(s[0], t)].append(round(r.g[t].sources[s][SCORE], 3))\n        for k in edge_label.keys():\n            edge_label[k] = \",\".join(str(s) for s in edge_label[k])\n\n    fig, ax = plt.subplots(figsize=(8,6))\n\n    if edges:\n        a = Graph(G, \n                node_layout = node_layout,\n                node_size = node_size,\n                node_color = node_color,\n                node_labels = node_label,\n                node_edge_width = border,\n                node_label_fontdict = dict(size=font_size),\n                node_edge_color = edge_color,\n                node_label_offset = 0.1,\n                node_alpha = 1,\n\n                arrows = arrows,\n                edge_layout = 'arc',\n                edge_label = label_type != LabelType.NoLabels,\n                edge_labels = edge_label,\n                edge_label_fontdict = dict(size=font_size),\n                edge_color = edge_color, \n                tail_color = 'white', \n                edge_width = edge_width,\n                edge_alpha = 1,\n                edge_zorder = 1,\n                edge_label_position = 0.35,\n                edge_layout_kwargs = dict(bundle_parallel_edges = bundle_parallel_edges, k = 0.05))\n\n        nx.draw_networkx_labels(G, \n                                pos = a.node_positions,\n                                labels = {n: n for n in G},\n                                font_size = font_size)\n\n    if save_name is not None:\n        plt.savefig(save_name + img_extention.value, dpi = 300)\n    else:\n        plt.show()\n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG.del_source","title":"<code>del_source(t, s, lag)</code>","text":"<p>Remove source node from a target node.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>str</code> <p>target node name.</p> required <code>s</code> <code>str</code> <p>source node name.</p> required <code>lag</code> <code>int</code> <p>dependency lag.</p> required Source code in <code>causalflow/graph/DAG.py</code> <pre><code>def del_source(self, t, s, lag):\n\"\"\"\n    Remove source node from a target node.\n\n    Args:\n        t (str): target node name.\n        s (str): source node name.\n        lag (int): dependency lag.\n    \"\"\"\n    del self.g[t].sources[(s, lag)]\n    self.g[s].children.remove(t)\n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG.get_Adj","title":"<code>get_Adj(indexed=False)</code>","text":"<p>Return Adjacency dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>indexed</code> <code>bool</code> <p>If true, returns the SCM with index instead of variables' names. Otherwise it uses variables' names. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>SCM.</p> Source code in <code>causalflow/graph/DAG.py</code> <pre><code>def get_Adj(self, indexed = False) -&gt; dict:   \n\"\"\"\n    Return Adjacency dictionary.\n\n    Args:\n        indexed (bool, optional): If true, returns the SCM with index instead of variables' names. Otherwise it uses variables' names. Defaults to False.\n\n    Returns:\n        dict: SCM.\n    \"\"\"\n    if not indexed:\n        scm = {v: list() for v in self.features}\n        for t in self.g:\n            for s in self.g[t].sources:\n                scm[t].append((s[0], -abs(s[1]))) \n    else:\n        scm = {self.features.index(v): list() for v in self.features}\n        for t in self.g:\n            for s in self.g[t].sources:\n                scm[self.features.index(t)].append((self.features.index(s[0]), -abs(s[1]))) \n    return scm\n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG.get_Graph","title":"<code>get_Graph()</code>","text":"<p>Return Graph dictionary. E.g. {X1: {(X2, -2): '--&gt;'}, X2: {(X3, -1): '-?&gt;'}, X3: {(X3, -1): '--&gt;'}}.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>graph dictionary.</p> Source code in <code>causalflow/graph/DAG.py</code> <pre><code>def get_Graph(self) -&gt; dict:\n\"\"\"\n    Return Graph dictionary. E.g. {X1: {(X2, -2): '--&gt;'}, X2: {(X3, -1): '-?&gt;'}, X3: {(X3, -1): '--&gt;'}}.\n\n    Returns:\n        dict: graph dictionary.\n    \"\"\"\n    scm = {v: dict() for v in self.features}\n    for t in self.g:\n        for s in self.g[t].sources:\n            scm[t][(s[0], -abs(s[1]))] = self.g[t].sources[s][TYPE] \n    return scm\n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG.get_graph_matrix","title":"<code>get_graph_matrix()</code>","text":"<p>Return graph matrix.</p> <p>Graph matrix contains information about the link type. E.g., --&gt;, &lt;-&gt;, ..</p> <p>Returns:</p> Type Description <code>np.array</code> <p>np.array: graph matrix.</p> Source code in <code>causalflow/graph/DAG.py</code> <pre><code>def get_graph_matrix(self) -&gt; np.array:\n\"\"\"\n    Return graph matrix.\n\n    Graph matrix contains information about the link type. E.g., --&gt;, &lt;-&gt;, ..\n\n    Returns:\n        np.array: graph matrix.\n    \"\"\"\n    r = np.full((len(self.features), len(self.features), self.max_lag + 1), '', dtype=object)\n    for t in self.g.keys():\n        for s, info in self.g[t].sources.items():\n            r[self.features.index(t), self.features.index(s[0])][s[1]] = info[TYPE]\n    return np.array(r)\n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG.get_link_assumptions","title":"<code>get_link_assumptions(autodep_ok=False)</code>","text":"<p>Return link assumption dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>autodep_ok</code> <code>bool</code> <p>If true, autodependecy link assumption = --&gt;. Otherwise -?&gt;. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>link assumption dictionary.</p> Source code in <code>causalflow/graph/DAG.py</code> <pre><code>def get_link_assumptions(self, autodep_ok = False) -&gt; dict:\n\"\"\"\n    Return link assumption dictionary.\n\n    Args:\n        autodep_ok (bool, optional): If true, autodependecy link assumption = --&gt;. Otherwise -?&gt;. Defaults to False.\n\n    Returns:\n        dict: link assumption dictionary.\n    \"\"\"\n    link_assump = {self.features.index(f): dict() for f in self.features}\n    for t in self.g:\n        for s in self.g[t].sources:\n            if autodep_ok and s[0] == t: # NOTE: new condition added in order to not control twice the autodependency links\n                link_assump[self.features.index(t)][(self.features.index(s[0]), -abs(s[1]))] = '--&gt;'\n\n            elif s[0] not in list(self.sys_context.values()):\n                if s[1] == 0 and (t, 0) in self.g[s[0]].sources:\n                    link_assump[self.features.index(t)][(self.features.index(s[0]), 0)] = 'o-o'\n                elif s[1] == 0 and (t, 0) not in self.g[s[0]].sources:\n                    link_assump[self.features.index(t)][(self.features.index(s[0]),0)] = '-?&gt;'\n                    link_assump[self.features.index(s[0])][(self.features.index(t), 0)] = '&lt;?-'\n                elif s[1] &gt; 0:\n                    link_assump[self.features.index(t)][(self.features.index(s[0]), -abs(s[1]))] = '-?&gt;'\n\n            elif t in self.sys_context.keys() and s[0] == self.sys_context[t]:\n                link_assump[self.features.index(t)][(self.features.index(s[0]), -abs(s[1]))] = '--&gt;'\n\n    return link_assump\n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG.get_pval_matrix","title":"<code>get_pval_matrix()</code>","text":"<p>Return pval matrix.</p> <p>Pval matrix contains information about the pval of the links componing the causal model.</p> <p>Returns:</p> Type Description <code>np.array</code> <p>np.array: pval matrix</p> Source code in <code>causalflow/graph/DAG.py</code> <pre><code>def get_pval_matrix(self) -&gt; np.array:\n\"\"\"\n    Return pval matrix.\n\n    Pval matrix contains information about the pval of the links componing the causal model.\n\n    Returns:\n        np.array: pval matrix\n    \"\"\"\n    r = np.zeros((len(self.features), len(self.features), self.max_lag + 1))\n    for t in self.g.keys():\n        for s, info in self.g[t].sources.items():\n            r[self.features.index(t), self.features.index(s[0])][s[1]] = info[PVAL]\n    return np.array(r)\n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG.get_skeleton","title":"<code>get_skeleton()</code>","text":"<p>Return skeleton matrix.</p> <p>Skeleton matrix is composed by 0 and 1. 1 &lt;- if there is a link from source to target  0 &lt;- if there is not a link from source to target </p> <p>Returns:</p> Type Description <code>np.array</code> <p>np.array: skeleton matrix</p> Source code in <code>causalflow/graph/DAG.py</code> <pre><code>def get_skeleton(self) -&gt; np.array:\n\"\"\"\n    Return skeleton matrix.\n\n    Skeleton matrix is composed by 0 and 1.\n    1 &lt;- if there is a link from source to target \n    0 &lt;- if there is not a link from source to target \n\n    Returns:\n        np.array: skeleton matrix\n    \"\"\"\n    r = np.full((len(self.features), len(self.features), self.max_lag + 1), '', dtype=object)\n    for t in self.g.keys():\n        for s in self.g[t].sources:\n            r[self.features.index(t), self.features.index(s[0])][s[1]] = 1\n    return np.array(r)\n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG.get_val_matrix","title":"<code>get_val_matrix()</code>","text":"<p>Return val matrix.</p> <p>Val matrix contains information about the strength of the links componing the causal model.</p> <p>Returns:</p> Type Description <code>np.array</code> <p>np.array: val matrix.</p> Source code in <code>causalflow/graph/DAG.py</code> <pre><code>def get_val_matrix(self) -&gt; np.array:\n\"\"\"\n    Return val matrix.\n\n    Val matrix contains information about the strength of the links componing the causal model.\n\n    Returns:\n        np.array: val matrix.\n    \"\"\"\n    r = np.zeros((len(self.features), len(self.features), self.max_lag + 1))\n    for t in self.g.keys():\n        for s, info in self.g[t].sources.items():\n                r[self.features.index(t), self.features.index(s[0])][s[1]] = info[SCORE]\n    return np.array(r)\n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG.make_pretty","title":"<code>make_pretty()</code>","text":"<p>Make variables' names pretty, i.e. $ varname $ with '{' after '_' and '}' at the end of the string.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>pretty DAG.</p> Source code in <code>causalflow/graph/DAG.py</code> <pre><code>def make_pretty(self) -&gt; dict:\n\"\"\"\n    Make variables' names pretty, i.e. $ varname $ with '{' after '_' and '}' at the end of the string.\n\n    Returns:\n        dict: pretty DAG.\n    \"\"\"\n    def prettify(name):\n        return '$' + re.sub(r'_(\\w+)', r'_{\\1}', name) + '$'\n\n    pretty = dict()\n    for t in self.g:\n        p_t = prettify(t)\n        pretty[p_t] = copy.deepcopy(self.g[t])\n        pretty[p_t].name = p_t\n        pretty[p_t].children = [prettify(c) for c in self.g[t].children]\n        for s in self.g[t].sources:\n            del pretty[p_t].sources[s]\n            p_s = prettify(s[0])\n            pretty[p_t].sources[(p_s, s[1])] = {\n                SCORE: self.g[t].sources[s][SCORE],\n                PVAL: self.g[t].sources[s][PVAL],\n                TYPE: self.g[t].sources[s][TYPE]\n            }\n    return pretty\n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG.remove_context","title":"<code>remove_context()</code>","text":"<p>Remove context variables.</p> Source code in <code>causalflow/graph/DAG.py</code> <pre><code>def remove_context(self):\n\"\"\"Remove context variables.\"\"\"\n    for sys_var, context_var in self.sys_context.items():\n        if sys_var in self.g:\n            # Removing context var from sys var\n            # self.g[sys_var].intervention_node = False\n            self.g[sys_var].associated_context = None\n            self.del_source(sys_var, context_var, 0)\n\n            # Removing context var from dag\n            del self.g[context_var]\n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG.remove_unneeded_features","title":"<code>remove_unneeded_features()</code>","text":"<p>Remove isolated nodes.</p> Source code in <code>causalflow/graph/DAG.py</code> <pre><code>def remove_unneeded_features(self):\n\"\"\"Remove isolated nodes.\"\"\"\n    tmp = copy.deepcopy(self.g)\n    for t in self.g.keys():\n        if self.g[t].is_isolated: \n            if self.g[t].intervention_node: del tmp[self.g[t].associated_context]\n            del tmp[t]\n    self.g = tmp\n</code></pre>"},{"location":"graph/#causalflow.graph.DAG.DAG.ts_dag","title":"<code>ts_dag(min_width=1, max_width=5, min_score=0, max_score=1, node_size=8, x_disp=2, y_disp=0.5, node_color='orange', edge_color='grey', tail_color='black', font_size=8, save_name=None, img_extention=ImageExt.PNG)</code>","text":"<p>Build a timeseries dag.</p> <p>Parameters:</p> Name Type Description Default <code>min_width</code> <code>int</code> <p>minimum linewidth. Defaults to 1.</p> <code>1</code> <code>max_width</code> <code>int</code> <p>maximum linewidth. Defaults to 5.</p> <code>5</code> <code>min_score</code> <code>int</code> <p>minimum score range. Defaults to 0.</p> <code>0</code> <code>max_score</code> <code>int</code> <p>maximum score range. Defaults to 1.</p> <code>1</code> <code>node_size</code> <code>int</code> <p>node size. Defaults to 8.</p> <code>8</code> <code>x_disp</code> <code>int</code> <p>node displacement along x. Defaults to 2.</p> <code>2</code> <code>y_disp</code> <code>int</code> <p>node displacement along y. Defaults to 0.5.</p> <code>0.5</code> <code>node_color</code> <code>str / list</code> <p>node color.                               If a string, all the nodes will have the same colour.                               If a list (same dimension of features), each colour will have the specified colour.                              Defaults to 'orange'.</p> <code>'orange'</code> <code>edge_color</code> <code>str</code> <p>edge color. Defaults to 'grey'.</p> <code>'grey'</code> <code>tail_color</code> <code>str</code> <p>tail color. Defaults to 'black'.</p> <code>'black'</code> <code>font_size</code> <code>int</code> <p>font size. Defaults to 12.</p> <code>8</code> <code>save_name</code> <code>str</code> <p>Filename path. If None, plot is shown and not saved. Defaults to None.</p> <code>None</code> <code>img_extention</code> <code>ImageExt</code> <p>Image Extension. Defaults to PNG.</p> <code>ImageExt.PNG</code> Source code in <code>causalflow/graph/DAG.py</code> <pre><code>def ts_dag(self,\n           min_width = 1, \n           max_width = 5,\n           min_score = 0, \n           max_score = 1,\n           node_size = 8,\n           x_disp = 2,\n           y_disp = 0.5,\n           node_color = 'orange',\n           edge_color = 'grey',\n           tail_color = 'black',\n           font_size = 8,\n           save_name = None,\n           img_extention = ImageExt.PNG):\n\"\"\"\n    Build a timeseries dag.\n\n    Args:\n        min_width (int, optional): minimum linewidth. Defaults to 1.\n        max_width (int, optional): maximum linewidth. Defaults to 5.\n        min_score (int, optional): minimum score range. Defaults to 0.\n        max_score (int, optional): maximum score range. Defaults to 1.\n        node_size (int, optional): node size. Defaults to 8.\n        x_disp (int, optional): node displacement along x. Defaults to 2.\n        y_disp (int, optional): node displacement along y. Defaults to 0.5.\n        node_color (str/list, optional): node color. \n                                         If a string, all the nodes will have the same colour. \n                                         If a list (same dimension of features), each colour will have the specified colour.\n                                         Defaults to 'orange'.\n        edge_color (str, optional): edge color. Defaults to 'grey'.\n        tail_color (str, optional): tail color. Defaults to 'black'.\n        font_size (int, optional): font size. Defaults to 12.\n        save_name (str, optional): Filename path. If None, plot is shown and not saved. Defaults to None.\n        img_extention (ImageExt, optional): Image Extension. Defaults to PNG.\n    \"\"\"\n    r = copy.deepcopy(self)\n    r.g = r.make_pretty()\n\n    G = nx.DiGraph()\n\n    # Add nodes to the graph\n    if isinstance(node_color, list):\n        node_c = dict()\n    else:\n        node_c = node_color\n    for i in range(len(self.features)):\n        for j in range(self.max_lag + 1):\n            G.add_node((j, i))\n            if isinstance(node_color, list): node_c[(j, i)] = node_color[abs(i - (len(r.g.keys()) - 1))]\n\n    pos = {n : (n[0]*x_disp, n[1]*y_disp) for n in G.nodes()}\n    scale = max(pos.values())\n\n    # edges definition\n    edges = list()\n    edge_width = dict()\n    arrows = dict()\n\n    for t in r.g:\n        for s in r.g[t].sources:\n            s_index = len(r.g.keys())-1 - list(r.g.keys()).index(s[0])\n            t_index = len(r.g.keys())-1 - list(r.g.keys()).index(t)\n\n            # Contemporaneous dependecies\n            if s[1] == 0:\n                for i in range(self.max_lag + 1):\n                    s_node = (i, s_index)\n                    t_node = (i, t_index)\n                    self.__add_edge(min_width, max_width, min_score, max_score, \n                                    edges, edge_width, arrows, r, t, s, \n                                    s_node, t_node)\n\n            # Lagged dependecies\n            else:\n                s_lag = self.max_lag - s[1]\n                t_lag = self.max_lag\n                while s_lag &gt;= 0:\n                    s_node = (s_lag, s_index)\n                    t_node = (t_lag, t_index)\n                    self.__add_edge(min_width, max_width, min_score, max_score, \n                                    edges, edge_width, arrows, r, t, s, \n                                    s_node, t_node)\n                    s_lag -= 1\n                    t_lag -= 1\n\n    G.add_edges_from(edges)\n\n    fig, ax = plt.subplots(figsize=(8,6))\n\n    # label definition\n    for n in G.nodes():\n        if n[0] == 0:\n            ax.text(pos[n][0]-0.1, pos[n][1], list(r.g.keys())[len(r.g.keys()) - 1 - n[1]], horizontalalignment='center', verticalalignment='center', fontsize=font_size)\n\n    # time line text drawing\n    pos_tau = set([pos[p][0] for p in pos])\n    max_y = max([pos[p][1] for p in pos])\n    for p in pos_tau:\n        if abs(int(p/x_disp) - self.max_lag) == 0:\n            ax.text(p, max_y + 0.1, r\"$t$\", horizontalalignment='center', fontsize=font_size)\n        else:\n            ax.text(p, max_y + 0.1, r\"$t-\" + str(abs(int(p/x_disp) - self.max_lag)) + \"$\", horizontalalignment='center', fontsize=font_size)\n\n    Graph(G,\n        node_layout = {p : np.array(pos[p]) for p in pos},\n        node_size = node_size,\n        node_color = node_c,\n        node_label_offset = 0,\n        node_edge_width = 0,\n        node_label_fontdict = dict(size=font_size),\n        node_alpha = 1,\n\n        arrows = arrows,\n        edge_layout = 'curved',\n        edge_label = False,\n        edge_color = edge_color, \n        tail_color = tail_color, \n        edge_width = edge_width,\n        edge_alpha = 1,\n        edge_zorder = 1,\n        scale = (scale[0] + 2, scale[1] + 2))\n\n    if save_name is not None:\n        plt.savefig(save_name + img_extention.value, dpi = 300)\n    else:\n        plt.show()\n</code></pre>"},{"location":"graph/#causalflow.graph.PAG.PAG","title":"<code>PAG</code>","text":"<p>PAG class.</p> Source code in <code>causalflow/graph/PAG.py</code> <pre><code>class PAG():\n\"\"\"PAG class.\"\"\"\n\n    def __init__(self, dag, tau_max, latents) -&gt; None:\n\"\"\"\n        Class constructor.\n\n        Args:\n            dag (DAG): DAG to convert.\n            tau_max (int): max time lag.\n            latents (list[str]): list of latent variables.\n\n        Raises:\n            ValueError: latent must be a string\n        \"\"\"\n        if not isinstance(latents, list): raise ValueError('latents must be a list')\n        self.link_assumptions = dag\n        self.tau_max = tau_max\n        self.latents = latents\n        self.dSepSets = {}\n\n        self.tsDAG = self.createDAG(self.link_assumptions, self.tau_max)\n\n        self.pag = self.tsDAG2tsDPAG()\n\n\n    def convert2Graph(self) -&gt; dict:\n\"\"\"\n        Convert a PAG to a graph representation.\n\n        Returns:\n            dict: Graph representation of a PAG.\n        \"\"\"\n        out = {t: {} for t in self.pag}\n        for t in self.pag:\n            for s in self.pag[t]:\n                out[t][(s[0], s[1])] = s[2]\n        return out\n\n\n    @staticmethod\n    def createDAG(link_assumptions, tau_max) -&gt; BayesianNetwork:\n\"\"\"\n        Create a DAG represented by a Baysian Network.\n\n        Args:\n            link_assumptions (dict): DAG link assumptions.\n            tau_max (int): max time lag.\n\n        Raises:\n            ValueError: source not well defined.\n\n        Returns:\n            BayesianNetwork: DAG represented by a Baysian Network.\n        \"\"\"\n        BN = BayesianNetwork()\n        BN.add_nodes_from([(t, -l) for t in link_assumptions.keys() for l in range(0, tau_max)])\n\n        # Edges\n        edges = []\n        for t in link_assumptions.keys():\n            for source in link_assumptions[t]:\n                if len(source) == 0: continue\n                elif len(source) == 2: s, l = source\n                elif len(source) == 3: s, l, _ = source\n                else: raise ValueError(\"Source not well defined\")\n                edges.append(((s, l), (t, 0)))\n                # Add edges across time slices from -1 to -tau_max\n                for lag in range(1, tau_max + 1):\n                    if l - lag &gt;= -tau_max:\n                        edges.append(((s, l - lag), (t, -lag)))\n        BN.add_edges_from(edges)\n        return BN\n\n\n    def alreadyChecked(self, source, target):\n\"\"\"\n        Check if a link has been already checked.\n\n        Args:\n            source (str): source node\n            target (str): target node\n\n        Returns:\n            (bool, tuple): tuple containing if the link has been checked and, if so, their separation set. Otherwise None.\n        \"\"\"\n        if (source, target) in self.dSepSets: return True, self.dSepSets[(source, target)]\n        elif (target, source) in self.dSepSets: return True, self.dSepSets[(target, source)]\n        elif ((source[0], source[1] - target[1]), (target[0], 0)) in self.dSepSets: return True, self.dSepSets[((source[0], source[1] - target[1]), (target[0], 0))]\n        elif ((target[0], target[1] - source[1]), (source[0], 0)) in self.dSepSets: return True, self.dSepSets[((target[0], target[1] - source[1]), (source[0], 0))]\n        return False, None\n\n\n    def tsDAG2tsDPAG(self) -&gt; dict:\n\"\"\"\n        Convert a DAG to a Time-series DPAG.\n\n        Returns:\n            dict: Time-series DPAG.\n        \"\"\"\n        self.tsDPAG = {t: [(s[0], s[1], '--&gt;') for s in self.link_assumptions[t] if s[0] not in self.latents] for t in self.link_assumptions.keys() if t not in self.latents}\n\n        if len(self.latents) &gt; 0:\n            self.ambiguous_links = []\n\n            # Separate nodes based on their time index\n            time_zero_nodes = []\n            other_nodes = []\n\n            for node in self.tsDAG.nodes():\n                if node[1] == 0:\n                    time_zero_nodes.append(node)\n                else:\n                    other_nodes.append(node)\n\n            for target in time_zero_nodes + other_nodes:\n                print(f\"Analysing target: {target}\")\n                if target[0] in self.latents: continue\n                tmp = []\n                for n in list(self.tsDAG.nodes()):\n                    if n[0] != target[0] or n[1] != target[1]:\n                        tmp.append(n)\n                for p in list(self.tsDAG.predecessors(target)) + list(self.tsDAG.successors(target)):\n                    if p in tmp: tmp.remove(p)\n                for source in tmp:\n                    alreadyChecked, d_sep = self.alreadyChecked(source, target)\n                    if alreadyChecked:\n                        print(f\"\\t- {source} \u22a5 {target} | {d_sep} ALREADY CHECKED\")\n                    else:\n                        areDsep, d_sep = self.find_d_separators(source, target, self.latents)\n                        self.dSepSets[(source, target)] = d_sep\n                        print(f\"\\t- {source} \u22a5 {target} | {d_sep}\")\n                    if areDsep and any(node[0] in self.latents for node in d_sep):\n                        if source[0] in self.latents or target[0] in self.latents: continue\n                        if target[1] == 0:\n                            print(f\"\\t- SPURIOUS LINK: ({source[0]}, {source[1]}) o-o ({target[0]}, {target[1]})\")\n                            if (source[0], source[1], 'o-o') not in self.tsDPAG[target[0]]: self.tsDPAG[target[0]].append((source[0], source[1], 'o-o'))\n                            if (source, target, 'o-o') not in self.ambiguous_links: self.ambiguous_links.append((source, target, 'o-o'))\n                        elif source[1] == 0:\n                            print(f\"\\t- SPURIOUS LINK: ({target[0]}, {target[1]}) o-o ({source[0]}, {source[1]})\")\n                            if (target[0], target[1], 'o-o') not in self.tsDPAG[source[0]]: self.tsDPAG[source[0]].append((target[0], target[1], 'o-o'))\n                            if (target, source, 'o-o') not in self.ambiguous_links: self.ambiguous_links.append((target, source, 'o-o'))\n\n\n            print(f\"--------------------------------------------------\")\n            print(f\"    Bidirected link due to latent confounders     \")\n            print(f\"--------------------------------------------------\")\n            # *(1) Bidirected link between variables confounded by a latent variable  \n            # *    if a link between them does not exist already\n            confounders = self.find_latent_confounders()\n            for confounded in copy.deepcopy(list(confounders.values())):\n                for c1 in copy.deepcopy(confounded):\n                    tmp = copy.deepcopy(confounded)\n                    tmp.remove(c1)\n                    for c2 in tmp:\n                        if (c1, c2, 'o-o') in self.ambiguous_links:\n                            self.update_link_type(c1, c2, '&lt;-&gt;')\n                            self.ambiguous_links.remove((c1, c2, 'o-o'))\n                            print(f\"\\t- SPURIOUS LINK REMOVED: {c1} o-o {c2}\")\n                        elif (c2, c1, 'o-o') in self.ambiguous_links:\n                            self.update_link_type(c1, c2, '&lt;-&gt;')\n                            self.ambiguous_links.remove((c2, c1, 'o-o'))\n                            print(f\"\\t- SPURIOUS LINK REMOVED: {c2} o-o {c1}\")\n                    confounded.remove(c1)\n\n            print(f\"--------------------------------------------------\")\n            print(f\"              Collider orientation                \")\n            print(f\"--------------------------------------------------\")\n            # *(2) Identify and orient the colliders:\n            # *    for any path X \u2013 Z \u2013 Y where there is no edge between\n            # *    X and Y and, Z was never included in the conditioning set ==&gt; X \u2192 Z \u2190 Y collider\n            colliders = self.find_colliders()\n            for ambiguous_link in copy.deepcopy(self.ambiguous_links):\n                source, target, linktype = ambiguous_link\n                for parent1, collider, parent2 in colliders:\n                    if collider == target and (parent1 == source or parent2 == source):\n                        if not self.tsDAG.has_edge(parent1, parent2) and not self.tsDAG.has_edge(parent2, parent1):\n                            self.update_link_type(parent1, target, '--&gt;')\n                            self.update_link_type(parent2, target, '--&gt;')\n                            self.ambiguous_links.remove(ambiguous_link)\n                            break\n\n            print(f\"--------------------------------------------------\")\n            print(f\"Non-collider orientation (orientation propagation)\")\n            print(f\"--------------------------------------------------\")\n            # *(3) Orient the non-colliders edges (orientation propagation)\n            # *    any edge Z \u2013 Y part of a partially directed path X \u2192 Z \u2013 Y,\n            # *    where there is no edge between X and Y can be oriented as Z \u2192 Y\n            for ambiguous_link in copy.deepcopy(self.ambiguous_links):\n                triples = self.find_triples_containing_link(ambiguous_link)\n                for triple in triples: self.update_link_type(triple[1], triple[2], '--&gt;')\n\n        # TODO: (3) Check if cycles are present\n\n        else:\n            print(\"No latent variable\")\n\n        return self.tsDPAG\n\n\n    def find_colliders(self) -&gt; list:\n\"\"\"\n        Find colliders.\n\n        Returns:\n            list: colliders.\n        \"\"\"\n        colliders = []\n        for node in self.tsDPAG.keys():\n            parents = [(p[0], p[1]) for p in self.tsDPAG[node]]\n            if len(parents) &gt;= 2:\n                for i in range(len(parents)):\n                    for j in range(i + 1, len(parents)):\n                        parent1 = parents[i]\n                        parent2 = parents[j]\n                        colliders.append((parent1, (node, 0), parent2))\n        return colliders\n\n\n    def update_link_type(self, parent, target, linktype):\n\"\"\"\n        Update link type.\n\n        Args:\n            parent (str): parent node.\n            target (str): target node\n            linktype (str): link type. E.g. --&gt; or -?&gt;.\n        \"\"\"\n        for idx, link in enumerate(self.tsDPAG[target[0]]):\n            if link[0] == parent[0] and link[1] == parent[1]:\n                self.tsDPAG[target[0]][idx] = (link[0], link[1], linktype)\n\n\n    def find_latent_confounders(self) -&gt; dict:\n\"\"\"\n        Find latent confounders.\n\n        Returns:\n            dict: latent confounders.\n        \"\"\"\n        confounders = {(latent, -t): list(self.tsDAG.successors((latent, -t))) for latent in self.latents for t in range(self.tau_max + 1) if len(list(self.tsDAG.successors((latent, -t)))) &gt; 1}\n\n        # Initialize a new dictionary to store unique edges\n        shrinked_confounders = defaultdict(list)\n\n        # Set to keep track of added edges without considering the time slice\n        seen_edges = set()\n\n        for key, value in confounders.items():\n            # Normalize key by removing the time slice\n            key_normalized = key[0]\n\n            for v in value:\n                # Normalize value by removing the time slice\n                v_normalized = v[0]\n\n                # Create a tuple of normalized edge\n                edge = (key_normalized, v_normalized)\n\n                # Check if edge or its reverse has been seen\n                if edge not in seen_edges and (v_normalized, key_normalized) not in seen_edges:\n                    # If not seen, add to unique edges and mark as seen\n                    shrinked_confounders[key].append(v)\n                    seen_edges.add(edge)\n\n        return shrinked_confounders\n\n\n    def find_d_separators(self, source, target):\n\"\"\"\n        Find D-Separation set.\n\n        Args:\n            source (str): source node.\n            target (str): target node.\n\n        Returns:\n            (bool, set): (True, separation set) if source and target are d-separated. Otherwise (False, empty set). \n        \"\"\"\n        paths = self.find_all_paths(source, target)\n\n        if len(paths) == 0: \n            return True, set()\n        else:\n            nodes = set()\n            obs_nodes = set()\n            for path in paths:\n                path.remove(source)\n                path.remove(target)\n                for node in path:\n                    nodes.add(node)\n                    if node not in self.latents: obs_nodes.add(node)\n\n            for r in range(len(obs_nodes) + 1):\n                for subset in combinations(obs_nodes, r):\n                    subset_set = set(subset)\n                    if not self.tsDAG.is_dconnected(source, target, subset_set):\n                        return True, subset_set\n\n            for r in range(len(nodes) + 1):\n                for subset in combinations(nodes, r):\n                    subset_set = set(subset)\n                    if not self.tsDAG.is_dconnected(source, target, subset_set):\n                        return True, subset_set\n\n        return False, set()\n\n    def find_triples_containing_link(self, ambiguous_link) -&gt; set:\n\"\"\"\n        Find all triples containing a link.\n\n        Args:\n            ambiguous_link (tuple): ambiguous_link\n\n        Returns:\n            set: triples containing the specified link.\n        \"\"\"\n        pag = self.createDAG(self.tsDPAG, self.tau_max)\n\n        source, target, _ = ambiguous_link\n        triples = set()\n\n        for n in pag.predecessors(source): \n            if n != target and not pag.has_edge(n, target) and not pag.has_edge(target, n): triples.add((n, source, target))\n        for n in pag.predecessors(target): \n            if n != source and not pag.has_edge(n, source) and not pag.has_edge(source, n): triples.add((n, target, source))\n\n        return triples\n\n\n    # DFS to find all paths\n    def find_all_paths(self, start, goal, path=[]) -&gt; list:\n\"\"\"\n        Find all path from start to goal.\n\n        Args:\n            start (str): starting node.\n            goal (str): goal node.\n            path (list, optional): Found paths. Defaults to [].\n\n        Returns:\n            list: paths\n        \"\"\"\n        path = path + [start]\n        if start == goal:\n            return [path]\n        paths = []\n        for node in self.tsDAG.successors(start):\n            if node not in path:\n                new_paths = self.find_all_paths(node, goal, path)\n                for new_path in new_paths:\n                    paths.append(new_path)\n        for node in self.tsDAG.predecessors(start):\n            if node not in path:\n                new_paths = self.find_all_paths(node, goal, path)\n                for new_path in new_paths:\n                    paths.append(new_path)\n        return paths\n</code></pre>"},{"location":"graph/#causalflow.graph.PAG.PAG.__init__","title":"<code>__init__(dag, tau_max, latents)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>dag</code> <code>DAG</code> <p>DAG to convert.</p> required <code>tau_max</code> <code>int</code> <p>max time lag.</p> required <code>latents</code> <code>list[str]</code> <p>list of latent variables.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>latent must be a string</p> Source code in <code>causalflow/graph/PAG.py</code> <pre><code>def __init__(self, dag, tau_max, latents) -&gt; None:\n\"\"\"\n    Class constructor.\n\n    Args:\n        dag (DAG): DAG to convert.\n        tau_max (int): max time lag.\n        latents (list[str]): list of latent variables.\n\n    Raises:\n        ValueError: latent must be a string\n    \"\"\"\n    if not isinstance(latents, list): raise ValueError('latents must be a list')\n    self.link_assumptions = dag\n    self.tau_max = tau_max\n    self.latents = latents\n    self.dSepSets = {}\n\n    self.tsDAG = self.createDAG(self.link_assumptions, self.tau_max)\n\n    self.pag = self.tsDAG2tsDPAG()\n</code></pre>"},{"location":"graph/#causalflow.graph.PAG.PAG.alreadyChecked","title":"<code>alreadyChecked(source, target)</code>","text":"<p>Check if a link has been already checked.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>source node</p> required <code>target</code> <code>str</code> <p>target node</p> required <p>Returns:</p> Type Description <code>(bool, tuple)</code> <p>tuple containing if the link has been checked and, if so, their separation set. Otherwise None.</p> Source code in <code>causalflow/graph/PAG.py</code> <pre><code>def alreadyChecked(self, source, target):\n\"\"\"\n    Check if a link has been already checked.\n\n    Args:\n        source (str): source node\n        target (str): target node\n\n    Returns:\n        (bool, tuple): tuple containing if the link has been checked and, if so, their separation set. Otherwise None.\n    \"\"\"\n    if (source, target) in self.dSepSets: return True, self.dSepSets[(source, target)]\n    elif (target, source) in self.dSepSets: return True, self.dSepSets[(target, source)]\n    elif ((source[0], source[1] - target[1]), (target[0], 0)) in self.dSepSets: return True, self.dSepSets[((source[0], source[1] - target[1]), (target[0], 0))]\n    elif ((target[0], target[1] - source[1]), (source[0], 0)) in self.dSepSets: return True, self.dSepSets[((target[0], target[1] - source[1]), (source[0], 0))]\n    return False, None\n</code></pre>"},{"location":"graph/#causalflow.graph.PAG.PAG.convert2Graph","title":"<code>convert2Graph()</code>","text":"<p>Convert a PAG to a graph representation.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Graph representation of a PAG.</p> Source code in <code>causalflow/graph/PAG.py</code> <pre><code>def convert2Graph(self) -&gt; dict:\n\"\"\"\n    Convert a PAG to a graph representation.\n\n    Returns:\n        dict: Graph representation of a PAG.\n    \"\"\"\n    out = {t: {} for t in self.pag}\n    for t in self.pag:\n        for s in self.pag[t]:\n            out[t][(s[0], s[1])] = s[2]\n    return out\n</code></pre>"},{"location":"graph/#causalflow.graph.PAG.PAG.createDAG","title":"<code>createDAG(link_assumptions, tau_max)</code>  <code>staticmethod</code>","text":"<p>Create a DAG represented by a Baysian Network.</p> <p>Parameters:</p> Name Type Description Default <code>link_assumptions</code> <code>dict</code> <p>DAG link assumptions.</p> required <code>tau_max</code> <code>int</code> <p>max time lag.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>source not well defined.</p> <p>Returns:</p> Name Type Description <code>BayesianNetwork</code> <code>BayesianNetwork</code> <p>DAG represented by a Baysian Network.</p> Source code in <code>causalflow/graph/PAG.py</code> <pre><code>@staticmethod\ndef createDAG(link_assumptions, tau_max) -&gt; BayesianNetwork:\n\"\"\"\n    Create a DAG represented by a Baysian Network.\n\n    Args:\n        link_assumptions (dict): DAG link assumptions.\n        tau_max (int): max time lag.\n\n    Raises:\n        ValueError: source not well defined.\n\n    Returns:\n        BayesianNetwork: DAG represented by a Baysian Network.\n    \"\"\"\n    BN = BayesianNetwork()\n    BN.add_nodes_from([(t, -l) for t in link_assumptions.keys() for l in range(0, tau_max)])\n\n    # Edges\n    edges = []\n    for t in link_assumptions.keys():\n        for source in link_assumptions[t]:\n            if len(source) == 0: continue\n            elif len(source) == 2: s, l = source\n            elif len(source) == 3: s, l, _ = source\n            else: raise ValueError(\"Source not well defined\")\n            edges.append(((s, l), (t, 0)))\n            # Add edges across time slices from -1 to -tau_max\n            for lag in range(1, tau_max + 1):\n                if l - lag &gt;= -tau_max:\n                    edges.append(((s, l - lag), (t, -lag)))\n    BN.add_edges_from(edges)\n    return BN\n</code></pre>"},{"location":"graph/#causalflow.graph.PAG.PAG.find_all_paths","title":"<code>find_all_paths(start, goal, path=[])</code>","text":"<p>Find all path from start to goal.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>str</code> <p>starting node.</p> required <code>goal</code> <code>str</code> <p>goal node.</p> required <code>path</code> <code>list</code> <p>Found paths. Defaults to [].</p> <code>[]</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>paths</p> Source code in <code>causalflow/graph/PAG.py</code> <pre><code>def find_all_paths(self, start, goal, path=[]) -&gt; list:\n\"\"\"\n    Find all path from start to goal.\n\n    Args:\n        start (str): starting node.\n        goal (str): goal node.\n        path (list, optional): Found paths. Defaults to [].\n\n    Returns:\n        list: paths\n    \"\"\"\n    path = path + [start]\n    if start == goal:\n        return [path]\n    paths = []\n    for node in self.tsDAG.successors(start):\n        if node not in path:\n            new_paths = self.find_all_paths(node, goal, path)\n            for new_path in new_paths:\n                paths.append(new_path)\n    for node in self.tsDAG.predecessors(start):\n        if node not in path:\n            new_paths = self.find_all_paths(node, goal, path)\n            for new_path in new_paths:\n                paths.append(new_path)\n    return paths\n</code></pre>"},{"location":"graph/#causalflow.graph.PAG.PAG.find_colliders","title":"<code>find_colliders()</code>","text":"<p>Find colliders.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>colliders.</p> Source code in <code>causalflow/graph/PAG.py</code> <pre><code>def find_colliders(self) -&gt; list:\n\"\"\"\n    Find colliders.\n\n    Returns:\n        list: colliders.\n    \"\"\"\n    colliders = []\n    for node in self.tsDPAG.keys():\n        parents = [(p[0], p[1]) for p in self.tsDPAG[node]]\n        if len(parents) &gt;= 2:\n            for i in range(len(parents)):\n                for j in range(i + 1, len(parents)):\n                    parent1 = parents[i]\n                    parent2 = parents[j]\n                    colliders.append((parent1, (node, 0), parent2))\n    return colliders\n</code></pre>"},{"location":"graph/#causalflow.graph.PAG.PAG.find_d_separators","title":"<code>find_d_separators(source, target)</code>","text":"<p>Find D-Separation set.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>source node.</p> required <code>target</code> <code>str</code> <p>target node.</p> required <p>Returns:</p> Type Description <code>(bool, set)</code> <p>(True, separation set) if source and target are d-separated. Otherwise (False, empty set).</p> Source code in <code>causalflow/graph/PAG.py</code> <pre><code>def find_d_separators(self, source, target):\n\"\"\"\n    Find D-Separation set.\n\n    Args:\n        source (str): source node.\n        target (str): target node.\n\n    Returns:\n        (bool, set): (True, separation set) if source and target are d-separated. Otherwise (False, empty set). \n    \"\"\"\n    paths = self.find_all_paths(source, target)\n\n    if len(paths) == 0: \n        return True, set()\n    else:\n        nodes = set()\n        obs_nodes = set()\n        for path in paths:\n            path.remove(source)\n            path.remove(target)\n            for node in path:\n                nodes.add(node)\n                if node not in self.latents: obs_nodes.add(node)\n\n        for r in range(len(obs_nodes) + 1):\n            for subset in combinations(obs_nodes, r):\n                subset_set = set(subset)\n                if not self.tsDAG.is_dconnected(source, target, subset_set):\n                    return True, subset_set\n\n        for r in range(len(nodes) + 1):\n            for subset in combinations(nodes, r):\n                subset_set = set(subset)\n                if not self.tsDAG.is_dconnected(source, target, subset_set):\n                    return True, subset_set\n\n    return False, set()\n</code></pre>"},{"location":"graph/#causalflow.graph.PAG.PAG.find_latent_confounders","title":"<code>find_latent_confounders()</code>","text":"<p>Find latent confounders.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>latent confounders.</p> Source code in <code>causalflow/graph/PAG.py</code> <pre><code>def find_latent_confounders(self) -&gt; dict:\n\"\"\"\n    Find latent confounders.\n\n    Returns:\n        dict: latent confounders.\n    \"\"\"\n    confounders = {(latent, -t): list(self.tsDAG.successors((latent, -t))) for latent in self.latents for t in range(self.tau_max + 1) if len(list(self.tsDAG.successors((latent, -t)))) &gt; 1}\n\n    # Initialize a new dictionary to store unique edges\n    shrinked_confounders = defaultdict(list)\n\n    # Set to keep track of added edges without considering the time slice\n    seen_edges = set()\n\n    for key, value in confounders.items():\n        # Normalize key by removing the time slice\n        key_normalized = key[0]\n\n        for v in value:\n            # Normalize value by removing the time slice\n            v_normalized = v[0]\n\n            # Create a tuple of normalized edge\n            edge = (key_normalized, v_normalized)\n\n            # Check if edge or its reverse has been seen\n            if edge not in seen_edges and (v_normalized, key_normalized) not in seen_edges:\n                # If not seen, add to unique edges and mark as seen\n                shrinked_confounders[key].append(v)\n                seen_edges.add(edge)\n\n    return shrinked_confounders\n</code></pre>"},{"location":"graph/#causalflow.graph.PAG.PAG.find_triples_containing_link","title":"<code>find_triples_containing_link(ambiguous_link)</code>","text":"<p>Find all triples containing a link.</p> <p>Parameters:</p> Name Type Description Default <code>ambiguous_link</code> <code>tuple</code> <p>ambiguous_link</p> required <p>Returns:</p> Name Type Description <code>set</code> <code>set</code> <p>triples containing the specified link.</p> Source code in <code>causalflow/graph/PAG.py</code> <pre><code>def find_triples_containing_link(self, ambiguous_link) -&gt; set:\n\"\"\"\n    Find all triples containing a link.\n\n    Args:\n        ambiguous_link (tuple): ambiguous_link\n\n    Returns:\n        set: triples containing the specified link.\n    \"\"\"\n    pag = self.createDAG(self.tsDPAG, self.tau_max)\n\n    source, target, _ = ambiguous_link\n    triples = set()\n\n    for n in pag.predecessors(source): \n        if n != target and not pag.has_edge(n, target) and not pag.has_edge(target, n): triples.add((n, source, target))\n    for n in pag.predecessors(target): \n        if n != source and not pag.has_edge(n, source) and not pag.has_edge(source, n): triples.add((n, target, source))\n\n    return triples\n</code></pre>"},{"location":"graph/#causalflow.graph.PAG.PAG.tsDAG2tsDPAG","title":"<code>tsDAG2tsDPAG()</code>","text":"<p>Convert a DAG to a Time-series DPAG.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Time-series DPAG.</p> Source code in <code>causalflow/graph/PAG.py</code> <pre><code>def tsDAG2tsDPAG(self) -&gt; dict:\n\"\"\"\n    Convert a DAG to a Time-series DPAG.\n\n    Returns:\n        dict: Time-series DPAG.\n    \"\"\"\n    self.tsDPAG = {t: [(s[0], s[1], '--&gt;') for s in self.link_assumptions[t] if s[0] not in self.latents] for t in self.link_assumptions.keys() if t not in self.latents}\n\n    if len(self.latents) &gt; 0:\n        self.ambiguous_links = []\n\n        # Separate nodes based on their time index\n        time_zero_nodes = []\n        other_nodes = []\n\n        for node in self.tsDAG.nodes():\n            if node[1] == 0:\n                time_zero_nodes.append(node)\n            else:\n                other_nodes.append(node)\n\n        for target in time_zero_nodes + other_nodes:\n            print(f\"Analysing target: {target}\")\n            if target[0] in self.latents: continue\n            tmp = []\n            for n in list(self.tsDAG.nodes()):\n                if n[0] != target[0] or n[1] != target[1]:\n                    tmp.append(n)\n            for p in list(self.tsDAG.predecessors(target)) + list(self.tsDAG.successors(target)):\n                if p in tmp: tmp.remove(p)\n            for source in tmp:\n                alreadyChecked, d_sep = self.alreadyChecked(source, target)\n                if alreadyChecked:\n                    print(f\"\\t- {source} \u22a5 {target} | {d_sep} ALREADY CHECKED\")\n                else:\n                    areDsep, d_sep = self.find_d_separators(source, target, self.latents)\n                    self.dSepSets[(source, target)] = d_sep\n                    print(f\"\\t- {source} \u22a5 {target} | {d_sep}\")\n                if areDsep and any(node[0] in self.latents for node in d_sep):\n                    if source[0] in self.latents or target[0] in self.latents: continue\n                    if target[1] == 0:\n                        print(f\"\\t- SPURIOUS LINK: ({source[0]}, {source[1]}) o-o ({target[0]}, {target[1]})\")\n                        if (source[0], source[1], 'o-o') not in self.tsDPAG[target[0]]: self.tsDPAG[target[0]].append((source[0], source[1], 'o-o'))\n                        if (source, target, 'o-o') not in self.ambiguous_links: self.ambiguous_links.append((source, target, 'o-o'))\n                    elif source[1] == 0:\n                        print(f\"\\t- SPURIOUS LINK: ({target[0]}, {target[1]}) o-o ({source[0]}, {source[1]})\")\n                        if (target[0], target[1], 'o-o') not in self.tsDPAG[source[0]]: self.tsDPAG[source[0]].append((target[0], target[1], 'o-o'))\n                        if (target, source, 'o-o') not in self.ambiguous_links: self.ambiguous_links.append((target, source, 'o-o'))\n\n\n        print(f\"--------------------------------------------------\")\n        print(f\"    Bidirected link due to latent confounders     \")\n        print(f\"--------------------------------------------------\")\n        # *(1) Bidirected link between variables confounded by a latent variable  \n        # *    if a link between them does not exist already\n        confounders = self.find_latent_confounders()\n        for confounded in copy.deepcopy(list(confounders.values())):\n            for c1 in copy.deepcopy(confounded):\n                tmp = copy.deepcopy(confounded)\n                tmp.remove(c1)\n                for c2 in tmp:\n                    if (c1, c2, 'o-o') in self.ambiguous_links:\n                        self.update_link_type(c1, c2, '&lt;-&gt;')\n                        self.ambiguous_links.remove((c1, c2, 'o-o'))\n                        print(f\"\\t- SPURIOUS LINK REMOVED: {c1} o-o {c2}\")\n                    elif (c2, c1, 'o-o') in self.ambiguous_links:\n                        self.update_link_type(c1, c2, '&lt;-&gt;')\n                        self.ambiguous_links.remove((c2, c1, 'o-o'))\n                        print(f\"\\t- SPURIOUS LINK REMOVED: {c2} o-o {c1}\")\n                confounded.remove(c1)\n\n        print(f\"--------------------------------------------------\")\n        print(f\"              Collider orientation                \")\n        print(f\"--------------------------------------------------\")\n        # *(2) Identify and orient the colliders:\n        # *    for any path X \u2013 Z \u2013 Y where there is no edge between\n        # *    X and Y and, Z was never included in the conditioning set ==&gt; X \u2192 Z \u2190 Y collider\n        colliders = self.find_colliders()\n        for ambiguous_link in copy.deepcopy(self.ambiguous_links):\n            source, target, linktype = ambiguous_link\n            for parent1, collider, parent2 in colliders:\n                if collider == target and (parent1 == source or parent2 == source):\n                    if not self.tsDAG.has_edge(parent1, parent2) and not self.tsDAG.has_edge(parent2, parent1):\n                        self.update_link_type(parent1, target, '--&gt;')\n                        self.update_link_type(parent2, target, '--&gt;')\n                        self.ambiguous_links.remove(ambiguous_link)\n                        break\n\n        print(f\"--------------------------------------------------\")\n        print(f\"Non-collider orientation (orientation propagation)\")\n        print(f\"--------------------------------------------------\")\n        # *(3) Orient the non-colliders edges (orientation propagation)\n        # *    any edge Z \u2013 Y part of a partially directed path X \u2192 Z \u2013 Y,\n        # *    where there is no edge between X and Y can be oriented as Z \u2192 Y\n        for ambiguous_link in copy.deepcopy(self.ambiguous_links):\n            triples = self.find_triples_containing_link(ambiguous_link)\n            for triple in triples: self.update_link_type(triple[1], triple[2], '--&gt;')\n\n    # TODO: (3) Check if cycles are present\n\n    else:\n        print(\"No latent variable\")\n\n    return self.tsDPAG\n</code></pre>"},{"location":"graph/#causalflow.graph.PAG.PAG.update_link_type","title":"<code>update_link_type(parent, target, linktype)</code>","text":"<p>Update link type.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>str</code> <p>parent node.</p> required <code>target</code> <code>str</code> <p>target node</p> required <code>linktype</code> <code>str</code> <p>link type. E.g. --&gt; or -?&gt;.</p> required Source code in <code>causalflow/graph/PAG.py</code> <pre><code>def update_link_type(self, parent, target, linktype):\n\"\"\"\n    Update link type.\n\n    Args:\n        parent (str): parent node.\n        target (str): target node\n        linktype (str): link type. E.g. --&gt; or -?&gt;.\n    \"\"\"\n    for idx, link in enumerate(self.tsDPAG[target[0]]):\n        if link[0] == parent[0] and link[1] == parent[1]:\n            self.tsDPAG[target[0]][idx] = (link[0], link[1], linktype)\n</code></pre>"},{"location":"preprocessing/","title":"Preprocessing","text":"<p>This module provides the Data class.</p> Classes <p>Data: public class for handling data used for the causal discovery.</p> <p>This module provides the Subsampler class.</p> Classes <p>Subsampler: public class for subsampling.</p> <p>This module provides the EntropyBasedMethod class.</p> Classes <p>EntropyBasedMethod: EntropyBasedMethod abstract class.</p> <p>This module provides subsampling methods for data preprocessing.</p> Classes <p>SSMode: An enumerator containing all the supported subsampling methods. SubsamplingMethod: A class for implementing various subsampling techniques.</p> <p>This module provides the MovingWindow class to facilitate the entropy-based subsampling methods.</p> Classes <p>MovingWindow: A class used by the entropy-based subsampling methods.</p> <p>This module provides the Static class.</p> Classes <p>Static: Subsamples data by taking one sample each step-samples.</p> <p>This module provides the WSDynamic class.</p> Classes <p>WSDynamic: Subsampling method with dynamic window size based on entropy analysis.</p> <p>This module provides the WSFFTStatic class.</p> Classes <p>WSFFTStatic: Subsampling method with static window size based on Fourier analysis.</p> <p>This module provides the WSStatic class.</p> Classes <p>WSStatic: Entropy based subsampling method with static window size.</p>"},{"location":"preprocessing/#causalflow.preprocessing.data.Data","title":"<code>Data</code>","text":"<p>Data class manages the preprocess of the data before the causal analysis.</p> Source code in <code>causalflow/preprocessing/data.py</code> <pre><code>class Data():\n\"\"\"Data class manages the preprocess of the data before the causal analysis.\"\"\"\n\n    def __init__(self, data, vars = None, fill_nan = True, stand = False, subsampling : SubsamplingMethod = None, show_subsampling = False):\n\"\"\"\n        Class constructor.\n\n        Args:\n            data (str/DataFrame/np.array): it can be a string specifing the path of a csv file to load/pandas.DataFrame/numpy.array.\n            vars (list(str), optional): List containing variable names. If unset then, \n                if data = (str/DataFrame) vars = data columns name elif data = np.array vars = [X_0 .. X_N]\n                Defaults to None.\n            fill_nan (bool, optional): Fill NaNs bit. Defaults to True.\n            stand (bool, optional): Standardization bit. Defaults to False.\n            subsampling (SubsamplingMethod, optional): Subsampling method. If None not active. Defaults to None.\n            show_subsampling (bool, optional): If True shows subsampling result. Defaults to False.\n\n        Raises:\n            TypeError: if data is not str - DataFrame - ndarray.\n        \"\"\"\n        # Data handling\n        if type(data) == np.ndarray:\n            self.d = pd.DataFrame(data)\n            if vars is None: self.d.columns = list(['X_' + str(f) for f in range(len(self.d.columns))])\n        elif type(data) == pd.DataFrame:\n            self.d = data\n        elif type(data) == str:\n            self.d = pd.read_csv(data)\n        else:\n            raise TypeError(\"data field not in the correct type\\ndata must be one of the following type:\\n- numpy.ndarray\\n- pandas.DataFrame\\n- .csv path\")\n\n\n        # Columns name handling\n        if vars is not None:\n            self.d.columns = list(vars)\n\n\n        self.orig_features = self.features\n        self.orig_pretty_features = self.pretty_features\n        self.orig_N = self.N\n        self.orig_T = len(self.d)\n\n        # Filling NaNs\n        if fill_nan:\n            if self.d.isnull().values.any():\n                self.d.fillna(inplace=True, method=\"ffill\")\n                self.d.fillna(inplace=True, method=\"bfill\")\n\n        # Subsampling data\n        if subsampling is not None:\n            subsampler = Subsampler(self.d, ss_method = subsampling)\n            self.d = pd.DataFrame(subsampler.subsample(), columns = self.features)\n            if show_subsampling: subsampler.plot_subsampled_data()\n\n        # Standardize data\n        if stand:\n            scaler = StandardScaler()\n            scaler = scaler.fit(self.d)\n            self.d = pd.DataFrame(scaler.transform(self.d), columns = self.features)\n\n    @property  \n    def features(self):\n\"\"\"\n        Return list of features.\n\n        Returns:\n            list(str): list of feature names.\n        \"\"\"\n        return list(self.d.columns)\n\n    @property\n    def pretty_features(self):\n\"\"\"\n        Return list of features with LATEX symbols.\n\n        Returns:\n            list(str): list of feature names.\n        \"\"\"\n        return [r'$' + str(v) + '$' for v in self.d.columns]\n\n    @property\n    def N(self):\n\"\"\"\n        Number of features.\n\n        Returns:\n            (int): number of features.\n        \"\"\"\n        return len(self.d.columns)\n\n    @property\n    def T(self):\n\"\"\"\n        Dataframe length.\n\n        Returns:\n            (int): dataframe length.\n        \"\"\"\n        return len(self.d)\n\n\n    def shrink(self, selected_features):\n\"\"\"\n        Shrink dataframe d on the selected features.\n\n        Args:\n            selected_features (list(str)): list of variables.\n        \"\"\"\n        self.d = self.d[selected_features]\n\n\n    def plot_timeseries(self, savefig = None):\n\"\"\"\n        Plot timeseries data.\n\n        Args:\n            savefig (str): figure path.\n        \"\"\"\n        # Create grid\n        gs = gridspec.GridSpec(self.N, 1)\n\n        # Time vector\n        T = list(range(self.T))\n\n        plt.figure()\n        for i in range(0, self.d.shape[1]):\n            ax = plt.subplot(gs[i, 0])\n            plt.plot(T, self.d.values[:, i], color = 'tab:red')\n            plt.ylabel(str(self.pretty_features[i]))\n\n        if savefig is not None:\n            plt.savefig(savefig)\n        else:\n            plt.show()\n\n\n    def save_csv(self, csvpath):\n\"\"\"\n        Save timeseries data into a CSV file.\n\n        Args:\n            csvpath (str): CSV path.\n        \"\"\"\n        self.d.to_csv(csvpath, index=False)\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.data.Data.N","title":"<code>N</code>  <code>property</code>","text":"<p>Number of features.</p> <p>Returns:</p> Type Description <code>int</code> <p>number of features.</p>"},{"location":"preprocessing/#causalflow.preprocessing.data.Data.T","title":"<code>T</code>  <code>property</code>","text":"<p>Dataframe length.</p> <p>Returns:</p> Type Description <code>int</code> <p>dataframe length.</p>"},{"location":"preprocessing/#causalflow.preprocessing.data.Data.features","title":"<code>features</code>  <code>property</code>","text":"<p>Return list of features.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>str</code> <p>list of feature names.</p>"},{"location":"preprocessing/#causalflow.preprocessing.data.Data.pretty_features","title":"<code>pretty_features</code>  <code>property</code>","text":"<p>Return list of features with LATEX symbols.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>str</code> <p>list of feature names.</p>"},{"location":"preprocessing/#causalflow.preprocessing.data.Data.__init__","title":"<code>__init__(data, vars=None, fill_nan=True, stand=False, subsampling=None, show_subsampling=False)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str / DataFrame / np.array</code> <p>it can be a string specifing the path of a csv file to load/pandas.DataFrame/numpy.array.</p> required <code>vars</code> <code>list(str)</code> <p>List containing variable names. If unset then,  if data = (str/DataFrame) vars = data columns name elif data = np.array vars = [X_0 .. X_N] Defaults to None.</p> <code>None</code> <code>fill_nan</code> <code>bool</code> <p>Fill NaNs bit. Defaults to True.</p> <code>True</code> <code>stand</code> <code>bool</code> <p>Standardization bit. Defaults to False.</p> <code>False</code> <code>subsampling</code> <code>SubsamplingMethod</code> <p>Subsampling method. If None not active. Defaults to None.</p> <code>None</code> <code>show_subsampling</code> <code>bool</code> <p>If True shows subsampling result. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>if data is not str - DataFrame - ndarray.</p> Source code in <code>causalflow/preprocessing/data.py</code> <pre><code>def __init__(self, data, vars = None, fill_nan = True, stand = False, subsampling : SubsamplingMethod = None, show_subsampling = False):\n\"\"\"\n    Class constructor.\n\n    Args:\n        data (str/DataFrame/np.array): it can be a string specifing the path of a csv file to load/pandas.DataFrame/numpy.array.\n        vars (list(str), optional): List containing variable names. If unset then, \n            if data = (str/DataFrame) vars = data columns name elif data = np.array vars = [X_0 .. X_N]\n            Defaults to None.\n        fill_nan (bool, optional): Fill NaNs bit. Defaults to True.\n        stand (bool, optional): Standardization bit. Defaults to False.\n        subsampling (SubsamplingMethod, optional): Subsampling method. If None not active. Defaults to None.\n        show_subsampling (bool, optional): If True shows subsampling result. Defaults to False.\n\n    Raises:\n        TypeError: if data is not str - DataFrame - ndarray.\n    \"\"\"\n    # Data handling\n    if type(data) == np.ndarray:\n        self.d = pd.DataFrame(data)\n        if vars is None: self.d.columns = list(['X_' + str(f) for f in range(len(self.d.columns))])\n    elif type(data) == pd.DataFrame:\n        self.d = data\n    elif type(data) == str:\n        self.d = pd.read_csv(data)\n    else:\n        raise TypeError(\"data field not in the correct type\\ndata must be one of the following type:\\n- numpy.ndarray\\n- pandas.DataFrame\\n- .csv path\")\n\n\n    # Columns name handling\n    if vars is not None:\n        self.d.columns = list(vars)\n\n\n    self.orig_features = self.features\n    self.orig_pretty_features = self.pretty_features\n    self.orig_N = self.N\n    self.orig_T = len(self.d)\n\n    # Filling NaNs\n    if fill_nan:\n        if self.d.isnull().values.any():\n            self.d.fillna(inplace=True, method=\"ffill\")\n            self.d.fillna(inplace=True, method=\"bfill\")\n\n    # Subsampling data\n    if subsampling is not None:\n        subsampler = Subsampler(self.d, ss_method = subsampling)\n        self.d = pd.DataFrame(subsampler.subsample(), columns = self.features)\n        if show_subsampling: subsampler.plot_subsampled_data()\n\n    # Standardize data\n    if stand:\n        scaler = StandardScaler()\n        scaler = scaler.fit(self.d)\n        self.d = pd.DataFrame(scaler.transform(self.d), columns = self.features)\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.data.Data.plot_timeseries","title":"<code>plot_timeseries(savefig=None)</code>","text":"<p>Plot timeseries data.</p> <p>Parameters:</p> Name Type Description Default <code>savefig</code> <code>str</code> <p>figure path.</p> <code>None</code> Source code in <code>causalflow/preprocessing/data.py</code> <pre><code>def plot_timeseries(self, savefig = None):\n\"\"\"\n    Plot timeseries data.\n\n    Args:\n        savefig (str): figure path.\n    \"\"\"\n    # Create grid\n    gs = gridspec.GridSpec(self.N, 1)\n\n    # Time vector\n    T = list(range(self.T))\n\n    plt.figure()\n    for i in range(0, self.d.shape[1]):\n        ax = plt.subplot(gs[i, 0])\n        plt.plot(T, self.d.values[:, i], color = 'tab:red')\n        plt.ylabel(str(self.pretty_features[i]))\n\n    if savefig is not None:\n        plt.savefig(savefig)\n    else:\n        plt.show()\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.data.Data.save_csv","title":"<code>save_csv(csvpath)</code>","text":"<p>Save timeseries data into a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>csvpath</code> <code>str</code> <p>CSV path.</p> required Source code in <code>causalflow/preprocessing/data.py</code> <pre><code>def save_csv(self, csvpath):\n\"\"\"\n    Save timeseries data into a CSV file.\n\n    Args:\n        csvpath (str): CSV path.\n    \"\"\"\n    self.d.to_csv(csvpath, index=False)\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.data.Data.shrink","title":"<code>shrink(selected_features)</code>","text":"<p>Shrink dataframe d on the selected features.</p> <p>Parameters:</p> Name Type Description Default <code>selected_features</code> <code>list(str</code> <p>list of variables.</p> required Source code in <code>causalflow/preprocessing/data.py</code> <pre><code>def shrink(self, selected_features):\n\"\"\"\n    Shrink dataframe d on the selected features.\n\n    Args:\n        selected_features (list(str)): list of variables.\n    \"\"\"\n    self.d = self.d[selected_features]\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.Subsampler.Subsampler","title":"<code>Subsampler</code>","text":"<p>Subsampler class.</p> It subsamples the data by using a subsampling method chosen among <ul> <li>Static - subsamples data by taking one sample each step-samples</li> <li>WSDynamic - entropy based method with dynamic window size computed by breakpoint analysis</li> <li>WSFFTStatic - entropy based method with fixed window size computed by FFT analysis</li> <li>WSStatic - entropy base method with predefined window size</li> </ul> Source code in <code>causalflow/preprocessing/Subsampler.py</code> <pre><code>class Subsampler():\n\"\"\"\n    Subsampler class.\n\n    It subsamples the data by using a subsampling method chosen among:\n        - Static - subsamples data by taking one sample each step-samples\n        - WSDynamic - entropy based method with dynamic window size computed by breakpoint analysis\n        - WSFFTStatic - entropy based method with fixed window size computed by FFT analysis\n        - WSStatic - entropy base method with predefined window size\n    \"\"\"\n\n    def __init__(self, \n                 df: pd.DataFrame, \n                 ss_method: SubsamplingMethod):\n\"\"\"\n        Class constructor.\n\n        Args:\n            df (pd.DataFrame): dataframe to subsample.\n            ss_method (SubsamplingMethod): subsampling method.\n        \"\"\"\n        self.df = df\n        self.ss_method = ss_method\n        self.ss_method.initialise(df)\n\n\n    def subsample(self):\n\"\"\"\n        Run the subsampling algorithm and returns the subsapled ndarray.\n\n        Returns:\n            (ndarray): Subsampled dataframe value.\n        \"\"\"\n        self.result = self.ss_method.run()\n        return self.df.values[self.result, :]\n\n\n    def plot_subsampled_data(self, dpi = 100, show = True):\n\"\"\"\n        Plot dataframe sub-sampled data.\n\n        Args:\n            dpi (int, optional): image dpi. Defaults to 100.\n            show (bool, optional): if True it shows the figure and block the process. Defaults to True.\n        \"\"\"\n        n_plot = self.df.shape[1]\n\n        # Create grid\n        gs = gridspec.GridSpec(n_plot, 1)\n\n        # Time vector\n        T = list(range(0, self.df.shape[0]))\n\n        pl.figure(dpi = dpi)\n        for i in range(0, n_plot):\n            ax = pl.subplot(gs[i, 0])\n            pl.plot(T, self.df.values[:, i], color = 'tab:red')\n            pl.scatter(np.array(T)[self.result],\n                       self.df.values[self.result, i],\n                       s = 80,\n                       facecolors = 'none',\n                       edgecolors = 'b')\n            pl.gca().set(ylabel = r'$' + str(self.df.columns.values[i]) + '$')\n        if show:\n            pl.show()\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.Subsampler.Subsampler.__init__","title":"<code>__init__(df, ss_method)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>dataframe to subsample.</p> required <code>ss_method</code> <code>SubsamplingMethod</code> <p>subsampling method.</p> required Source code in <code>causalflow/preprocessing/Subsampler.py</code> <pre><code>def __init__(self, \n             df: pd.DataFrame, \n             ss_method: SubsamplingMethod):\n\"\"\"\n    Class constructor.\n\n    Args:\n        df (pd.DataFrame): dataframe to subsample.\n        ss_method (SubsamplingMethod): subsampling method.\n    \"\"\"\n    self.df = df\n    self.ss_method = ss_method\n    self.ss_method.initialise(df)\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.Subsampler.Subsampler.plot_subsampled_data","title":"<code>plot_subsampled_data(dpi=100, show=True)</code>","text":"<p>Plot dataframe sub-sampled data.</p> <p>Parameters:</p> Name Type Description Default <code>dpi</code> <code>int</code> <p>image dpi. Defaults to 100.</p> <code>100</code> <code>show</code> <code>bool</code> <p>if True it shows the figure and block the process. Defaults to True.</p> <code>True</code> Source code in <code>causalflow/preprocessing/Subsampler.py</code> <pre><code>def plot_subsampled_data(self, dpi = 100, show = True):\n\"\"\"\n    Plot dataframe sub-sampled data.\n\n    Args:\n        dpi (int, optional): image dpi. Defaults to 100.\n        show (bool, optional): if True it shows the figure and block the process. Defaults to True.\n    \"\"\"\n    n_plot = self.df.shape[1]\n\n    # Create grid\n    gs = gridspec.GridSpec(n_plot, 1)\n\n    # Time vector\n    T = list(range(0, self.df.shape[0]))\n\n    pl.figure(dpi = dpi)\n    for i in range(0, n_plot):\n        ax = pl.subplot(gs[i, 0])\n        pl.plot(T, self.df.values[:, i], color = 'tab:red')\n        pl.scatter(np.array(T)[self.result],\n                   self.df.values[self.result, i],\n                   s = 80,\n                   facecolors = 'none',\n                   edgecolors = 'b')\n        pl.gca().set(ylabel = r'$' + str(self.df.columns.values[i]) + '$')\n    if show:\n        pl.show()\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.Subsampler.Subsampler.subsample","title":"<code>subsample()</code>","text":"<p>Run the subsampling algorithm and returns the subsapled ndarray.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Subsampled dataframe value.</p> Source code in <code>causalflow/preprocessing/Subsampler.py</code> <pre><code>def subsample(self):\n\"\"\"\n    Run the subsampling algorithm and returns the subsapled ndarray.\n\n    Returns:\n        (ndarray): Subsampled dataframe value.\n    \"\"\"\n    self.result = self.ss_method.run()\n    return self.df.values[self.result, :]\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod","title":"<code>EntropyBasedMethod</code>","text":"<p>             Bases: <code>ABC</code></p> <p>EntropyBasedMethod abstract class.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>class EntropyBasedMethod(ABC):\n\"\"\"EntropyBasedMethod abstract class.\"\"\"\n\n    def __init__(self, threshold):\n\"\"\"\n        Class constructor.\n\n        Args:\n            threshold (float): entropy threshold.\n        \"\"\"\n        self.windows = list()\n        self.segments = list()\n        self.threshold = threshold\n\n\n    def create_rounded_copy(self):\n\"\"\"\n        Create deepcopy of the dataframe but with rounded values.\n\n        Returns:\n            (pd.DataFrame): rounded dataframe.\n        \"\"\"\n        de = deepcopy(self.df)\n        de = de.round(1)\n        return de\n\n\n    def __normalization(self):\n\"\"\"Normalize entropy for each moving window.\"\"\"\n        max_e = max([mw.entropy for mw in self.windows])\n        for mw in self.windows:\n            mw.entropy = mw.entropy / max_e\n\n\n    def moving_window_analysis(self):\n\"\"\"Compute dataframe entropy on moving windows.\"\"\"\n        de = self.create_rounded_copy()\n\n        for ll, rl in self.segments:\n            # Create moving window\n            mw_df = de.values[ll: rl]\n\n            # Build a Moving Window\n            mw = MovingWindow(mw_df)\n\n            # Compute entropy\n            mw.get_entropy()\n\n            # Compute optimal number of samples\n            mw.optimal_sampling(self.threshold)\n\n            # Collect result in a list\n            self.windows.append(mw)\n\n        # Entropy normalization\n        self.__normalization()\n\n\n    def extract_indexes(self):\n\"\"\"Extract a list of indexes corresponding to the samples selected by the subsampling procedure.\"\"\"\n        _sample_index_list = list()\n        for i, mw in enumerate(self.windows):\n            sum_ws = sum([wind.T for wind in self.windows[:i]])\n            sample_index = [si + sum_ws for si in mw.opt_samples_index]\n            _sample_index_list += sample_index\n        return _sample_index_list\n\n\n    @abstractmethod\n    def dataset_segmentation(self):\n\"\"\"Abstract method.\"\"\"\n        pass\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.__init__","title":"<code>__init__(threshold)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>entropy threshold.</p> required Source code in <code>causalflow/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>def __init__(self, threshold):\n\"\"\"\n    Class constructor.\n\n    Args:\n        threshold (float): entropy threshold.\n    \"\"\"\n    self.windows = list()\n    self.segments = list()\n    self.threshold = threshold\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.__normalization","title":"<code>__normalization()</code>","text":"<p>Normalize entropy for each moving window.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>def __normalization(self):\n\"\"\"Normalize entropy for each moving window.\"\"\"\n    max_e = max([mw.entropy for mw in self.windows])\n    for mw in self.windows:\n        mw.entropy = mw.entropy / max_e\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.create_rounded_copy","title":"<code>create_rounded_copy()</code>","text":"<p>Create deepcopy of the dataframe but with rounded values.</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>rounded dataframe.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>def create_rounded_copy(self):\n\"\"\"\n    Create deepcopy of the dataframe but with rounded values.\n\n    Returns:\n        (pd.DataFrame): rounded dataframe.\n    \"\"\"\n    de = deepcopy(self.df)\n    de = de.round(1)\n    return de\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.dataset_segmentation","title":"<code>dataset_segmentation()</code>  <code>abstractmethod</code>","text":"<p>Abstract method.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>@abstractmethod\ndef dataset_segmentation(self):\n\"\"\"Abstract method.\"\"\"\n    pass\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.extract_indexes","title":"<code>extract_indexes()</code>","text":"<p>Extract a list of indexes corresponding to the samples selected by the subsampling procedure.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>def extract_indexes(self):\n\"\"\"Extract a list of indexes corresponding to the samples selected by the subsampling procedure.\"\"\"\n    _sample_index_list = list()\n    for i, mw in enumerate(self.windows):\n        sum_ws = sum([wind.T for wind in self.windows[:i]])\n        sample_index = [si + sum_ws for si in mw.opt_samples_index]\n        _sample_index_list += sample_index\n    return _sample_index_list\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.moving_window_analysis","title":"<code>moving_window_analysis()</code>","text":"<p>Compute dataframe entropy on moving windows.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>def moving_window_analysis(self):\n\"\"\"Compute dataframe entropy on moving windows.\"\"\"\n    de = self.create_rounded_copy()\n\n    for ll, rl in self.segments:\n        # Create moving window\n        mw_df = de.values[ll: rl]\n\n        # Build a Moving Window\n        mw = MovingWindow(mw_df)\n\n        # Compute entropy\n        mw.get_entropy()\n\n        # Compute optimal number of samples\n        mw.optimal_sampling(self.threshold)\n\n        # Collect result in a list\n        self.windows.append(mw)\n\n    # Entropy normalization\n    self.__normalization()\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.SubsamplingMethod.SSMode","title":"<code>SSMode</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Enumerator containing all the supported subsampling methods.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/SubsamplingMethod.py</code> <pre><code>class SSMode(Enum):\n\"\"\"Enumerator containing all the supported subsampling methods.\"\"\"\n\n    WSDynamic = 'Dynamic-size moving window'\n    WSStatic = 'Static-size moving window'\n    WSFFTStatic = 'FFT static-size moving window'\n    Static = 'Static'\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.SubsamplingMethod.SubsamplingMethod","title":"<code>SubsamplingMethod</code>","text":"<p>             Bases: <code>ABC</code></p> <p>SubsamplingMethod abstract class.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/SubsamplingMethod.py</code> <pre><code>class SubsamplingMethod(ABC):\n\"\"\"SubsamplingMethod abstract class.\"\"\"\n\n    def __init__(self, ssmode: SSMode):\n\"\"\"\n        Class constructor.\n\n        Args:\n            ssmode (SSMore): Subsampling method.\n        \"\"\"\n        self.ssmode = ssmode\n        self.df = None\n\n\n    def initialise(self, dataframe: pd.DataFrame):\n\"\"\"\n        Initialise class by setting the dataframe to subsample.\n\n        Args:\n            dataframe (pd.DataFrame): Pandas DataFrame to subsample.\n        \"\"\"\n        self.df = dataframe\n\n\n    @abstractmethod\n    def run(self):\n\"\"\"Run subsampler.\"\"\"\n        pass\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.SubsamplingMethod.SubsamplingMethod.__init__","title":"<code>__init__(ssmode)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>ssmode</code> <code>SSMore</code> <p>Subsampling method.</p> required Source code in <code>causalflow/preprocessing/subsampling_methods/SubsamplingMethod.py</code> <pre><code>def __init__(self, ssmode: SSMode):\n\"\"\"\n    Class constructor.\n\n    Args:\n        ssmode (SSMore): Subsampling method.\n    \"\"\"\n    self.ssmode = ssmode\n    self.df = None\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.SubsamplingMethod.SubsamplingMethod.initialise","title":"<code>initialise(dataframe)</code>","text":"<p>Initialise class by setting the dataframe to subsample.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>pd.DataFrame</code> <p>Pandas DataFrame to subsample.</p> required Source code in <code>causalflow/preprocessing/subsampling_methods/SubsamplingMethod.py</code> <pre><code>def initialise(self, dataframe: pd.DataFrame):\n\"\"\"\n    Initialise class by setting the dataframe to subsample.\n\n    Args:\n        dataframe (pd.DataFrame): Pandas DataFrame to subsample.\n    \"\"\"\n    self.df = dataframe\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.SubsamplingMethod.SubsamplingMethod.run","title":"<code>run()</code>  <code>abstractmethod</code>","text":"<p>Run subsampler.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/SubsamplingMethod.py</code> <pre><code>@abstractmethod\ndef run(self):\n\"\"\"Run subsampler.\"\"\"\n    pass\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.moving_window.MovingWindow","title":"<code>MovingWindow</code>","text":"<p>Moving window class used by the entropy-based subsampling methods.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/moving_window.py</code> <pre><code>class MovingWindow:\n\"\"\"Moving window class used by the entropy-based subsampling methods.\"\"\"\n\n    def __init__(self, window):\n\"\"\"\n        Class constuctor.\n\n        Args:\n            window (int): moving window size.\n        \"\"\"\n        self.window = window\n        self.T, self.dim = window.shape\n        self.entropy = None\n        self.opt_size = None\n        self.opt_samples_index = None\n\n    def get_pdf(self):\n\"\"\"Compute the probability distribution function from an array of data.\"\"\"\n        counts = {}\n\n        for i in range(0, self.T):\n            t = tuple(self.window[i, :])\n            if t in counts:\n                counts[t] += 1\n            else:\n                counts[t] = 1\n\n        pdf = {k: v / self.T for k, v in counts.items()}\n\n        return list(pdf.values())\n\n\n    def get_entropy(self):\n\"\"\"Compute the entropy based on probability distribution function.\"\"\"\n        self.entropy = entropy(self.get_pdf(), base = 2)\n\n\n    def samples_selector(self, step) -&gt; list:\n\"\"\"\n        Select sample to be taken from a moving window.\n\n        Args:\n            step (int): subsampling frequency.\n\n        Returns:\n            list: list of indexes corresponding to the sample to be taken.\n        \"\"\"\n        return [i for i in range(0, self.T, step)]\n\n\n    def optimal_sampling(self, thres):\n\"\"\"\n        Find the optimal number of sample for a particular moving window.\n\n        Args:\n            thres (float): stopping criteria threshold.\n        \"\"\"\n        converged = False\n        _old_step = 0\n        _sub_index = list(range(0, self.T))\n        _old_sub_index = list(range(0, self.T))\n        _max_n = math.floor(self.T / 2)\n\n        for n in range(_max_n, 1, -1):\n            # resampling window with n samples and build another Moving Window\n            step = int(self.T / n)\n            if step == _old_step:\n                continue\n            _old_step = step\n            _old_sub_index = _sub_index\n            _sub_index = self.samples_selector(step)\n            _sub_w = MovingWindow(self.window[_sub_index])\n\n            # compute entropy on the sub window\n            _sub_w.get_entropy()\n\n            # stopping criteria\n            if self.entropy != 0:\n                if abs(_sub_w.entropy - self.entropy) / self.entropy &gt;= thres:\n                    converged = True\n                    break\n        self.opt_size = len(_old_sub_index) if converged else len(_sub_index)\n        self.opt_samples_index = _old_sub_index if converged else _sub_index\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.moving_window.MovingWindow.__init__","title":"<code>__init__(window)</code>","text":"<p>Class constuctor.</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>int</code> <p>moving window size.</p> required Source code in <code>causalflow/preprocessing/subsampling_methods/moving_window.py</code> <pre><code>def __init__(self, window):\n\"\"\"\n    Class constuctor.\n\n    Args:\n        window (int): moving window size.\n    \"\"\"\n    self.window = window\n    self.T, self.dim = window.shape\n    self.entropy = None\n    self.opt_size = None\n    self.opt_samples_index = None\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.moving_window.MovingWindow.get_entropy","title":"<code>get_entropy()</code>","text":"<p>Compute the entropy based on probability distribution function.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/moving_window.py</code> <pre><code>def get_entropy(self):\n\"\"\"Compute the entropy based on probability distribution function.\"\"\"\n    self.entropy = entropy(self.get_pdf(), base = 2)\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.moving_window.MovingWindow.get_pdf","title":"<code>get_pdf()</code>","text":"<p>Compute the probability distribution function from an array of data.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/moving_window.py</code> <pre><code>def get_pdf(self):\n\"\"\"Compute the probability distribution function from an array of data.\"\"\"\n    counts = {}\n\n    for i in range(0, self.T):\n        t = tuple(self.window[i, :])\n        if t in counts:\n            counts[t] += 1\n        else:\n            counts[t] = 1\n\n    pdf = {k: v / self.T for k, v in counts.items()}\n\n    return list(pdf.values())\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.moving_window.MovingWindow.optimal_sampling","title":"<code>optimal_sampling(thres)</code>","text":"<p>Find the optimal number of sample for a particular moving window.</p> <p>Parameters:</p> Name Type Description Default <code>thres</code> <code>float</code> <p>stopping criteria threshold.</p> required Source code in <code>causalflow/preprocessing/subsampling_methods/moving_window.py</code> <pre><code>def optimal_sampling(self, thres):\n\"\"\"\n    Find the optimal number of sample for a particular moving window.\n\n    Args:\n        thres (float): stopping criteria threshold.\n    \"\"\"\n    converged = False\n    _old_step = 0\n    _sub_index = list(range(0, self.T))\n    _old_sub_index = list(range(0, self.T))\n    _max_n = math.floor(self.T / 2)\n\n    for n in range(_max_n, 1, -1):\n        # resampling window with n samples and build another Moving Window\n        step = int(self.T / n)\n        if step == _old_step:\n            continue\n        _old_step = step\n        _old_sub_index = _sub_index\n        _sub_index = self.samples_selector(step)\n        _sub_w = MovingWindow(self.window[_sub_index])\n\n        # compute entropy on the sub window\n        _sub_w.get_entropy()\n\n        # stopping criteria\n        if self.entropy != 0:\n            if abs(_sub_w.entropy - self.entropy) / self.entropy &gt;= thres:\n                converged = True\n                break\n    self.opt_size = len(_old_sub_index) if converged else len(_sub_index)\n    self.opt_samples_index = _old_sub_index if converged else _sub_index\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.moving_window.MovingWindow.samples_selector","title":"<code>samples_selector(step)</code>","text":"<p>Select sample to be taken from a moving window.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>subsampling frequency.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>list of indexes corresponding to the sample to be taken.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/moving_window.py</code> <pre><code>def samples_selector(self, step) -&gt; list:\n\"\"\"\n    Select sample to be taken from a moving window.\n\n    Args:\n        step (int): subsampling frequency.\n\n    Returns:\n        list: list of indexes corresponding to the sample to be taken.\n    \"\"\"\n    return [i for i in range(0, self.T, step)]\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.Static.Static","title":"<code>Static</code>","text":"<p>             Bases: <code>SubsamplingMethod</code></p> <p>Subsample data by taking one sample each step-samples.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/Static.py</code> <pre><code>class Static(SubsamplingMethod):\n\"\"\"Subsample data by taking one sample each step-samples.\"\"\"\n\n    def __init__(self, step):\n\"\"\"\n        Class constructor.\n\n        Args:\n            step (int): integer subsampling step.\n\n        Raises:\n            ValueError: if step == None.\n        \"\"\"\n        super().__init__(SSMode.Static)\n        if step is None:\n            raise ValueError(\"step not specified\")\n        self.step = step\n\n    def run(self):\n\"\"\"Run subsampler.\"\"\"\n        return range(0, len(self.df.values), self.step)\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.Static.Static.__init__","title":"<code>__init__(step)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>integer subsampling step.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if step == None.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/Static.py</code> <pre><code>def __init__(self, step):\n\"\"\"\n    Class constructor.\n\n    Args:\n        step (int): integer subsampling step.\n\n    Raises:\n        ValueError: if step == None.\n    \"\"\"\n    super().__init__(SSMode.Static)\n    if step is None:\n        raise ValueError(\"step not specified\")\n    self.step = step\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.Static.Static.run","title":"<code>run()</code>","text":"<p>Run subsampler.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/Static.py</code> <pre><code>def run(self):\n\"\"\"Run subsampler.\"\"\"\n    return range(0, len(self.df.values), self.step)\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.WSDynamic.WSDynamic","title":"<code>WSDynamic</code>","text":"<p>             Bases: <code>SubsamplingMethod</code>, <code>EntropyBasedMethod</code></p> <p>Subsampling method with dynamic window size based on entropy analysis.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>class WSDynamic(SubsamplingMethod, EntropyBasedMethod):\n\"\"\"Subsampling method with dynamic window size based on entropy analysis.\"\"\"\n\n    def __init__(self, window_min_size, entropy_threshold):\n\"\"\"\n        Class constructor.\n\n        Args:\n            window_min_size (int): minimun window size.\n            entropy_threshold (float): entropy threshold.\n\n        Raises:\n            ValueError: if window_min_size == None.\n        \"\"\"\n        SubsamplingMethod.__init__(self, SSMode.WSDynamic)\n        EntropyBasedMethod.__init__(self, entropy_threshold)\n        if window_min_size is None:\n            raise ValueError(\"window_type = DYNAMIC but window_min_size not specified\")\n        self.wms = window_min_size\n        self.ws = None\n\n\n    def dataset_segmentation(self):\n\"\"\"Segment dataset based on breakpoint analysis and a min window size.\"\"\"\n        de = self.create_rounded_copy()\n        algo = rpt.Pelt(model = \"l2\", min_size = self.wms).fit(de)\n        seg_res = algo.predict(pen = 10)\n        self.segments = [(seg_res[i - 1], seg_res[i]) for i in range(1, len(seg_res))]\n        self.segments.insert(0, (0, seg_res[0]))\n\n\n    def run(self):\n\"\"\"\n        Run subsampler.\n\n        Returns:\n            (list[int]): indexes of the remaining samples.\n        \"\"\"\n        # build list of segment\n        self.dataset_segmentation()\n\n        # compute entropy moving window\n        self.moving_window_analysis()\n\n        # extracting subsampling procedure results\n        idxs = self.extract_indexes()\n\n        return idxs\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.WSDynamic.WSDynamic.__init__","title":"<code>__init__(window_min_size, entropy_threshold)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>window_min_size</code> <code>int</code> <p>minimun window size.</p> required <code>entropy_threshold</code> <code>float</code> <p>entropy threshold.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if window_min_size == None.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>def __init__(self, window_min_size, entropy_threshold):\n\"\"\"\n    Class constructor.\n\n    Args:\n        window_min_size (int): minimun window size.\n        entropy_threshold (float): entropy threshold.\n\n    Raises:\n        ValueError: if window_min_size == None.\n    \"\"\"\n    SubsamplingMethod.__init__(self, SSMode.WSDynamic)\n    EntropyBasedMethod.__init__(self, entropy_threshold)\n    if window_min_size is None:\n        raise ValueError(\"window_type = DYNAMIC but window_min_size not specified\")\n    self.wms = window_min_size\n    self.ws = None\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.WSDynamic.WSDynamic.dataset_segmentation","title":"<code>dataset_segmentation()</code>","text":"<p>Segment dataset based on breakpoint analysis and a min window size.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>def dataset_segmentation(self):\n\"\"\"Segment dataset based on breakpoint analysis and a min window size.\"\"\"\n    de = self.create_rounded_copy()\n    algo = rpt.Pelt(model = \"l2\", min_size = self.wms).fit(de)\n    seg_res = algo.predict(pen = 10)\n    self.segments = [(seg_res[i - 1], seg_res[i]) for i in range(1, len(seg_res))]\n    self.segments.insert(0, (0, seg_res[0]))\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.WSDynamic.WSDynamic.run","title":"<code>run()</code>","text":"<p>Run subsampler.</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>indexes of the remaining samples.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>def run(self):\n\"\"\"\n    Run subsampler.\n\n    Returns:\n        (list[int]): indexes of the remaining samples.\n    \"\"\"\n    # build list of segment\n    self.dataset_segmentation()\n\n    # compute entropy moving window\n    self.moving_window_analysis()\n\n    # extracting subsampling procedure results\n    idxs = self.extract_indexes()\n\n    return idxs\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic","title":"<code>WSFFTStatic</code>","text":"<p>             Bases: <code>SubsamplingMethod</code>, <code>EntropyBasedMethod</code></p> <p>Subsampling method with static window size based on Fourier analysis.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/WSFFTStatic.py</code> <pre><code>class WSFFTStatic(SubsamplingMethod, EntropyBasedMethod):\n\"\"\"Subsampling method with static window size based on Fourier analysis.\"\"\"\n\n    def __init__(self, sampling_time, entropy_threshold):\n\"\"\"\n        Class constructor.\n\n        Args:\n            sampling_time (float): timeseries sampling time.\n            entropy_threshold (float): entropy threshold.\n        \"\"\"\n        SubsamplingMethod.__init__(self, SSMode.WSFFTStatic)\n        EntropyBasedMethod.__init__(self, entropy_threshold)\n        self.sampling_time = sampling_time\n\n\n    def __fourier_window(self):\n\"\"\"\n        Compute window size based on Fourier analysis performed on dataframe.\n\n        Returns:\n            (int): window size\n        \"\"\"\n        N, dim = self.df.shape\n        xf = rfftfreq(N, self.sampling_time)\n        w_array = list()\n        for i in range(0, dim):\n            yf = np.abs(rfft(self.df.values[:, i]))\n\n            peak_indices, _ = scipy.signal.find_peaks(yf)\n            highest_peak_index = peak_indices[np.argmax(yf[peak_indices])]\n            w_array.append(ceil(1 / (2 * xf[highest_peak_index]) / self.sampling_time))\n            # fig, ax = pl.subplots()\n            # ax.plot(xf, yf)\n            # ax.plot(xf[highest_peak_index], np.abs(yf[highest_peak_index]), \"x\")\n            # pl.show()\n        return min(w_array)\n\n\n    def dataset_segmentation(self):\n\"\"\"Segments dataset with a fixed window size.\"\"\"\n        seg_res = [i for i in range(0, len(self.df.values), self.ws)]\n        self.segments = [(i, i + self.ws) for i in range(0, len(self.df.values) - self.ws, self.ws)]\n        if not seg_res.__contains__(len(self.df.values)):\n            self.segments.append((seg_res[-1], len(self.df.values)))\n            seg_res.append(len(self.df.values))\n\n\n    def run(self):\n\"\"\"\n        Run subsampler.\n\n        Returns:\n            (list[int]): indexes of the remaining samples.\n        \"\"\"\n        # define window size\n        self.ws = self.__fourier_window()\n\n        # build list of segment\n        self.dataset_segmentation()\n\n        # compute entropy moving window\n        self.moving_window_analysis()\n\n        # extracting subsampling procedure results\n        idxs = self.extract_indexes()\n\n        return idxs\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic.__fourier_window","title":"<code>__fourier_window()</code>","text":"<p>Compute window size based on Fourier analysis performed on dataframe.</p> <p>Returns:</p> Type Description <code>int</code> <p>window size</p> Source code in <code>causalflow/preprocessing/subsampling_methods/WSFFTStatic.py</code> <pre><code>def __fourier_window(self):\n\"\"\"\n    Compute window size based on Fourier analysis performed on dataframe.\n\n    Returns:\n        (int): window size\n    \"\"\"\n    N, dim = self.df.shape\n    xf = rfftfreq(N, self.sampling_time)\n    w_array = list()\n    for i in range(0, dim):\n        yf = np.abs(rfft(self.df.values[:, i]))\n\n        peak_indices, _ = scipy.signal.find_peaks(yf)\n        highest_peak_index = peak_indices[np.argmax(yf[peak_indices])]\n        w_array.append(ceil(1 / (2 * xf[highest_peak_index]) / self.sampling_time))\n        # fig, ax = pl.subplots()\n        # ax.plot(xf, yf)\n        # ax.plot(xf[highest_peak_index], np.abs(yf[highest_peak_index]), \"x\")\n        # pl.show()\n    return min(w_array)\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic.__init__","title":"<code>__init__(sampling_time, entropy_threshold)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>sampling_time</code> <code>float</code> <p>timeseries sampling time.</p> required <code>entropy_threshold</code> <code>float</code> <p>entropy threshold.</p> required Source code in <code>causalflow/preprocessing/subsampling_methods/WSFFTStatic.py</code> <pre><code>def __init__(self, sampling_time, entropy_threshold):\n\"\"\"\n    Class constructor.\n\n    Args:\n        sampling_time (float): timeseries sampling time.\n        entropy_threshold (float): entropy threshold.\n    \"\"\"\n    SubsamplingMethod.__init__(self, SSMode.WSFFTStatic)\n    EntropyBasedMethod.__init__(self, entropy_threshold)\n    self.sampling_time = sampling_time\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic.dataset_segmentation","title":"<code>dataset_segmentation()</code>","text":"<p>Segments dataset with a fixed window size.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/WSFFTStatic.py</code> <pre><code>def dataset_segmentation(self):\n\"\"\"Segments dataset with a fixed window size.\"\"\"\n    seg_res = [i for i in range(0, len(self.df.values), self.ws)]\n    self.segments = [(i, i + self.ws) for i in range(0, len(self.df.values) - self.ws, self.ws)]\n    if not seg_res.__contains__(len(self.df.values)):\n        self.segments.append((seg_res[-1], len(self.df.values)))\n        seg_res.append(len(self.df.values))\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic.run","title":"<code>run()</code>","text":"<p>Run subsampler.</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>indexes of the remaining samples.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/WSFFTStatic.py</code> <pre><code>def run(self):\n\"\"\"\n    Run subsampler.\n\n    Returns:\n        (list[int]): indexes of the remaining samples.\n    \"\"\"\n    # define window size\n    self.ws = self.__fourier_window()\n\n    # build list of segment\n    self.dataset_segmentation()\n\n    # compute entropy moving window\n    self.moving_window_analysis()\n\n    # extracting subsampling procedure results\n    idxs = self.extract_indexes()\n\n    return idxs\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.WSStatic.WSStatic","title":"<code>WSStatic</code>","text":"<p>             Bases: <code>SubsamplingMethod</code>, <code>EntropyBasedMethod</code></p> <p>Entropy based subsampling method with static window size.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/WSStatic.py</code> <pre><code>class WSStatic(SubsamplingMethod, EntropyBasedMethod):\n\"\"\"Entropy based subsampling method with static window size.\"\"\"\n\n    def __init__(self, window_size, entropy_threshold):\n\"\"\"\n        Class constructor.\n\n        Args:\n            window_size (int): minimun window size.\n            entropy_threshold (float): entropy threshold.\n\n        Raises:\n            ValueError: if window_size == None.\n        \"\"\"\n        SubsamplingMethod.__init__(self, SSMode.WSDynamic)\n        EntropyBasedMethod.__init__(self, entropy_threshold)\n        if window_size is None:\n            raise ValueError(\"window_type = STATIC but window_size not specified\")\n        self.ws = window_size\n\n\n    def dataset_segmentation(self):\n\"\"\"Segment dataset with a fixed window size.\"\"\"\n        seg_res = [i for i in range(0, len(self.df.values), self.ws)]\n        self.segments = [(i, i + self.ws) for i in range(0, len(self.df.values) - self.ws, self.ws)]\n        if not seg_res.__contains__(len(self.df.values)):\n            self.segments.append((seg_res[-1], len(self.df.values)))\n            seg_res.append(len(self.df.values))\n\n\n    def run(self):\n\"\"\"\n        Run subsampler.\n\n        Returns:\n            (list[int]): indexes of the remaining samples.\n        \"\"\"\n        # build list of segment\n        self.dataset_segmentation()\n\n        # compute entropy moving window\n        self.moving_window_analysis()\n\n        # extracting subsampling procedure results\n        idxs = self.extract_indexes()\n\n        return idxs\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.WSStatic.WSStatic.__init__","title":"<code>__init__(window_size, entropy_threshold)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>minimun window size.</p> required <code>entropy_threshold</code> <code>float</code> <p>entropy threshold.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if window_size == None.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/WSStatic.py</code> <pre><code>def __init__(self, window_size, entropy_threshold):\n\"\"\"\n    Class constructor.\n\n    Args:\n        window_size (int): minimun window size.\n        entropy_threshold (float): entropy threshold.\n\n    Raises:\n        ValueError: if window_size == None.\n    \"\"\"\n    SubsamplingMethod.__init__(self, SSMode.WSDynamic)\n    EntropyBasedMethod.__init__(self, entropy_threshold)\n    if window_size is None:\n        raise ValueError(\"window_type = STATIC but window_size not specified\")\n    self.ws = window_size\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.WSStatic.WSStatic.dataset_segmentation","title":"<code>dataset_segmentation()</code>","text":"<p>Segment dataset with a fixed window size.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/WSStatic.py</code> <pre><code>def dataset_segmentation(self):\n\"\"\"Segment dataset with a fixed window size.\"\"\"\n    seg_res = [i for i in range(0, len(self.df.values), self.ws)]\n    self.segments = [(i, i + self.ws) for i in range(0, len(self.df.values) - self.ws, self.ws)]\n    if not seg_res.__contains__(len(self.df.values)):\n        self.segments.append((seg_res[-1], len(self.df.values)))\n        seg_res.append(len(self.df.values))\n</code></pre>"},{"location":"preprocessing/#causalflow.preprocessing.subsampling_methods.WSStatic.WSStatic.run","title":"<code>run()</code>","text":"<p>Run subsampler.</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>indexes of the remaining samples.</p> Source code in <code>causalflow/preprocessing/subsampling_methods/WSStatic.py</code> <pre><code>def run(self):\n\"\"\"\n    Run subsampler.\n\n    Returns:\n        (list[int]): indexes of the remaining samples.\n    \"\"\"\n    # build list of segment\n    self.dataset_segmentation()\n\n    # compute entropy moving window\n    self.moving_window_analysis()\n\n    # extracting subsampling procedure results\n    idxs = self.extract_indexes()\n\n    return idxs\n</code></pre>"},{"location":"random_system/","title":"RandomGraph","text":"<p>This module provides various classes for the creation of random system of equations.</p> Classes <p>NoiseType: support class for handling different types of noise. PriorityOp: support class for handling different types of operator. RandomGraph: facilitate the creation of random graphs.</p>"},{"location":"random_system/#causalflow.random_system.RandomGraph.NoiseType","title":"<code>NoiseType</code>","text":"<p>             Bases: <code>Enum</code></p> <p>NoiseType Enumerator.</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>class NoiseType(Enum):\n\"\"\"NoiseType Enumerator.\"\"\"\n\n    Uniform = 'uniform'\n    Gaussian = 'gaussian'\n    Weibull = 'weibull'\n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.PriorityOp","title":"<code>PriorityOp</code>","text":"<p>             Bases: <code>Enum</code></p> <p>PriorityOp Enumerator.</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>class PriorityOp(Enum):\n\"\"\"PriorityOp Enumerator.\"\"\"\n\n    M = '*'\n    D = '/'\n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph","title":"<code>RandomGraph</code>","text":"<p>RandomGraph Class.</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>class RandomGraph:\n\"\"\"RandomGraph Class.\"\"\"\n\n    def __init__(self, nvars, nsamples, link_density, coeff_range: tuple, \n                 min_lag, max_lag, max_exp = None, noise_config: tuple = None, \n                 operators = ['+', '-', '*'], \n                 functions = ['','sin', 'cos', 'exp', 'abs', 'pow'],\n                 n_hidden_confounders = 0,\n                 n_confounded_vars = None):\n\"\"\"\n        Class constructor.\n\n        Args:\n            nvars (int): Number of variable.\n            nsamples (int): Number of samples.\n            link_density (int): Max number of parents per variable.\n            coeff_range (tuple): Coefficient range. E.g. (-1, 1).\n            min_lag (int): Min lagged dependency.\n            max_lag (int): Max lagged dependency.\n            max_exp (int): Max permitted exponent used by the 'pow' function. Used only if 'pow' is in the list of functions. Defaults to None.\n            noise_config (tuple, optional): Noise configuration, e.g. (NoiseType.Uniform, -0.1, 0.1). Defaults to None.\n            operators (list, optional): list of possible operators between variables. Defaults to ['+', '-', '*'].\n            functions (list, optional): list of possible functions. Defaults to ['','sin', 'cos', 'exp', 'abs', 'pow'].\n            n_hidden_confounders (int, optional): Number of hidden confounders. Defaults to 0.\n            n_confounded_vars (int, optional): Number of confounded variables. If None, n_confounded_vars will be set as random.randint(2, nvars). Defaults to None.\n\n        Raises:\n            ValueError: max_exp cannot be None if functions list contains pow.\n        \"\"\"\n        if 'pow' in functions and max_exp is None:\n            raise ValueError('max_exp cannot be None if functions list contains pow')\n\n        self.T = nsamples\n        self.link_density = link_density\n        self.coeff_range = coeff_range\n        self.exponents = list(range(0, max_exp))\n        self.min_lag = min_lag\n        self.max_lag = max_lag\n        self.n_hidden_confounders = n_hidden_confounders\n        self.n_confounded = n_confounded_vars\n\n        self.obsVar = ['X_' + str(i) for i in range(nvars)]\n        self.hiddenVar = ['H_' + str(i) for i in range(n_hidden_confounders)]\n        self.operators = operators\n        self.functions = functions\n        self.equations = {var: list() for var in self.obsVar + self.hiddenVar}\n        self.confounders = {h: list() for h in self.hiddenVar}\n        self.dependency_graph = {var: set() for var in self.obsVar + self.hiddenVar}\n        self.PAG = None\n\n        self.noise_config = noise_config\n        self.noise = None\n        if noise_config is not None:\n            if noise_config[0] is NoiseType.Uniform:\n                self.noise = np.random.uniform(noise_config[1], noise_config[2], (self.T, self.N))\n            elif noise_config[0] is NoiseType.Gaussian:\n                self.noise = np.random.normal(noise_config[1], noise_config[2], (self.T, self.N))\n            elif noise_config[0] is NoiseType.Weibull:\n                self.noise = np.random.weibull(noise_config[1], (self.T, self.N)) * noise_config[2]\n\n\n    @property            \n    def variables(self) -&gt; list:\n\"\"\"\n        Retrieve the full set of observed and hidden variables.\n\n        Returns:\n            list: A list containing both observed and hidden variables.\n        \"\"\"\n        return self.obsVar + self.hiddenVar \n\n\n    @property            \n    def Nobs(self) -&gt; int:\n\"\"\"\n        Return number of observable variables.\n\n        Returns:\n            int: number of observable variables.\n        \"\"\"\n        return len(self.obsVar) \n\n\n    @property            \n    def N(self) -&gt; int:\n\"\"\"\n        Return total number of variables (observed and hidden).\n\n        Returns:\n            int: total number of variables.\n        \"\"\"\n        return len(self.obsVar) + len(self.hiddenVar)\n\n\n    @property\n    def obsEquations(self) -&gt; dict:\n\"\"\"\n        Return equations corresponding to the observed variables.\n\n        Returns:\n            dict: equations corresponding to the observed variables.\n        \"\"\"\n        tmp = copy.deepcopy(self.equations)\n        for h in self.hiddenVar: del tmp[h]\n        return tmp\n\n\n    def __build_equation(self, var_lagged_choice: list, var_contemp_choice: list, target_var) -&gt; list:\n\"\"\"\n        Generate random equations.\n\n        Args:\n            var_lagged_choice (list): list of possible lagged parents for the target variable.\n            var_contemp_choice (list): list of possible contemporaneous parents for the target variable.\n            target_var (str): target variable.\n\n        Returns:\n            list: equation (list of tuple).\n        \"\"\"\n        no_cycles_attempt = 0\n        equation = []\n        n_parents = random.randint(1, self.link_density)\n        while len(equation) &lt; n_parents:\n            coefficient = random.uniform(self.coeff_range[0], self.coeff_range[1])\n            lag = random.randint(self.min_lag, self.max_lag)\n            if lag != 0:\n                variable = random.choice(var_lagged_choice)\n                var_lagged_choice.remove(variable)\n            else:\n                variable = random.choice(var_contemp_choice)\n                var_contemp_choice.remove(variable)\n\n            if not self.__creates_cycle((target_var, 0), (variable, lag)):\n                operator = random.choice(self.operators)\n                function = random.choice(self.functions)\n                if function == 'pow':\n                    exponent = random.choice(self.exponents)\n                    term = (operator, coefficient, function, variable, lag, exponent)\n                else:\n                    term = (operator, coefficient, function, variable, lag)\n                equation.append(term)\n            else:\n                no_cycles_attempt += 1\n                if no_cycles_attempt &gt;= NO_CYCLES_THRESHOLD:\n                    raise ValueError(\"Cycle configuration impossible to be avoided!\")\n\n        return equation\n\n\n    def __creates_cycle(self, target_var_lag, variable_lag) -&gt; bool:\n\"\"\"\n        Check the presence of cycles.\n\n        Specifically, it checks whether adding an edge from variable_lag to target_var_lag \n        creates a cycle considering only the same time lag\n\n        Args:\n            target_var_lag (str): target node.\n            variable_lag (str): source node.\n\n        Returns:\n            bool: True if it finds cycles. Otherwise False.\n        \"\"\"\n        target_var, target_lag = target_var_lag\n        variable, lag = variable_lag\n\n        visited = set()\n        stack = [(variable, lag, [(variable, lag)], lag - target_lag)]\n        while stack:\n            current_var, current_lag, path, initial_lag_diff = stack.pop()\n            if (current_var, current_lag) == target_var_lag:\n                print(f\"Cycle path: {' -&gt; '.join([f'{var} (lag {l})' for var, l in [target_var_lag] + path])}\")\n                return True\n            if (current_var, current_lag) not in visited:\n                visited.add((current_var, current_lag))\n                for neighbor_var, neighbor_lag in self.dependency_graph.get(current_var, []):\n                    if (neighbor_var, neighbor_lag) not in visited:\n                        # Check if the lag difference is the same as the initial lag difference\n                        if (neighbor_lag - current_lag) == initial_lag_diff:\n                            stack.append((neighbor_var, neighbor_lag, path + [(neighbor_var, neighbor_lag)], initial_lag_diff))\n        # Update dependency graph\n        self.dependency_graph[target_var].add(variable_lag)\n        return False\n\n\n    def gen_equations(self):\n\"\"\"Generate random equations using the operator and function lists provided in the constructor.\"\"\"\n        for var in self.obsVar:\n            var_lagged_choice = copy.deepcopy(self.obsVar)\n            var_contemp_choice = copy.deepcopy(var_lagged_choice)\n            var_contemp_choice.remove(var)\n            self.equations[var] = self.__build_equation(var_lagged_choice, var_contemp_choice, var)\n\n        for hid in self.hiddenVar:\n            var_lagged_choice = copy.deepcopy(self.obsVar + self.hiddenVar)\n            var_contemp_choice = copy.deepcopy(var_lagged_choice)\n            var_contemp_choice.remove(hid)\n            self.equations[hid] = self.__build_equation(var_lagged_choice, var_contemp_choice, hid)\n\n        self.__add_conf_links()\n\n\n    def __add_conf_links(self):\n\"\"\"Add confounder links to a predefined causal model.\"\"\"\n        no_cycles_attempt = 0\n        self.expected_bidirected_links = list()\n        firstvar_choice = copy.deepcopy(self.obsVar)\n        for hid in self.hiddenVar:\n            tmp_n_confounded = 0\n            isContemporaneous = random.choice([True, False])\n            n_confounded = random.randint(2, self.Nobs) if self.n_confounded is None else self.n_confounded \n\n            if isContemporaneous:\n                lag = random.randint(self.min_lag, self.max_lag)\n                confVar = list()\n                while tmp_n_confounded &lt; n_confounded:\n                    variable = random.choice(firstvar_choice)\n\n                    if not self.__creates_cycle((variable, 0), (hid, lag)):\n                        tmp_n_confounded += 1\n                        firstvar_choice.remove(variable)\n                        confVar.append(variable)\n\n                        function = random.choice(self.functions)\n                        coefficient = random.uniform(self.coeff_range[0], self.coeff_range[1])\n                        operator = random.choice(self.operators)\n                        if function == 'pow':\n                            exponent = random.choice(self.exponents)\n                            term = (operator, coefficient, function, hid, lag, exponent)\n                        else:\n                            term = (operator, coefficient, function, hid, lag)\n\n                        #! NOTE: This is to remove the true link between confounded variable for ensuring \n                        #! that the link due to the confounder is classified as spurious\n                        if len(confVar) &gt; 1:\n                            for source in confVar:\n                                tmp = copy.deepcopy(confVar)\n                                tmp.remove(source)\n                                for target in tmp:\n                                    if (source, 0) in self.get_Adj()[target]:\n                                        self.equations[target] = list(filter(lambda item: item[3] != source and item[3] != 0, self.equations[target]))\n                        self.equations[variable].append(term)\n\n                        self.confounders[hid].append((variable, lag))\n                    else:\n                        no_cycles_attempt += 1\n                        if no_cycles_attempt &gt;= NO_CYCLES_THRESHOLD:\n                            raise ValueError(\"Impossible to avoid the cycle configuration!\")\n                for source in confVar:\n                    tmp = copy.deepcopy(confVar)\n                    tmp.remove(source)\n                    for target in tmp:\n                        if not (source, 0) in self.get_Adj()[target]:\n                            self.expected_bidirected_links.append({target: (source, 0)})\n            else:    \n                var_choice = copy.deepcopy(self.obsVar)\n                firstConf = True\n                source = None\n                sourceLag = None\n                targets = list()\n\n                while tmp_n_confounded &lt; n_confounded:\n                    if firstConf:\n                        variable = random.choice(firstvar_choice)\n                        lag = random.randint(self.min_lag, self.max_lag - 1)\n                        sourceLag = lag\n                    else:\n                        variable = random.choice(var_choice)\n                        lag = random.randint(sourceLag + 1, self.max_lag)\n\n                    if not self.__creates_cycle((variable, 0), (hid, lag)):\n                        tmp_n_confounded += 1\n                        var_choice.remove(variable)\n                        if firstConf:\n                            firstvar_choice.remove(variable)\n                            firstConf = False\n                            source = variable\n                        else:\n                            targets.append((variable, lag))\n\n                            #! NOTE: This is to remove the true link between confounded variable for ensuring \n                            #! that the link due to the confounder is classified as spurious\n                            if (source, lag - sourceLag) in self.get_Adj()[variable]:\n                                self.equations[variable] = list(filter(lambda item: item[3] != source and item[3] != lag - sourceLag, self.equations[variable]))\n\n                        function = random.choice(self.functions)\n                        coefficient = random.uniform(self.coeff_range[0], self.coeff_range[1])\n                        operator = random.choice(self.operators)\n                        if function == 'pow':\n                            exponent = random.choice(self.exponents)\n                            term = (operator, coefficient, function, hid, lag, exponent)\n                        else:\n                            term = (operator, coefficient, function, hid, lag)\n                        self.equations[variable].append(term)\n\n                        self.confounders[hid].append((variable, lag))\n                    else:\n                        no_cycles_attempt += 1\n                        if no_cycles_attempt &gt;= NO_CYCLES_THRESHOLD:\n                            raise ValueError(\"Cycle configuration impossible to be avoided!\")\n\n                # Lagged bidirected links\n                for v in targets:\n                    if not (source, v[1] - sourceLag) in self.get_Adj()[v[0]]:\n                        self.expected_bidirected_links.append({v[0]: (source, v[1] - sourceLag)})\n                # Contemporaneous bidirected links\n                for source in targets:\n                    tmp = copy.deepcopy(targets)\n                    tmp.remove(source)\n                    for target in tmp:\n                        if not (source, 0) in self.get_Adj()[target[0]]:\n                            self.expected_bidirected_links.append({target[0]: (source[0], 0)})\n\n\n    def print_equations(self):\n\"\"\"Print the generated equations.\"\"\"\n        toprint = list()\n        for target, eq in self.equations.items():\n            equation_str = target + '(t) = '\n            for i, term in enumerate(eq):\n                if len(term) == 6:\n                    operator, coefficient, function, variable, lag, exponent = term\n                    coefficient = round(coefficient, 2)\n                    if i != 0: \n                        term_str = f\"{operator} {coefficient} * {function}({variable}, {exponent})(t-{lag}) \"\n                    else:\n                        term_str = f\"{coefficient} * {function}({variable}, {exponent})(t-{lag}) \"\n                else:\n                    operator, coefficient, function, variable, lag = term\n                    coefficient = round(coefficient, 2)\n                    if function != '':\n                        if i != 0: \n                            term_str = f\"{operator} {coefficient} * {function}({variable})(t-{lag}) \"\n                        else:\n                            term_str = f\"{coefficient} * {function}({variable})(t-{lag}) \"\n                    else:\n                        if i != 0: \n                            term_str = f\"{operator} {coefficient} * {variable}(t-{lag}) \"\n                        else:\n                            term_str = f\"{coefficient} * {variable}(t-{lag}) \"\n\n                equation_str += term_str\n            toprint.append(equation_str)\n        eq = \"\\n\".join(toprint)\n        print(eq)\n        return eq\n\n\n    def __evaluate_term(self, term, t, data) -&gt; tuple:\n\"\"\"\n        Evaluate single term componing an equation.\n\n        Args:\n            term (tuple): term to evaluate.\n            t (int): time step.\n            data (numpy array): time-series.\n\n        Returns:\n            tuple: operator and value of the term.\n        \"\"\"\n        operator, coefficient, function, variable, *args = term\n        if function == '':\n            lag = args[0]\n            term_value = coefficient * (data[t - lag, self.variables.index(variable)])\n        elif function == 'pow':\n            lag, exponent = args\n            term_value = coefficient * data[t - lag, self.variables.index(variable)] ** exponent\n        elif function == 'abs':\n            lag = args[0]\n            term_value = coefficient * abs(data[t - lag, self.variables.index(variable)])\n        else:\n            lag = args[0]\n            term_value = coefficient * getattr(math, function)(data[t - lag, self.variables.index(variable)])\n        return operator, term_value\n\n\n    def __handle_priority_operator(self, eq) -&gt; list:\n\"\"\"\n        Evaluate all the terms with operato * ans /.\n\n        Args:\n            eq (list): equation (list of term).\n\n        Returns:\n            list: equation with all * and / evaluated.\n        \"\"\"\n        op = '*'\n        while (op in eq):\n            op_i = eq.index(op)\n            op1_i = op_i - 1\n            op2_i = op_i + 1\n            eq[op1_i] = eq[op1_i] * eq[op2_i]\n\n            indices_set = set([op_i, op2_i])\n            eq = [item for i, item in enumerate(eq) if i not in indices_set]\n\n        op = '/'\n        while (op in eq):\n            op_i = eq.index(op)\n            op1_i = op_i - 1\n            op2_i = op_i + 1\n            eq[op1_i] = eq[op1_i] / eq[op2_i]\n\n            indices_set = set([op_i, op2_i])\n            eq = [item for i, item in enumerate(eq) if i not in indices_set]\n\n        return eq\n\n\n    def __evaluate_equation(self, equation, t, data) -&gt; float:\n\"\"\"\n        Evaluate equation.\n\n        Args:\n            equation (list): equation (list of term).\n            t (int): time step.\n            data (numpy array): time-series.\n\n        Returns:\n            float: equation value.\n        \"\"\"\n        eq = list()\n        for i, term in enumerate(equation):\n            operator, term = self.__evaluate_term(term, t, data)\n            if i == 0:\n                eq.append(term)\n            else:\n                eq.append(operator)\n                eq.append(term)\n\n        # Handle * and / before + and -\n        eq = self.__handle_priority_operator(eq)\n\n        equation_value = eq.pop(0)\n        for i in range(0, len(eq), 2):\n            op = eq[i]\n            term = eq[i+1]\n            if op == '+': equation_value = equation_value + term\n            elif op == '-': equation_value = equation_value - term\n        return equation_value\n\n\n    def gen_obs_ts(self) -&gt; tuple:\n\"\"\"\n        Generate time-series data.\n\n        Returns:\n            tuple: (Data obj with hidden vars, Data obj without hidden vars).\n        \"\"\"\n        np_data = np.zeros((self.T, self.N))\n        for t in range(self.T):\n            if t &lt; self.max_lag:\n                for target, eq in self.equations.items():\n                    np_data[t, self.variables.index(target)] = self.noise[t, self.variables.index(target)]\n            else:\n                for target, eq in self.equations.items():\n                    np_data[t, self.variables.index(target)] = self.__evaluate_equation(eq, t, np_data)\n                    if self.noise is not None: np_data[t, self.variables.index(target)] += self.noise[t, self.variables.index(target)]\n\n        data = Data(np_data, self.variables)\n        only_obs = copy.deepcopy(data)\n        only_obs.shrink(self.obsVar)\n        return data, only_obs\n\n\n    def gen_interv_ts(self, interventions, obs) -&gt; dict:\n\"\"\"\n        Generate time-series corresponding to intervention(s).\n\n        Args:\n            interventions (dict): dictionary {INT_VAR : {INT_LEN: int_len, INT_VAL: int_val}}.\n            obs (DataFrame): Observational DataFrame.\n\n        Returns:\n            dict: {interventional variable: interventional time-series data}.\n        \"\"\"\n        starting_point = obs.values\n        int_data = dict()\n        for int_var in interventions:\n            T = int(interventions[int_var][\"T\"])\n            if self.noise_config is not None:\n                if self.noise_config[0] is NoiseType.Uniform:\n                    int_noise = np.random.uniform(self.noise_config[1], self.noise_config[2], (T, self.N))\n                elif self.noise_config[0] is NoiseType.Gaussian:\n                    int_noise = np.random.normal(self.noise_config[1], self.noise_config[2], (T, self.N))\n                elif self.noise_config[0] is NoiseType.Weibull:\n                    int_noise= np.random.weibull(self.noise_config[1], (self.T, self.N)) * self.noise_config[2]\n            np_data = np.zeros((T, self.N))\n            np_data[0:self.max_lag, :] = starting_point[len(starting_point)-self.max_lag:,:]\n\n            for t in range(self.max_lag, T):\n                for target, eq in self.equations.items():\n                    if target != int_var:\n                        np_data[t, self.variables.index(target)] = self.__evaluate_equation(eq, t, np_data)\n                        if self.noise_config is not None: np_data[t, self.variables.index(target)] += int_noise[t, self.variables.index(target)]\n                    else:\n                        np_data[t, self.variables.index(target)] = interventions[int_var][\"VAL\"]\n\n            int_data[int_var] = Data(np_data, self.variables)\n            int_data[int_var].shrink(self.obsVar)\n            starting_point = np_data\n        return int_data\n\n\n    def get_DPAG(self) -&gt; dict:\n\"\"\"\n        Output the PAG starting from a DAG.\n\n        Returns:\n            dict: scm.\n        \"\"\"\n        if self.PAG is None:\n            scm = self.get_Adj(withHidden=True)\n            self.PAG = PAG(scm, self.max_lag, self.hiddenVar)\n        return self.PAG.convert2Graph()\n\n\n    def get_Adj(self, withHidden = False) -&gt; dict:\n\"\"\"\n        Output the Structural Causal Model.\n\n        Returns:\n            dict: scm.\n        \"\"\"\n        eqs = self.equations if withHidden else self.obsEquations\n        scm = {target : list() for target in eqs.keys()}\n        for target, eq in eqs.items():\n            for term in eq:\n                if len(term) == 6:\n                    _, _, _, variable, lag, _ = term\n                else:\n                    _, _, _, variable, lag = term\n                if variable not in scm.keys(): continue # NOTE: this is needed to avoid adding hidden vars\n                scm[target].append((variable, -abs(lag)))\n        return scm\n\n\n    def print_SCM(self, withHidden = False):\n\"\"\"Print the Structural Causal Model.\"\"\"\n        scm = self.get_Adj(withHidden)\n        for t in scm: print(t + ' : ' + str(scm[t]))    \n\n\n    def intervene(self, int_var, int_len, int_value, obs) -&gt; dict:\n\"\"\"\n        Generate intervention on a single variable.\n\n        Args:\n            int_var (str): variable name.\n            int_len (int): intervention length.\n            int_value (float): intervention value.\n            obs (DataFrame): Observational DataFrame.\n\n        Returns:\n            dict: {interventional variable: interventional time-series data}.\n        \"\"\"\n        if not isinstance(int_var, list): int_var = [int_var]\n        if not isinstance(int_len, list): int_len = [int_len]\n        if not isinstance(int_value, list): int_value = [int_value]\n        return self.gen_interv_ts({v: {\"T\": l, \"VAL\": val} for v, l, val in zip(int_var, int_len, int_value)}, obs)\n\n\n    def ts_dag(self, withHidden = False, save_name = None, randomColors = False):\n\"\"\"\n        Draw a Time-seris DAG.\n\n        Args:\n            withHidden (bool, optional): bit to decide whether to output the SCM including the hidden variables or not. Defaults to False.\n            save_name (str, optional): figure path. Defaults to None.\n            randomColors (bool, optional): random color for each node. Defaults to False.\n        \"\"\"\n        gt = self.get_Adj(withHidden) if withHidden else self.get_DPAG()\n        var = self.variables if withHidden else self.obsVar\n        g = DAG(var, self.min_lag, self.max_lag, False, gt)\n        node_color = []\n\n        tab_colors = plt.cm.get_cmap('tab20', 20).colors  # You can adjust the number of colors if needed\n        avail_tab_colors = list(copy.deepcopy(tab_colors))\n        for t in g.g:\n            if t in self.hiddenVar:\n                node_color.append('peachpuff')\n            else:\n                if randomColors :\n                    c = random.randint(0, len(avail_tab_colors)-1)\n                    node_color.append(avail_tab_colors[c])\n                    avail_tab_colors.pop(c)\n                else:\n                    node_color.append('orange')\n\n        # Edges color definition\n        edge_color = dict()\n        for t in g.g:\n            for s in g.g[t].sources:\n                s_index = len(g.g.keys())-1 - list(g.g.keys()).index(s[0])\n                t_index = len(g.g.keys())-1 - list(g.g.keys()).index(t)\n\n                s_lag = self.max_lag - s[1]\n                t_lag = self.max_lag\n                while s_lag &gt;= 0:\n                    s_node = (s_lag, s_index)\n                    t_node = (t_lag, t_index)\n                    if s[0] in self.hiddenVar:\n                        edge_color[(s_node, t_node)] = 'gainsboro'\n                    else:\n                        edge_color[(s_node, t_node)] = 'gray'\n\n                    s_lag -= 1\n                    t_lag -= 1\n\n        g.ts_dag(save_name = save_name, node_color = node_color, edge_color = edge_color, min_width=2, max_width=5, x_disp=1, node_size=6)\n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.N","title":"<code>N: int</code>  <code>property</code>","text":"<p>Return total number of variables (observed and hidden).</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>total number of variables.</p>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.Nobs","title":"<code>Nobs: int</code>  <code>property</code>","text":"<p>Return number of observable variables.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>number of observable variables.</p>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.obsEquations","title":"<code>obsEquations: dict</code>  <code>property</code>","text":"<p>Return equations corresponding to the observed variables.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>equations corresponding to the observed variables.</p>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.variables","title":"<code>variables: list</code>  <code>property</code>","text":"<p>Retrieve the full set of observed and hidden variables.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list containing both observed and hidden variables.</p>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.__add_conf_links","title":"<code>__add_conf_links()</code>","text":"<p>Add confounder links to a predefined causal model.</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>def __add_conf_links(self):\n\"\"\"Add confounder links to a predefined causal model.\"\"\"\n    no_cycles_attempt = 0\n    self.expected_bidirected_links = list()\n    firstvar_choice = copy.deepcopy(self.obsVar)\n    for hid in self.hiddenVar:\n        tmp_n_confounded = 0\n        isContemporaneous = random.choice([True, False])\n        n_confounded = random.randint(2, self.Nobs) if self.n_confounded is None else self.n_confounded \n\n        if isContemporaneous:\n            lag = random.randint(self.min_lag, self.max_lag)\n            confVar = list()\n            while tmp_n_confounded &lt; n_confounded:\n                variable = random.choice(firstvar_choice)\n\n                if not self.__creates_cycle((variable, 0), (hid, lag)):\n                    tmp_n_confounded += 1\n                    firstvar_choice.remove(variable)\n                    confVar.append(variable)\n\n                    function = random.choice(self.functions)\n                    coefficient = random.uniform(self.coeff_range[0], self.coeff_range[1])\n                    operator = random.choice(self.operators)\n                    if function == 'pow':\n                        exponent = random.choice(self.exponents)\n                        term = (operator, coefficient, function, hid, lag, exponent)\n                    else:\n                        term = (operator, coefficient, function, hid, lag)\n\n                    #! NOTE: This is to remove the true link between confounded variable for ensuring \n                    #! that the link due to the confounder is classified as spurious\n                    if len(confVar) &gt; 1:\n                        for source in confVar:\n                            tmp = copy.deepcopy(confVar)\n                            tmp.remove(source)\n                            for target in tmp:\n                                if (source, 0) in self.get_Adj()[target]:\n                                    self.equations[target] = list(filter(lambda item: item[3] != source and item[3] != 0, self.equations[target]))\n                    self.equations[variable].append(term)\n\n                    self.confounders[hid].append((variable, lag))\n                else:\n                    no_cycles_attempt += 1\n                    if no_cycles_attempt &gt;= NO_CYCLES_THRESHOLD:\n                        raise ValueError(\"Impossible to avoid the cycle configuration!\")\n            for source in confVar:\n                tmp = copy.deepcopy(confVar)\n                tmp.remove(source)\n                for target in tmp:\n                    if not (source, 0) in self.get_Adj()[target]:\n                        self.expected_bidirected_links.append({target: (source, 0)})\n        else:    \n            var_choice = copy.deepcopy(self.obsVar)\n            firstConf = True\n            source = None\n            sourceLag = None\n            targets = list()\n\n            while tmp_n_confounded &lt; n_confounded:\n                if firstConf:\n                    variable = random.choice(firstvar_choice)\n                    lag = random.randint(self.min_lag, self.max_lag - 1)\n                    sourceLag = lag\n                else:\n                    variable = random.choice(var_choice)\n                    lag = random.randint(sourceLag + 1, self.max_lag)\n\n                if not self.__creates_cycle((variable, 0), (hid, lag)):\n                    tmp_n_confounded += 1\n                    var_choice.remove(variable)\n                    if firstConf:\n                        firstvar_choice.remove(variable)\n                        firstConf = False\n                        source = variable\n                    else:\n                        targets.append((variable, lag))\n\n                        #! NOTE: This is to remove the true link between confounded variable for ensuring \n                        #! that the link due to the confounder is classified as spurious\n                        if (source, lag - sourceLag) in self.get_Adj()[variable]:\n                            self.equations[variable] = list(filter(lambda item: item[3] != source and item[3] != lag - sourceLag, self.equations[variable]))\n\n                    function = random.choice(self.functions)\n                    coefficient = random.uniform(self.coeff_range[0], self.coeff_range[1])\n                    operator = random.choice(self.operators)\n                    if function == 'pow':\n                        exponent = random.choice(self.exponents)\n                        term = (operator, coefficient, function, hid, lag, exponent)\n                    else:\n                        term = (operator, coefficient, function, hid, lag)\n                    self.equations[variable].append(term)\n\n                    self.confounders[hid].append((variable, lag))\n                else:\n                    no_cycles_attempt += 1\n                    if no_cycles_attempt &gt;= NO_CYCLES_THRESHOLD:\n                        raise ValueError(\"Cycle configuration impossible to be avoided!\")\n\n            # Lagged bidirected links\n            for v in targets:\n                if not (source, v[1] - sourceLag) in self.get_Adj()[v[0]]:\n                    self.expected_bidirected_links.append({v[0]: (source, v[1] - sourceLag)})\n            # Contemporaneous bidirected links\n            for source in targets:\n                tmp = copy.deepcopy(targets)\n                tmp.remove(source)\n                for target in tmp:\n                    if not (source, 0) in self.get_Adj()[target[0]]:\n                        self.expected_bidirected_links.append({target[0]: (source[0], 0)})\n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.__build_equation","title":"<code>__build_equation(var_lagged_choice, var_contemp_choice, target_var)</code>","text":"<p>Generate random equations.</p> <p>Parameters:</p> Name Type Description Default <code>var_lagged_choice</code> <code>list</code> <p>list of possible lagged parents for the target variable.</p> required <code>var_contemp_choice</code> <code>list</code> <p>list of possible contemporaneous parents for the target variable.</p> required <code>target_var</code> <code>str</code> <p>target variable.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>equation (list of tuple).</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>def __build_equation(self, var_lagged_choice: list, var_contemp_choice: list, target_var) -&gt; list:\n\"\"\"\n    Generate random equations.\n\n    Args:\n        var_lagged_choice (list): list of possible lagged parents for the target variable.\n        var_contemp_choice (list): list of possible contemporaneous parents for the target variable.\n        target_var (str): target variable.\n\n    Returns:\n        list: equation (list of tuple).\n    \"\"\"\n    no_cycles_attempt = 0\n    equation = []\n    n_parents = random.randint(1, self.link_density)\n    while len(equation) &lt; n_parents:\n        coefficient = random.uniform(self.coeff_range[0], self.coeff_range[1])\n        lag = random.randint(self.min_lag, self.max_lag)\n        if lag != 0:\n            variable = random.choice(var_lagged_choice)\n            var_lagged_choice.remove(variable)\n        else:\n            variable = random.choice(var_contemp_choice)\n            var_contemp_choice.remove(variable)\n\n        if not self.__creates_cycle((target_var, 0), (variable, lag)):\n            operator = random.choice(self.operators)\n            function = random.choice(self.functions)\n            if function == 'pow':\n                exponent = random.choice(self.exponents)\n                term = (operator, coefficient, function, variable, lag, exponent)\n            else:\n                term = (operator, coefficient, function, variable, lag)\n            equation.append(term)\n        else:\n            no_cycles_attempt += 1\n            if no_cycles_attempt &gt;= NO_CYCLES_THRESHOLD:\n                raise ValueError(\"Cycle configuration impossible to be avoided!\")\n\n    return equation\n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.__creates_cycle","title":"<code>__creates_cycle(target_var_lag, variable_lag)</code>","text":"<p>Check the presence of cycles.</p> <p>Specifically, it checks whether adding an edge from variable_lag to target_var_lag  creates a cycle considering only the same time lag</p> <p>Parameters:</p> Name Type Description Default <code>target_var_lag</code> <code>str</code> <p>target node.</p> required <code>variable_lag</code> <code>str</code> <p>source node.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if it finds cycles. Otherwise False.</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>def __creates_cycle(self, target_var_lag, variable_lag) -&gt; bool:\n\"\"\"\n    Check the presence of cycles.\n\n    Specifically, it checks whether adding an edge from variable_lag to target_var_lag \n    creates a cycle considering only the same time lag\n\n    Args:\n        target_var_lag (str): target node.\n        variable_lag (str): source node.\n\n    Returns:\n        bool: True if it finds cycles. Otherwise False.\n    \"\"\"\n    target_var, target_lag = target_var_lag\n    variable, lag = variable_lag\n\n    visited = set()\n    stack = [(variable, lag, [(variable, lag)], lag - target_lag)]\n    while stack:\n        current_var, current_lag, path, initial_lag_diff = stack.pop()\n        if (current_var, current_lag) == target_var_lag:\n            print(f\"Cycle path: {' -&gt; '.join([f'{var} (lag {l})' for var, l in [target_var_lag] + path])}\")\n            return True\n        if (current_var, current_lag) not in visited:\n            visited.add((current_var, current_lag))\n            for neighbor_var, neighbor_lag in self.dependency_graph.get(current_var, []):\n                if (neighbor_var, neighbor_lag) not in visited:\n                    # Check if the lag difference is the same as the initial lag difference\n                    if (neighbor_lag - current_lag) == initial_lag_diff:\n                        stack.append((neighbor_var, neighbor_lag, path + [(neighbor_var, neighbor_lag)], initial_lag_diff))\n    # Update dependency graph\n    self.dependency_graph[target_var].add(variable_lag)\n    return False\n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.__evaluate_equation","title":"<code>__evaluate_equation(equation, t, data)</code>","text":"<p>Evaluate equation.</p> <p>Parameters:</p> Name Type Description Default <code>equation</code> <code>list</code> <p>equation (list of term).</p> required <code>t</code> <code>int</code> <p>time step.</p> required <code>data</code> <code>numpy array</code> <p>time-series.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>equation value.</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>def __evaluate_equation(self, equation, t, data) -&gt; float:\n\"\"\"\n    Evaluate equation.\n\n    Args:\n        equation (list): equation (list of term).\n        t (int): time step.\n        data (numpy array): time-series.\n\n    Returns:\n        float: equation value.\n    \"\"\"\n    eq = list()\n    for i, term in enumerate(equation):\n        operator, term = self.__evaluate_term(term, t, data)\n        if i == 0:\n            eq.append(term)\n        else:\n            eq.append(operator)\n            eq.append(term)\n\n    # Handle * and / before + and -\n    eq = self.__handle_priority_operator(eq)\n\n    equation_value = eq.pop(0)\n    for i in range(0, len(eq), 2):\n        op = eq[i]\n        term = eq[i+1]\n        if op == '+': equation_value = equation_value + term\n        elif op == '-': equation_value = equation_value - term\n    return equation_value\n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.__evaluate_term","title":"<code>__evaluate_term(term, t, data)</code>","text":"<p>Evaluate single term componing an equation.</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>tuple</code> <p>term to evaluate.</p> required <code>t</code> <code>int</code> <p>time step.</p> required <code>data</code> <code>numpy array</code> <p>time-series.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>operator and value of the term.</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>def __evaluate_term(self, term, t, data) -&gt; tuple:\n\"\"\"\n    Evaluate single term componing an equation.\n\n    Args:\n        term (tuple): term to evaluate.\n        t (int): time step.\n        data (numpy array): time-series.\n\n    Returns:\n        tuple: operator and value of the term.\n    \"\"\"\n    operator, coefficient, function, variable, *args = term\n    if function == '':\n        lag = args[0]\n        term_value = coefficient * (data[t - lag, self.variables.index(variable)])\n    elif function == 'pow':\n        lag, exponent = args\n        term_value = coefficient * data[t - lag, self.variables.index(variable)] ** exponent\n    elif function == 'abs':\n        lag = args[0]\n        term_value = coefficient * abs(data[t - lag, self.variables.index(variable)])\n    else:\n        lag = args[0]\n        term_value = coefficient * getattr(math, function)(data[t - lag, self.variables.index(variable)])\n    return operator, term_value\n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.__handle_priority_operator","title":"<code>__handle_priority_operator(eq)</code>","text":"<p>Evaluate all the terms with operato * ans /.</p> <p>Parameters:</p> Name Type Description Default <code>eq</code> <code>list</code> <p>equation (list of term).</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>equation with all * and / evaluated.</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>def __handle_priority_operator(self, eq) -&gt; list:\n\"\"\"\n    Evaluate all the terms with operato * ans /.\n\n    Args:\n        eq (list): equation (list of term).\n\n    Returns:\n        list: equation with all * and / evaluated.\n    \"\"\"\n    op = '*'\n    while (op in eq):\n        op_i = eq.index(op)\n        op1_i = op_i - 1\n        op2_i = op_i + 1\n        eq[op1_i] = eq[op1_i] * eq[op2_i]\n\n        indices_set = set([op_i, op2_i])\n        eq = [item for i, item in enumerate(eq) if i not in indices_set]\n\n    op = '/'\n    while (op in eq):\n        op_i = eq.index(op)\n        op1_i = op_i - 1\n        op2_i = op_i + 1\n        eq[op1_i] = eq[op1_i] / eq[op2_i]\n\n        indices_set = set([op_i, op2_i])\n        eq = [item for i, item in enumerate(eq) if i not in indices_set]\n\n    return eq\n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.__init__","title":"<code>__init__(nvars, nsamples, link_density, coeff_range, min_lag, max_lag, max_exp=None, noise_config=None, operators=['+', '-', '*'], functions=['', 'sin', 'cos', 'exp', 'abs', 'pow'], n_hidden_confounders=0, n_confounded_vars=None)</code>","text":"<p>Class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>nvars</code> <code>int</code> <p>Number of variable.</p> required <code>nsamples</code> <code>int</code> <p>Number of samples.</p> required <code>link_density</code> <code>int</code> <p>Max number of parents per variable.</p> required <code>coeff_range</code> <code>tuple</code> <p>Coefficient range. E.g. (-1, 1).</p> required <code>min_lag</code> <code>int</code> <p>Min lagged dependency.</p> required <code>max_lag</code> <code>int</code> <p>Max lagged dependency.</p> required <code>max_exp</code> <code>int</code> <p>Max permitted exponent used by the 'pow' function. Used only if 'pow' is in the list of functions. Defaults to None.</p> <code>None</code> <code>noise_config</code> <code>tuple</code> <p>Noise configuration, e.g. (NoiseType.Uniform, -0.1, 0.1). Defaults to None.</p> <code>None</code> <code>operators</code> <code>list</code> <p>list of possible operators between variables. Defaults to ['+', '-', '*'].</p> <code>['+', '-', '*']</code> <code>functions</code> <code>list</code> <p>list of possible functions. Defaults to ['','sin', 'cos', 'exp', 'abs', 'pow'].</p> <code>['', 'sin', 'cos', 'exp', 'abs', 'pow']</code> <code>n_hidden_confounders</code> <code>int</code> <p>Number of hidden confounders. Defaults to 0.</p> <code>0</code> <code>n_confounded_vars</code> <code>int</code> <p>Number of confounded variables. If None, n_confounded_vars will be set as random.randint(2, nvars). Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>max_exp cannot be None if functions list contains pow.</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>def __init__(self, nvars, nsamples, link_density, coeff_range: tuple, \n             min_lag, max_lag, max_exp = None, noise_config: tuple = None, \n             operators = ['+', '-', '*'], \n             functions = ['','sin', 'cos', 'exp', 'abs', 'pow'],\n             n_hidden_confounders = 0,\n             n_confounded_vars = None):\n\"\"\"\n    Class constructor.\n\n    Args:\n        nvars (int): Number of variable.\n        nsamples (int): Number of samples.\n        link_density (int): Max number of parents per variable.\n        coeff_range (tuple): Coefficient range. E.g. (-1, 1).\n        min_lag (int): Min lagged dependency.\n        max_lag (int): Max lagged dependency.\n        max_exp (int): Max permitted exponent used by the 'pow' function. Used only if 'pow' is in the list of functions. Defaults to None.\n        noise_config (tuple, optional): Noise configuration, e.g. (NoiseType.Uniform, -0.1, 0.1). Defaults to None.\n        operators (list, optional): list of possible operators between variables. Defaults to ['+', '-', '*'].\n        functions (list, optional): list of possible functions. Defaults to ['','sin', 'cos', 'exp', 'abs', 'pow'].\n        n_hidden_confounders (int, optional): Number of hidden confounders. Defaults to 0.\n        n_confounded_vars (int, optional): Number of confounded variables. If None, n_confounded_vars will be set as random.randint(2, nvars). Defaults to None.\n\n    Raises:\n        ValueError: max_exp cannot be None if functions list contains pow.\n    \"\"\"\n    if 'pow' in functions and max_exp is None:\n        raise ValueError('max_exp cannot be None if functions list contains pow')\n\n    self.T = nsamples\n    self.link_density = link_density\n    self.coeff_range = coeff_range\n    self.exponents = list(range(0, max_exp))\n    self.min_lag = min_lag\n    self.max_lag = max_lag\n    self.n_hidden_confounders = n_hidden_confounders\n    self.n_confounded = n_confounded_vars\n\n    self.obsVar = ['X_' + str(i) for i in range(nvars)]\n    self.hiddenVar = ['H_' + str(i) for i in range(n_hidden_confounders)]\n    self.operators = operators\n    self.functions = functions\n    self.equations = {var: list() for var in self.obsVar + self.hiddenVar}\n    self.confounders = {h: list() for h in self.hiddenVar}\n    self.dependency_graph = {var: set() for var in self.obsVar + self.hiddenVar}\n    self.PAG = None\n\n    self.noise_config = noise_config\n    self.noise = None\n    if noise_config is not None:\n        if noise_config[0] is NoiseType.Uniform:\n            self.noise = np.random.uniform(noise_config[1], noise_config[2], (self.T, self.N))\n        elif noise_config[0] is NoiseType.Gaussian:\n            self.noise = np.random.normal(noise_config[1], noise_config[2], (self.T, self.N))\n        elif noise_config[0] is NoiseType.Weibull:\n            self.noise = np.random.weibull(noise_config[1], (self.T, self.N)) * noise_config[2]\n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.gen_equations","title":"<code>gen_equations()</code>","text":"<p>Generate random equations using the operator and function lists provided in the constructor.</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>def gen_equations(self):\n\"\"\"Generate random equations using the operator and function lists provided in the constructor.\"\"\"\n    for var in self.obsVar:\n        var_lagged_choice = copy.deepcopy(self.obsVar)\n        var_contemp_choice = copy.deepcopy(var_lagged_choice)\n        var_contemp_choice.remove(var)\n        self.equations[var] = self.__build_equation(var_lagged_choice, var_contemp_choice, var)\n\n    for hid in self.hiddenVar:\n        var_lagged_choice = copy.deepcopy(self.obsVar + self.hiddenVar)\n        var_contemp_choice = copy.deepcopy(var_lagged_choice)\n        var_contemp_choice.remove(hid)\n        self.equations[hid] = self.__build_equation(var_lagged_choice, var_contemp_choice, hid)\n\n    self.__add_conf_links()\n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.gen_interv_ts","title":"<code>gen_interv_ts(interventions, obs)</code>","text":"<p>Generate time-series corresponding to intervention(s).</p> <p>Parameters:</p> Name Type Description Default <code>interventions</code> <code>dict</code> <p>dictionary {INT_VAR : {INT_LEN: int_len, INT_VAL: int_val}}.</p> required <code>obs</code> <code>DataFrame</code> <p>Observational DataFrame.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>{interventional variable: interventional time-series data}.</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>def gen_interv_ts(self, interventions, obs) -&gt; dict:\n\"\"\"\n    Generate time-series corresponding to intervention(s).\n\n    Args:\n        interventions (dict): dictionary {INT_VAR : {INT_LEN: int_len, INT_VAL: int_val}}.\n        obs (DataFrame): Observational DataFrame.\n\n    Returns:\n        dict: {interventional variable: interventional time-series data}.\n    \"\"\"\n    starting_point = obs.values\n    int_data = dict()\n    for int_var in interventions:\n        T = int(interventions[int_var][\"T\"])\n        if self.noise_config is not None:\n            if self.noise_config[0] is NoiseType.Uniform:\n                int_noise = np.random.uniform(self.noise_config[1], self.noise_config[2], (T, self.N))\n            elif self.noise_config[0] is NoiseType.Gaussian:\n                int_noise = np.random.normal(self.noise_config[1], self.noise_config[2], (T, self.N))\n            elif self.noise_config[0] is NoiseType.Weibull:\n                int_noise= np.random.weibull(self.noise_config[1], (self.T, self.N)) * self.noise_config[2]\n        np_data = np.zeros((T, self.N))\n        np_data[0:self.max_lag, :] = starting_point[len(starting_point)-self.max_lag:,:]\n\n        for t in range(self.max_lag, T):\n            for target, eq in self.equations.items():\n                if target != int_var:\n                    np_data[t, self.variables.index(target)] = self.__evaluate_equation(eq, t, np_data)\n                    if self.noise_config is not None: np_data[t, self.variables.index(target)] += int_noise[t, self.variables.index(target)]\n                else:\n                    np_data[t, self.variables.index(target)] = interventions[int_var][\"VAL\"]\n\n        int_data[int_var] = Data(np_data, self.variables)\n        int_data[int_var].shrink(self.obsVar)\n        starting_point = np_data\n    return int_data\n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.gen_obs_ts","title":"<code>gen_obs_ts()</code>","text":"<p>Generate time-series data.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>(Data obj with hidden vars, Data obj without hidden vars).</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>def gen_obs_ts(self) -&gt; tuple:\n\"\"\"\n    Generate time-series data.\n\n    Returns:\n        tuple: (Data obj with hidden vars, Data obj without hidden vars).\n    \"\"\"\n    np_data = np.zeros((self.T, self.N))\n    for t in range(self.T):\n        if t &lt; self.max_lag:\n            for target, eq in self.equations.items():\n                np_data[t, self.variables.index(target)] = self.noise[t, self.variables.index(target)]\n        else:\n            for target, eq in self.equations.items():\n                np_data[t, self.variables.index(target)] = self.__evaluate_equation(eq, t, np_data)\n                if self.noise is not None: np_data[t, self.variables.index(target)] += self.noise[t, self.variables.index(target)]\n\n    data = Data(np_data, self.variables)\n    only_obs = copy.deepcopy(data)\n    only_obs.shrink(self.obsVar)\n    return data, only_obs\n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.get_Adj","title":"<code>get_Adj(withHidden=False)</code>","text":"<p>Output the Structural Causal Model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>scm.</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>def get_Adj(self, withHidden = False) -&gt; dict:\n\"\"\"\n    Output the Structural Causal Model.\n\n    Returns:\n        dict: scm.\n    \"\"\"\n    eqs = self.equations if withHidden else self.obsEquations\n    scm = {target : list() for target in eqs.keys()}\n    for target, eq in eqs.items():\n        for term in eq:\n            if len(term) == 6:\n                _, _, _, variable, lag, _ = term\n            else:\n                _, _, _, variable, lag = term\n            if variable not in scm.keys(): continue # NOTE: this is needed to avoid adding hidden vars\n            scm[target].append((variable, -abs(lag)))\n    return scm\n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.get_DPAG","title":"<code>get_DPAG()</code>","text":"<p>Output the PAG starting from a DAG.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>scm.</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>def get_DPAG(self) -&gt; dict:\n\"\"\"\n    Output the PAG starting from a DAG.\n\n    Returns:\n        dict: scm.\n    \"\"\"\n    if self.PAG is None:\n        scm = self.get_Adj(withHidden=True)\n        self.PAG = PAG(scm, self.max_lag, self.hiddenVar)\n    return self.PAG.convert2Graph()\n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.intervene","title":"<code>intervene(int_var, int_len, int_value, obs)</code>","text":"<p>Generate intervention on a single variable.</p> <p>Parameters:</p> Name Type Description Default <code>int_var</code> <code>str</code> <p>variable name.</p> required <code>int_len</code> <code>int</code> <p>intervention length.</p> required <code>int_value</code> <code>float</code> <p>intervention value.</p> required <code>obs</code> <code>DataFrame</code> <p>Observational DataFrame.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>{interventional variable: interventional time-series data}.</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>def intervene(self, int_var, int_len, int_value, obs) -&gt; dict:\n\"\"\"\n    Generate intervention on a single variable.\n\n    Args:\n        int_var (str): variable name.\n        int_len (int): intervention length.\n        int_value (float): intervention value.\n        obs (DataFrame): Observational DataFrame.\n\n    Returns:\n        dict: {interventional variable: interventional time-series data}.\n    \"\"\"\n    if not isinstance(int_var, list): int_var = [int_var]\n    if not isinstance(int_len, list): int_len = [int_len]\n    if not isinstance(int_value, list): int_value = [int_value]\n    return self.gen_interv_ts({v: {\"T\": l, \"VAL\": val} for v, l, val in zip(int_var, int_len, int_value)}, obs)\n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.print_SCM","title":"<code>print_SCM(withHidden=False)</code>","text":"<p>Print the Structural Causal Model.</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>def print_SCM(self, withHidden = False):\n\"\"\"Print the Structural Causal Model.\"\"\"\n    scm = self.get_Adj(withHidden)\n    for t in scm: print(t + ' : ' + str(scm[t]))    \n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.print_equations","title":"<code>print_equations()</code>","text":"<p>Print the generated equations.</p> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>def print_equations(self):\n\"\"\"Print the generated equations.\"\"\"\n    toprint = list()\n    for target, eq in self.equations.items():\n        equation_str = target + '(t) = '\n        for i, term in enumerate(eq):\n            if len(term) == 6:\n                operator, coefficient, function, variable, lag, exponent = term\n                coefficient = round(coefficient, 2)\n                if i != 0: \n                    term_str = f\"{operator} {coefficient} * {function}({variable}, {exponent})(t-{lag}) \"\n                else:\n                    term_str = f\"{coefficient} * {function}({variable}, {exponent})(t-{lag}) \"\n            else:\n                operator, coefficient, function, variable, lag = term\n                coefficient = round(coefficient, 2)\n                if function != '':\n                    if i != 0: \n                        term_str = f\"{operator} {coefficient} * {function}({variable})(t-{lag}) \"\n                    else:\n                        term_str = f\"{coefficient} * {function}({variable})(t-{lag}) \"\n                else:\n                    if i != 0: \n                        term_str = f\"{operator} {coefficient} * {variable}(t-{lag}) \"\n                    else:\n                        term_str = f\"{coefficient} * {variable}(t-{lag}) \"\n\n            equation_str += term_str\n        toprint.append(equation_str)\n    eq = \"\\n\".join(toprint)\n    print(eq)\n    return eq\n</code></pre>"},{"location":"random_system/#causalflow.random_system.RandomGraph.RandomGraph.ts_dag","title":"<code>ts_dag(withHidden=False, save_name=None, randomColors=False)</code>","text":"<p>Draw a Time-seris DAG.</p> <p>Parameters:</p> Name Type Description Default <code>withHidden</code> <code>bool</code> <p>bit to decide whether to output the SCM including the hidden variables or not. Defaults to False.</p> <code>False</code> <code>save_name</code> <code>str</code> <p>figure path. Defaults to None.</p> <code>None</code> <code>randomColors</code> <code>bool</code> <p>random color for each node. Defaults to False.</p> <code>False</code> Source code in <code>causalflow/random_system/RandomGraph.py</code> <pre><code>def ts_dag(self, withHidden = False, save_name = None, randomColors = False):\n\"\"\"\n    Draw a Time-seris DAG.\n\n    Args:\n        withHidden (bool, optional): bit to decide whether to output the SCM including the hidden variables or not. Defaults to False.\n        save_name (str, optional): figure path. Defaults to None.\n        randomColors (bool, optional): random color for each node. Defaults to False.\n    \"\"\"\n    gt = self.get_Adj(withHidden) if withHidden else self.get_DPAG()\n    var = self.variables if withHidden else self.obsVar\n    g = DAG(var, self.min_lag, self.max_lag, False, gt)\n    node_color = []\n\n    tab_colors = plt.cm.get_cmap('tab20', 20).colors  # You can adjust the number of colors if needed\n    avail_tab_colors = list(copy.deepcopy(tab_colors))\n    for t in g.g:\n        if t in self.hiddenVar:\n            node_color.append('peachpuff')\n        else:\n            if randomColors :\n                c = random.randint(0, len(avail_tab_colors)-1)\n                node_color.append(avail_tab_colors[c])\n                avail_tab_colors.pop(c)\n            else:\n                node_color.append('orange')\n\n    # Edges color definition\n    edge_color = dict()\n    for t in g.g:\n        for s in g.g[t].sources:\n            s_index = len(g.g.keys())-1 - list(g.g.keys()).index(s[0])\n            t_index = len(g.g.keys())-1 - list(g.g.keys()).index(t)\n\n            s_lag = self.max_lag - s[1]\n            t_lag = self.max_lag\n            while s_lag &gt;= 0:\n                s_node = (s_lag, s_index)\n                t_node = (t_lag, t_index)\n                if s[0] in self.hiddenVar:\n                    edge_color[(s_node, t_node)] = 'gainsboro'\n                else:\n                    edge_color[(s_node, t_node)] = 'gray'\n\n                s_lag -= 1\n                t_lag -= 1\n\n    g.ts_dag(save_name = save_name, node_color = node_color, edge_color = edge_color, min_width=2, max_width=5, x_disp=1, node_size=6)\n</code></pre>"}]}